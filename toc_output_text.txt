
--- Page 1 ---
Introduction to
Automata Theory,
Languages, and Computation



--- Page 2 ---
Theory of Cornputete

Introduction to
Automata Theory,
Languages, and Computation

thet been more than jo year ainee john Hapcroft and jeffrey Uitman firw

published thr clue book on formal languages. automata theory, and

computational complevity With this long-awaited rewiien, the author

Tonner fo pesent the material in acence and straightlorward manner
fow with aneve out lor the practical applcations along with the mathematics.

This edition hus been reviewed to make if more accevible 10 todays students inchud-
ing the addienon of more maenal on wring proofs, more figures ana) pectures to Con
wey ideas. wedebars to highlight related material, and a bew formal writing style ft
icles many new exerci in each chapter to help reader confirm and enhanwe
ther underuanding of the material

FEATURES

Compietety cewritren to be few formal providing more acoeedtaliry

to undergraduate stadents

Emptiness modern applications of the thecry

Ler mmeroos frgures bo help conmveey nea

Prevdes more deta) and intuition for definition and prects
— inchudes special adehan to prevent supplemental material that may te

of mitered to reader

Challenge: readers with euemeve exercnes of wide-ranging difficulty bevels
reer a graphical netation for PDAs and Tari machines

joe £ Hoperatt i the joweph Sibert Bean of Engineering at Cornel) Univeruty, and
winner of the roi AM Turing Award

fajees Motwon! iv Anociate Profesor and Director of Graduate Studies for
Computer Sriente at Yanlord Univerity.

jeffrey © Ulin iv the Sanford W. Ascherman Profesor of Computer Science af

AION va

[ISBN O-201-441245-)



--- Page 3 ---


--- Page 4 ---
Senior Acquisitions Editor Matte Suarez-Rivas
Project Editor Katherine Harutunian
Executive Marketing Manager Michael Hirsch
Cover Design Leslie Haimes
Art Direction Regina Hagen
Prepress and Manufacturing Caroline Feil

Access the latest information about Addison-Wesley titles from our World Wide
Web site: http:/Avwww.awLcom

The programs and applications presented in this book have been included for
their instructional value. They have been tested with care, but are not guaran-
teed for any particular purpose. The publisher does not offer any warranties or
representations, not does it accept any liabilities with respect to the programs
or applications.

Library of Congress Cataloging-in-Publication Data

Hopcroft, John E., 1939-
Intreduction to automata theory, languages, and computation / John E.
Hopcroft, Rajeev Motwani, Jeffrey D. Ullman. 2nd ed.
p. cm.
ISBN 0-201-44124-1
1. Machine theory. 2. Formal languages. 3. Computational complexity.
I. Motwani, Rajeev. If. Ullman, Jeffrey D., 1942-.

QA267 .H56 2001
§11.3—dc21 00-064608

Gopyright © 2001 by Addison-Wesley

All rights reserved. No part of this publication may be reproduced, stored ina
retrieval system, or transmitted, in any form or by any means, electronic,
mechanical, photocopying, recording, or otherwise, without the prior written
permission of the publisher. Printed in the United States of America.

3456789 10-MA-04030201


--- Page 5 ---
Preface

In the preface from the 1979 predecessor to this book, Hopcroft and Ullman
marveled at the fact that the subject of automata had exploded, compared with
its state at the time they wrote their first book, in 1969. Truly, the 1979 book
contained many topics not found in the earlier work and was about twice its
size. If you compare this book with the 1979 book, you will find that, like the
automobiles of the 1970's, this book is “larger on the outside, but smaller on
the inside.” That sounds like a retrograde step, but we are happy with the
changes for several reasons.

First, in 1979, automata and language theory was still an area of active
research. A purpose of that book was to encourage mathematically inclined
students to make new contributions to the field. Today, there is little direct
research in automata theory (as opposed to its applications), and thus little
motivation for us to retain the succinct, highly mathematical tone of the 1979
book.

Second, the role of automata and language theory has changed over the
past two decades. In 1979, automata was largely a graduate-level subject, and
we imagined our reader was an advanced graduate student, especially those
using the later chapters of the book. Today, the subject is a staple of the
undergraduate curriculum. As such, the content of the book must assume less
in the way of prerequisites from the student, and therefore must provide more
of the background and details of arguments than did the earlier book.

A third change in the environment is that Computer Science has grown to
an almost unimaginable degree in the past two decades. While in 1979 it was
often a challenge to fill up a curriculum with material that we felt would survive
the next wave of technology, today very many subdisciplines compete for the
limited amount of space in the undergraduate curriculum.

Fourthly, CS has become 2 more vocational subject, and there is a severe
pragmatism among many of its students. We continue to believe that aspects
of automata theory are essential tools in a variety of new disciplines, and we
believe that the theoretical, mind-expanding exercises embodied in the typical
automata course retain their value, no matter how much the student prefers to
learn only the most immediately monetizable technology. However, to assure
a continued place for the subject on the menu of topics available to the com-
puter science student, we believe it is necessary to emphasize the applications

iii :


--- Page 6 ---
iv PREFACE

along with the mathematics. Thus, we have replaced a number of the more
abstruse topics in the earlier book with examples of how the ideas are used
today. While applications of automata and language theory to compilers are
now so well understood that they are normally covered in a compiler course,
there are a variety of more recent uses, including model-checking algorithms
to verify protocols and document-description languages that are patterned on
context-free grammars.

A final explanation for the simultaneous growth and shrinkage of the book
is that we were today able to take advantage of the TEX and IATpX typesetting
systems developed by Don Knuth and Les Lamport. The latter, especially,
encourages the “open” style of typesetting that makes books larger, but easier
to read. We appreciate the efforts of both men.

Use of the Book

This book is suitable for a quarter or semester course at the Junior level or
above. At Stanford, we have used the notes in CS154, the course in automata
and language theory. It is a one-quarter course, which both Rajeev and Jeff have
taught. Because of the limited time available, Chapter 11 is not covered, and
some of the later material, such as the more difficult polynomial-time reductions
in Section 10.4 are omitted as well. The book’s Web site (see below) includes
notes and syllabi for several offerings of CS154.

Some years ago, we found that many graduate students came to Stanford
with a course in automata theory that did not include the theory of intractabil-
ity. As the Stanford faculty believes that these ideas are essential for every
computer scientist to know at more than the level of “NP-complete means it
takes too long,” there is another course, C5154N, that students may take to
cover only Chapters 8, 9, and 10. They actually participate in roughly the last
third of C5154 to fulfill the CS154N requirement. Even today, we find several
students each quarter availing themselves of this option. Since it requires little
extra effort, we recommend the approach.

Prerequisites

To make best use of this book, students should have taken previously a course
covering discrete mathematics, e.g., graphs, trees, logic, and proof techniques.
We assume also that they have had several courses in programming, and are
familiar with common data structures, recursion, and the role of major system
components such as compilers. These prerequisites should be obtained in a
typical freshman-sophomore CS program.


--- Page 7 ---
Exercises

The book contains extensive exercises, with some for almost every section. We
indicate harder exercises or parts of exercises with an exclamation point. The
hardest exercises have a double exclamation point.

Some of the exercises or parts are marked with a star. For these exercises,
we shall endeavor to maintain solutions accessible through the book’s Web page.
These solutions are publicly available and should be used for seif-testing. Note
that in a few cases, one exercise B asks for modification or adaptation of your
solution to another exercise A. If certain parts of A have solutions, then you
should expect the corresponding parts of B to have solutions as well.

Support on the World Wide Web

The book’s home page is
http: //www-db.stanford.edn/~ullman/ialc. html

Here are solutions to starred exercises, errata as we learn of them, and backup
materials. We hope to make available the notes for each offering of C5154 as
we teach it, including homeworks, solutions, and exams.

Acknowledgements

A handout on “how to do proofs” by Craig Silverstein influenced some of the
material in Chapter 1. Comments and errata on drafts of this book were re-
ceived from: Zoe Abrams, George Candea, Haowen Chen, Byong-Gun Chun,
Jeffrey Shallit, Bret Taylor, Jason Townsend, and Erik Uzureau. They are
pratefully acknowledged. Remaining errors are ours, of course.

R. M.

J.B. U.

Ithaca NY and Stanford CA
September, 2000


--- Page 8 ---


--- Page 9 ---
Table of Contents

1 Automata: The Methods and the Madness
1.1 Why Study Automata Theory? ©... 000. ee ee
1.1.1 Introduction to Finite Automata .........-200,
1.1.2 Structural Representations 2... 2. ee ee eee
1.1.3 Automata and Complexity 0... ........2-22050.
1.2 Introduction to Formal Proof... 2... 2 ee
1.2.1 Deductive Proofs.......-2..---5- 04 eee eee
1.2.2 Reduction to Definitions... ......--.--..0-24--

2

1.8

L.4

1.5

1.6
17

2.1

1.2.3 Other Theorem Forms ......--.008++------
1.2.4 Theorems That Appear Not to Be If-Then Statements . .
Additional Forms of Proof... - 2... 2 ee ee ee
1.3.1 Proving Equivalences About Sets ....-.......-.
1.3.2 The Contrapositive......-..2.-522-2-2205
1.3.3 Proof by Contradiction 2... ee ee ee
1.3.4 Counterexamples .. 2.6 00 ee ee
Inductive Proofs . 1... 0-2-2 ee
1.4.1 Inductions on Integers... 2... 2. ee ee ee es
1.4.2 More General Forms of Integer Inductions........
1.4.3 Structural Inductions 2... 0... ee es
1.4.4 Mutual Inductions ...- 0... 050 ee ee es
The Central Concepts of Automata Theory .........-..
1.5.1 Alphabets... ee ee ee
1.5.2 Strings...
1.5.3 Languages... 2.2. ee ee ee
1.5.4 Problems ..-..-..-. 2-20-00 8 e eee eee
Summary of Chapter1.. 2... 2.22 eee eee
References for Chapter]... 2-2-6 2 ee ee

Finite Automata
An Informal Picture of Finite Automata ....-.-.-....854
9.1.1 The Ground Rules ........-.--02.0.--2--.
91.2 The Protocol... 2... 2.02202. ee es
2.1.3 Enabling the Automata to Ignore Actions ....-.-..

vii


--- Page 10 ---
TABLE OF CONTENTS

vili

2.1.4 The Entire System as an Automaton............ 43
2.1.5 Using the Product Automaton to Validate the Protocol . 45
9.2 Deterministic Finite Automata .. 1... ee ee ee 45
2.2.1 Definition of a Deterministic Finite Automaton ...... 46
2.2.2 How a DFA Processes Strings... ...-..-+-2-05- 46
2.2.3 Simpler Notations for DFA’s ............005. 48
9.2.4 Extending the Transition Function to Strings ....... 49
9.2.5 The LanguageofaDFA.........-..-5000 4+ 52
92.6 Exercises for Section 2.2...-.--.-.-.---.-28005 53
2.3 WNondeterministic Finite Automata .-. 2... .0-0220----- 55
2.3.1 An Informal View of Nondeterministic Finite Automata . 56
2.3.2 Definition of Nondeterministic Finite Automata. ..... 57
9.3.3 The Extended Transition Function .............- 58
2.34 The Language ofan NFA .... 2... 2.0 ee ee eee 59

2.3.5 Equivalence of Deterministic and Nondeterministic Finite
Automata... 6.066 es 60
9.3.6 A Bad Case for the Subset Construction. ........., 65
9.3.7. Exercises for Section 2.3... - 2. ee ee 66
2.4 An Application: Text Search 2... 1... eee ee eee ee 68
2.4.1 Finding Strings in Text ©... 06. ee ee eee 68
2.4.2 Nondeterministic Finite Automata for Text Search .... 69
243 <A DFA to Recognize a Set of Keywords .......... 70
9.4.4 Exercises for Section 2.4.....0. 00.0508. teas 72
9.5 Finite Automata With Epsilon-Transitions. ........---- 72
9.5.1 Uses of e-Transitions . 2... ee 72
9.5.2 The Formal Notation for ane-NFA............. 74
2.5.3 Epsilon-Closures 2... 006. ee eee ee 75
9.5.4 Extended Transitions and Languages for e-NFA’s... . . 76
2.5.5 Eliminating e-Transitions .........-.--4+055 77
2.5.6 Exercises for Section 2.5 .......0.-------.005. 80
2.6 Summary of Chapter2.....-.-.- 500002502022 eee 80
2.7 References for Chapter2....-- 2-0 ee ee ee ee 81
3 Regular Expressions and Languages 8&3
3.1 Regular Expressions . 2... 0-2 ee es 83
3.1.1 The Operators of Regular Expressions ........... 84
$.1.2 Building Regular Expressions ...-.....-..---- 85
3.1.3 Precedence of Regular-Expression Operators .......- 88
3.1.4 Exercises for Section 3-1...-.....0--.-002004 89
3.2 Finite Automata and Regular Expressions .........-..-.- 90
3.2.1 From DFA’s to Regular Expressions .........+.+-- 91

3.2.2 Converting DFA’s to Regular Expressions by Eliminating
States oe 96
3.2.3 Converting Regular Expressions to Automata ....... 101

3.2.4 Exercises for Section3.2.......0-..0.058-. 0080 ee 106


--- Page 11 ---
TABLE OF CONTENTS ix

3.3 Applications of Regular Expressions ..........0-+00+ 108
3.3.1 Regular Expressions in UNIX ...-......------ 108
3.3.2 Lexical Analysis .. 0... 02.0000 0-202 tees 109
3.3.3 Finding Patterns in Text .....-...0-02522-5- 111
3.3.4 Exercises for Section 3.3.2. ..00--.0-0 050020005. 113

3.4 Algebraic Laws for Regular Expressions ..........-.4: 114
3.4.1 Associativity and Commutativity..........00.05 114
3.4.2 Identities and Annihilators .....-.-...00004- 115
3.4.3 Distributive Laws... 2... 0.00 eee es 115
3.4.4 The Idempotent Law... .......0.-54--2----5 116
3.4.5 Laws Involving Closures... .....------.----5 117
3.4.6 Discovering Laws for Regular Expressions ......... 117
3.4.7 The Test. for a Regular-Expression Algebraic Law... . . 119
3.4.8 Exercises for Section 3.4... ..-.-...0.0. 60222005. 120

3.5 Summary of Chapter3.......0.0-. 5020-550 2 eee ee 122

3.6 References for Chapter3.-..- 2... 20-0 - et eee 122

4 Properties of Regular Languages 125

4.1 Proving Languages not to be Regular... ...-....---.-. 126
4.1.1 The Pumping Lemma for Regular Languages ....... 126
4.1.2 Applications of the Pumping Lemma............- 127
4.1.3 Exercises for Section 4.1... 2.0000... 20 eee 129

4.2 Closure Properties of Regular Languages. .......-.---- 131
4.2.1 Closure of Regular Languages Under Boolean Operations 131
422 Reversal... 0... ee 137
4.2.3 Homomorphisms .. 2... 0. ee ee ee 139
4.2.4 Inverse Homomorphisms... 2... ee eee es 140
4.2.5 Exercises for Section 4.2 2... ee 145

4.3 Decision Properties of Regular Languages ...------.--.. 149
4.3.1 Converting Among Representations ...-......-.. 149
43.2 Testing Emptiness of Regular Languages. -.....-.- 151
4.3.3 Testing Membership in a Regular Language .......- « 153
4.3.4 Exercises for Section 4.3 6.0 ee ee 153

4.4 Equivalence and Minimization of Automata ............ 154
4.4.1 Testing Equivalence of States ....-..-..------ 154
44.2 Testing Equivalence of Regular Languages....-..-.. 157
4.4.3 Minimization of DFA’s........--0800-5 2-5: 159
4.4.4 Why the Minimized DFA Can’t Be Beaten ...-..... 162
4.4.5 Exercises for Section 4.4... 0... 0000 epee eee 164

4.5 Summary of Chapter4. 0... 0. ce ee ee 165

4.6 References for Chapter4d.-.. 2.2.2... 0-04 pe eee eee 166


--- Page 12 ---
TABLE OF CONTENTS

5 Context-Free Grammars and Languages
5.1 Context-Free Grammars . 2...

5.2

5.3

5.4

3.8
5.6

5.1.1
5.1.2
5.1.3
5.1.4
5.1.5
5.1.6
5.1.7

An Informal Example . 2... 2... ee ee
Definition of Context-Free Grammars ...........
Derivations Using a Grammar................
Leftmost and Rightmost Derivations ............
The Language ofa Grammar ......-.....2005%
Sentential Forms .......-...........220-.-
Exercises for Section 5.1 .....0.020.0.....0.0004

Parse Trees 2.22. ee ee

5.2.1
5.2.2
5.2.3
5.2.4
5.2.5
5.2.6
5.2.7

Constructing Parse Trees 2.2. 2 ee
The Yield ofa Parse Tree... 2... 0. 2 ee
Inference, Derivations, and Parse Trees
From Inferences to Trees... 2. 2. ee ee
From Trees to Derivations... . 2... ee ee
From Derivations to Recursive Inferences .......2..
Exercises for Section 5.2......0000000000 00008

Applications of Context-Free Grammars ..............

9.3.1
5.3.2
5.3.3
5.3.4
5.3.5

Parsers 2. ee ee te
The YACC Parser-Generator ........-.0.000%
Markup Languages... ........000 52 eee
XML and Document-Type Definitions ........0,.,
Exercises for Section 6.3. ....0.0. 0.00050 eae

Ambiguity in Grammars and Languages ..............

5.4.1
5.4.2
5.4.3
5.4.4
§.4.5

Ambiguous Grammars ..................-.-
Removing Ambiguity From Grammars ...........
Leftmost Derivations as a Way to Express Ambiguity

Inherent Ambiguity ......-....0-.-. 0000 eae
Exercises for Section 5.4 ...................

Summary of Chapter5.......0......0.0...202000.-
References for Chapter 5... ...0.0.000.0. 0.000042 ee

Pushdown Automata

6.1 Definition of the Pushdown Automaton .........202....

6.2

6.1.1
6.1.2
6.1.3
6.1.4
6.1.5

Informal Introduction .......0. 00000-0005
The Formal Definition of Pushdown Automata ......
A Graphical Notation for PDA’s .......0....0..
Instantaneous Descriptions ofa PDA... ......02~0.
Exercises for Section 6-1..-....0.....022. 0004

The Languagesofa PDA .......0.00-22 000004 a eae

6.2.1
6.2.2
6.2.3
6.2.4
6.2.5

Acceptance by Fimal State... 2-0... 0.00222.
Acceptance by Empty Stack... ...-....,0.240.
From Empty Stack to Final State... 2... 0.002.
From Final State to Empty Stack... .....0...0..
Exercises for Section 6.2.....0.0...0....020004

169
169
170
171
173
175
177
178
179
181
181
183

. 184

185
187
190
19]
191
192
194
196
198


--- Page 13 ---
TABLE OF CONTENTS

6.3 Equivalence of PDA’s and CFG’s .. 2.2... ...-.5-2---

6.4

6.3.1
§.3.2
6.3.3

From Grammars to Pushdown Automata .........
From PDA's to Grammars. 2... ee ee
Exercises for Section 6.3 2.0... 00 ee eee ee

Deterministic Pushdown Automata .......----.-.-.-+

6.4.1
6.4.2
6.4.3
6.4.4
6.4.5

Definition of a Deterministic PDA .......-...-..--
Regular Languages and Deterministic PDA’s ......-
DPDA’s and Context-Free Languages .......----
DPDA’s and Ambiguous Grammars ........-.--
Exercises for Section 6.4 0.0.0.0. ee eee ee

6.5 Summary of Chapter6. 2... 0-0 0-0. cee ee ee
6.6 References for Chapter6....-..-..0-0 000000 eee

Properties of Context-Free Languages

7.1 Normal Forms for Context-Free Grammars .....-.--....

7.2

7.3

74

7.1.1 Eliminating Useless Symbols .....-...--..2445
7.1.2 Computing the Generating and Reachable Symbols... .
7.1.3 Eliminating e-Productions.....-.-.-----+-.-.-.
7.14 Eliminating Unit Productions......-.---..-....-
7.143 Chomsky Normal Form .. 2... 0 ee eee
7.1.6 Exercises for Section 7.1.....0.0.0.0.- 0000 eee
The Pumping Lemma for Context-Free Languages ....-..- -
7.2.1 The Size of Parse Trees 2.2.2.0. 2 ee ee
7.2.2 Statement of the Pumping Lemma .........-.--
7.2.3 Applications of the Pumping Lemma for CFL’s..... .
7.2.4 Exercises for Section 7.2... ...-.0-...0 0002-40 G-
Closure Properties of Context-Free Languages. ......-...-
7.3.1 Substitutions 2. 0. 6 ee ee
7.3.2 Applications of the Substitution Theorem .........-
7.3.3 Reversal... 0.0.0.0 0 0000 0 eee
7.3.4 Intersection With a Regular Language ...........
7.3.5 Inverse Homomorphism .........-----.+---5
7.3.6 Exercises for Section 7.3.......0..--+-+.-.-+---
Decision Properties of CFL’s ....-..-002+5-2--5--06-
7.4.1 Complexity of Converting Among CFG’s and PDA’s . . .
7.4.2 Running Time of Conversion to Chomsky Normal Form .
7.4.3 Testing Emptiness of CFL’s ... 2... 2.00.0 ee ee
7.4.4 Testing MembershipinaCFL ..............-
7.4.5 Preview of Undecidable CFL Problems. ......--..-
7.4.6 Exercises for Section 7.4. ....0.-..--6025-220--

Summary of Chapter 7.2.2.0... 550000 eee eee
References for Chapter 7... 0.0.02... 0 ee eee eee


--- Page 14 ---
TABLE OF CONTENTS

xii

8 Introduction to Turing Machines 307
8.1 Problems That Computers Cannot Solve... .......2.40.. 307
8.1.1 Programs that Print “Hello, World” «2... ......5.- 308
8.1.2 The Hypothetical “Hello, World” Tester... .....2.. 310
8.1.3 Reducing One Problem to Another. ..........0.. 313
8.1.4 Exercises for Section 81 .......0..0.0......0.. 316
8.2 The Turing Machine .. 2... ee ee eee 316
8.2.1 The Quest to Decide All Mathematical Questions... . . 317
8.2.2 Notation for the Turing Machine ..-............ 318
8.2.3 Instantaneous Descriptions for Turing Machines... .. . 320
8.2.4 Transition Diagrams for Turing Machines ......... 323
8.2.5 The Language of a Turing Machine............. 326
8.2.6 Turing Machines and Halting .........-....2... 327
8.2.7 Exercises for Section 8.2. ..-..0-..020..0.2.22... 328
8.3 Programming Techniques for Turing Machines. .......... 329
8.3.1 Storagein the State ........-........-..- 330
8.3.2 Multiple Tracks... 2.2.2 ee ee 331
8.3.3 Subroutines... 2-0-0 -00002-0 00200220200. 333
8.3.4 Exercises for Section 8.3 ....0.000 0000002 ee eae 334
8.4 Extensions to the Basic Turing Machine .............. 336
8.4.1 Multitape Turing Machines... 2-0. pe ee ee 336
8.4.2 Egqnivalence of One-Tape and Multitape TM’s..... . . 337
8.4.3 Running Time and the Many-Tapes-to-One Construction 339
8.4.4 Nondeterministic Turing Machines .......,..-.-.- 340
8.4.5 Exercises for Section 8.4 .......0...0.-2....000, 342
8.5 Restricted Turing Machines... 0... ee ee 345
8.5.1 Turing Machines With Semi-infinite Tapes... ...... 346
8.5.2 Multistack Machines... .....-.....-.0.00048. 348
8.5.9 Counter Machines .........0.........--.. 351
8.5.4 The Power of Counter Machines .............. 352
8.5.5 Exercises for Section 85 2... 2... ee ee 354
8.6 Turing Machines and Computers .......-.-.--..-2--- 355
8.6.1 Simulating a Turing Machine by Computer ....-... - 355
8.6.2 Simulating a Computer by a Turing Machine ....... 306

8.6.3 Comparing the Running Times of Computers and Turing
Machines .. 2.0... 0-02 ee eee ee 361
8.7 Summary of Chapter8.......--.0.-..2..0-..-0.4. 363
8.8 References for Chapter 8... 0... 0. eee ee ee ee es 365
9 Undecidability 367
9.1 A Language That Is Not Recursively Enumerable... 2... . - 368
9.1.1 Enumerating the Binary Strings .............. 369
9.1.2 Codes for Turing Machines -. 2.2... Le. 369
9.1.3 The Diagonalization Language ...-.--..-..--.-..- 370

9.1.4 Proof that £g is not Recursively Enumerable ...... . 372


--- Page 15 ---
TABLE OF CONTENTS

9.1.5 Exercises for Section 9.1 .....-.-.-2+--+-2--2055%
9.2 An Undecidable Problem Thatis RE... ...-..-..----
9.2.1 Recursive Languages... .--- 2-2 eee eee
9.2.2 Complements of Recursive and RE languages -..... -
9.2.3 The Universal Language ......-.-----------
9.2.4 Undecidability of the Universal Language ...-..-...
9.2.5 Exercises for Section 9.2 .....-.-..00-2.-.000--
9.3 Undecidable Problems About Turing Machines .......-...-
9.3.1 Reductions .. 1... 2. ee es
9.3.2 Turing Machines That Accept the Empty Language . .
9.3.3 Rice’s Theorem and Properties of the RE Languages . .
9.3.4 Problems about Turing-Machine Specifications ......
9.3.5 Exercises for Section 9.3 0.0.0.0 ee ee ee
9.4 Post’s Correspondence Problem... ...---- 56-0 bees
9.4.1 Definition of Post’s Correspondence Problem .......
94.2 The “Modified” PCP... 2.0.0.0 2.050 e eee eee
9.4.3 Completion of the Proof of PCP Undecidability..... -
9.4.4 Exercises for Section 9.4... - 6.6 pe eee es
9.5 Other Undecidable Problems .........--------555%
9.5.1 Problems About Programs ......------.-4005
9.5.2 Undecidability of Ambiguity for CFG’s ......-.--
9.5.3 The Complement of a List Language .....-..--.--
9.5.4 Exercises for Section 9.5... -..0----0002-220--
9.6 Summary of Chapter9........--5-6 000002222505.
9.7 References for Chapter9... 2-006. ee es

10 Intractable Problems

10.1 The Classes Pand NP... 2 ee
10.1.1 Problems Solvable in Polynomial Time. ..........-
10.1.2 An Example: Kruskal’s Algorithm ......-.....-
10.1.3 Nondeterministic Polynomial Time ......-..-.-.--
10.1.4 An A’P Example: The Traveling Salesman Problem . . .
10.1.5 Polynomial-Time Reductions ......-.-.----.--
10.1.6 NP-Complete Problems ......--.-.-5-5++-+--
10.1.7 Exercises for Section O12... 0. ee eee

10.2 An NP-Complete Problem... ......----002--2---
10.2.1 The Satisfiability Problem. ....-...-.------5-
10.2.2 Representing SAT Instances... 0-6 20. eee ee ee
10.2.3 NP-Completeness of the SAT Problem ....-......-
10.2.4 Exercises for Section 10.2... ..-..---02--040--

10.3 A Restricted Satisfiability Problem .........+-----5:
10.3.1 Normal Forms for Boolean Expressions. ......+-.--
10.3.2 Converting Expressions to CNF. 1... ee ee ee
10.3.3 NP-Completeness of CSAT .......----- 20455
10.3.4 NP-Completeness of 35AT. 0. 0. ee es
10.3.5 Exercises for Section 10.3 2.000.062.0050. 0 ee

xiii

372
373
373
374
377
379
381
383
383

. 384
. 387

390
390

A427
428


--- Page 16 ---
TABLE OF CONTENTS

xiv

10.4 Additional NP-Complete Problems .......-..-----.-- 447
10.4.1 Describing NP-complete Problems ............. 447
10.4.2 The Problem of Independent Sets... ....-.-.-... 448
10.4.3 The Node-Cover Problem -....-.....--.-.0.. A452
10.4.4 The Directed Hamilton-Circuit Problem .......... 453
10.4.5 Undirected Hamilton Circuits andthe TSP ........ 460
10.4.6 Summary of NP-Complete Problems ............ 461
10.4.7 Exercises for Section 10.4 2... 0.0.0.0 0+ ee eee 462

10.5 Summary of Chapter 10 2.0... 00... . ee ee ee 466
10,6 References for Chapter 10 ........-.-.....--.-004. 467
11 Additional Classes of Problems 469
11.1 Complements of Languages in NP ow 0. oe ee 470
11.1.1 The Class of Languages Co-NP 1... .......000. 470
11.1.2 NP-Complete Problems and CoNP ............ 471
11.1.8 Exercises for Section 11.1 ....--...-.......... 472

11.2 Problems Solvable in Polynomial Space .......-....... 473
11.2.1 Polynomial-Space Turing Machines. ............ 473
11.2.2 Relationship of PS and APS to Previously Defined Classes474
11.2.3 Deterministic and Nondeterministic Polynomial Space . . 476

11.3 A Problem That Is Complete for PS ...-...-.......... 478
11.3.1 PS-Completeness .-.....--..-..----2..--4. 478
11.3.2 Quantified Boolean Formulas ............-... 479
11.3.3 Evaluating Quantified Boolean Formulas. ....... . . 480
11.3.4 PS-Completeness of the QBF Problem ........... 482
11.3.5 Exercises for Section 11.3 ..... 0000.04 pea 487

11.4 Language Classes Based on Randomization ............ 487
11.4.1 Quicksort: an Example of a Randomized Algorithm . . . 488
11.4.2 A Turing-Machine Model Using Randomization. ..... 489
11.4.8 The Language of a Randomized Turing Machine ... .. 490
11.4.4 The ClassRP ....--..--.2..22--2-00-- 492
11.4.5 Recognizing Languagesin RP ...............- 494
11.4.6 The Class 2PP 2.2... ee ee 495
11.4.7 Relationship Between RP and ZPP ............ 496
11.4.8 Relationships to the Classes P and VP .......0.. 497

11.5 The Complexity of Primality Testing... ...........0.. 498
11.5.1 The Importance of Testing Primality...........-. 499
11.5.2 Introduction to Modular Arithmetic ..........0.-. 501
11.5.3 The Complexity of Modular-Arithmetic Computations . . 503
11.5.4 Random-Polynomial Primality Testing ........... 504
11.5.5 Nondeterministic Primality Tests ............48. 506
11.5.6 Exercises for Section 11.5 ..............00.0. 508

11.6 Summary of Chapter]1 .......-..........-.--, 508
11.7 References for Chapter 11 .........-..2....0-.0006. 510

Index 513


--- Page 17 ---
Chapter 1

Automata: The Methods
and the Madness

Automata theory is the study of abstract computing devices, or “machines.”
Before there were computers, in the 1930’s, A. Turing studied an abstract ma-
chine that had all the capabilities of today’s computers, at least as far as in
what they could compute. Turing’s goal was to describe precisely the boundary
between what a computing machine could do and what it could not do; his
conchisions apply not only to his abstract Turing machines, but to today’s real
machines.

In the 1940°s and 1950’s, simpler kinds of machines, which we today call
“finite automata,” were studied by a number of researchers. These automata,
originally proposed to model brain function, turned out to be extremely useful
for a variety of other purposes, which we shall mention in Section 1.1. Also in
the late 1950’s, the linguist N. Chomsky began the study of formal “grammars.”
While not strictly machines, these grammars have close relationships to abstract
automata and serve today as the basis of some important software components,
including parts of compilers.

In 1969, S. Cook extended Turing’s study of what could and what could
not be computed. Cook was able to separate those problems that can be solved
efficiently by computer from those problems that can in principle be solved, but
in practice take so much time that computers are useless for all but very small
instances of the problem. The latter class of problems is called “intractable,”
or “NP-hard.” It is highly unlikely that even the exponential improvement in
computing speed that computer hardware has been following (“Moore's Law”)
will have significant impact on our ability to solve large instances of intractable
problems.

All of these theoretical developments bear directly on what. computer scien-
tists do today. Some of the concepts, like finite automata and certain kinds of
formal grammars, are used in the design and construction of important kinds
of software. Other concepts, like the Turing machine, help us understand what


--- Page 18 ---
2 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

we can expect from our software. Especially, the theory of intractable problems
lets us deduce whether we are likely to be able to meet a problem “head-on”
and write a program to solve it (because it is not in the intractable class}, or
whether we have to find some way to work around the intractable problem:
find an approximation, use a heuristic, or use some other method to limit the
ammount of time the program will spend solving the problem.

In this introductory chapter, we begin with a very high-level view of what
automata theory is about, and what its uses are. Much of the chapter is de-
voted to a survey of proof techniques and tricks for discovering proofs. We cover
deductive proofs, reformulating statements, proofs by contradiction. proofs by
induction, and other impertant concepts. A final section introduces the con-
cepts that pervade automata theory: alphabets, strings, and languages.

1.1 Why Study Automata Theory?

There are several reasons why the study of automata and complexity is an
important part of the core of Computer Science. This scction serves to introduce
the reader to the principal motivation and also outlines the major topics covered
in this book.

1.1.1 Introduction to Finite Automata

Finite automata are a useful model for many important kinds of hardware and
software. We shall see, starting in Chapter 2, examples of how the concepts are
used. For the moment, let us just list some of the most. important kinds:

1. Software for designing and checking the behavior of digital circuits.

2. The “lexical analyzer” of a typical compiler, that is, the compiler com-
ponent that breaks the input text into logical units, such as identifiers,
keywords, and punctuation.

3. Software for scanning large badies of text, such as collections of Web
pages, to find occurrences of words, phrases, or other patterns.

4. Software for verifying systems of all types that have a finite number of
distinct states, such ag communications protocols or protocols for secure
exchange of information.

While we shall soon meet a precise definition of antomata of various types,
let us begin our informal introduction with a sketch of what a finite automaton
is and does. There are many systems or components, such as those enumerated
above, that may be viewed as being at all times in one of a finite number
of “states.” The purpose of a state is to remember the relevant portion of the
system’s history. Since there are only a finite number of states, the entire history
generally cannot. be remembered, so the system must be designed carefully, to


--- Page 19 ---
ti. WHY STUDY AUTOMATA THEORY? 3

remember what is important and forget what is not. The advantage of having
only a finite number of states is that we can implement the system with a fixed
set of resources. For example, we could implement. it in hardware as a circuit, or
as a simple form of program that can make decisions looking only at a limited
amount of data or using the position in the code itself to make the decision.

Example 1.1: Perhaps the simpiest nontrivial finite automaton is an on/off
switch. The device remembers whether it is in the “on” state or the “off” state,
and it allows the user to press a button whose effect is different, depending on
the state of the switch. That is, if the switch is in the off state, then pressing
the button changes it to the on state, and if the switch is in the on state, then
pressing the same button turns it to the off state.

Push

wr) Ge)

Push
Figure 1.1: A finite automaton modeling an on/off switch

The finite-automaton model for the switch is shown in Fig. 1.1. As for all
finite automata, the states are represented by circles; in this example, we have
named the states on and off. Arcs between states are labeled by “inputs,” which
represent external influences on the system. Here, both arcs are labeled by the
input Push, which represents a user pushing the button. The intent of the two
arcs is that whichever state the system is in, when the Push input is received
it goes to the other state.

One of the states is designated the “start state,” the state in which the
system is placed initially. In our example, the start state is off, and we conven-
tionally indicate the start state by the word Start and an arrow leading to that
state.

It is often necessary to indicate one or more states as “final” or “accepting”
states. Entering one of these states after a sequence of inputs indicates that
the input sequence is good in some way. For instance, we could have regarded
the state en in Fig. 1.1 as accepting, because in that state, the device being
controlled by the switch will operate. It is conventional to designate accepting
states by a double circle, although we have not made any such designation in
Fig. 11.

Example 1.2: Sometimes, what is remembered by a state can be much more
complex than an on/off choice. Figure 1.2 shows another finite automaton that
could be part of a lexical analyzer. The job of this automaton is to recognize


--- Page 20 ---
4 CHAPTER i, AUTOMATA: THE METHODS AND THE MADNESS

the keyword then. It thus needs five states. each of which represents a different
position in the word then that has been reached so far. These positions corre-
spond to the prefixes of the word, ranging from the empty string (i.e., nothing
of the word has heen seen so far) to the complete word.

Start

Figure 1.2: A finite automaton modeling recognition of then

In Fig. 1.2, the five states are named by the prefix of then seen so far. Inputs
correspord to letters. We may imagine that the lexical analyzer examines one
character of the program that it is compiling at a time, and the next character
to be examined is the input to the automaton. The start state corresponds to
the empty string, and each state las a transition on the next letter of then to
the state that corresponds to the uext-larger prefix. The state named then is
entered when the input has spelled the word then. Since it is the job of this
automaton to recognize when then has been seen, we could consider that state
the lone accepting state. O

1.1.2 Structural Representations

There are two important notations that are not automaton-like, but play an
important role in the study of automata and their applications.

l. Grammars are useful models when designing software that processes data
with a recursive structure. The best-known example is a “parser,” the
component of a compiler that deals with the recursively nested features
of the typical programming language, such as expressions — arithmetic,
conditional, and so on. For instance, a grammatical rule like FE > E+E
states that au expression can be formed by taking any two expressions
and connecting them bv a plus sign; this rule is typical of how expressions
of real programming languages are formed. We introduce context-free
grammars, as they are usually called, in Chapter 5.

2. Regular Ezpressions also denote the structure of data, especially text
strings. As we shall see in Chapter 3, the patterns of strings they describe
are exactly the same as what can be described by finite automata. The
style of these expressions differs significantly from that of grammars, and
we shall content ourselves with a simple example here. The UNIX-style
rogniar expression ? [A-Z] [a-z]*[ ][A-Z] [A-Z]? represents capitalized
words followed by a space and two capital letters. This expression rep-
resents patterns in text that could be a city and state, e.g., Ithaca NY.
[It misses multiword city names, such as Palo Alto CA, which could be
captured by the more complex expression


--- Page 21 ---
1.2. INTRODUCTION TO FORMAL PROOF 5
*¢CA-Z] [a-zJ*T J)*C J[A-Z] [A~-Z]’

When interpreting such expressions, we only need to know that [A-Z]
represents a range of characters from capital “A” to capital “Z™ (i¢., any
capital letter), and [ ] is used to represent the blank character alone.
Also, the symbol * represents “any number of” the preceding expression.
Parentheses are used to group components of the expression; they do not
represent characters of the text described.

1.1.3 Automata and Complexity

Automata are essential for the study of the limits of computation. As we
mentioned in the introduction to the chapter, there are two important issues:

1. What can a computer do at all? This study is called “decidability,” and
the problems that can be solved by computer are called “decidable.” This
topic is addressed in Chapter 9.

2. What can a computer do efficiently? This study is called “intractabil-
ity,” and the problems that can be solved by a computer using no more
time than some slowly growing function of the size of the input are called
“tractable.” Often, we take all polynomial functions to be “slowly grow-
ing,” while functions that grow faster than any polynomial are deemed to
grow too fast. The subject is studied in Chapter 10.

1.2 Introduction to Formal Proof

If you studied plane geometry in high school any time before the 1990's, you
most likely had to do some detailed “deductive proofs,” where you showed
the truth of a statement by a detailed sequence of steps and reasons. While
geometry has its practical side {e.g., yon need to know the rule for computing
the area of a rectangle if you need to buy the correct amount of carpet for a
room), the study of formal proof methodologies was at least as important a
reason for covering this branch of mathematics in high school.

In the USA of the 1990’s it became popular to teach proof as a matter
of personal feelings about the statement. While it is good to feel the truth
of a statement you need to use, important techniques of proof are no longer
mastered in high school. Yet. proof is something that every computer scientist
needs to understand. Some computer scientists take the extreme view that a
formal proof of the correctness of a program should go hand-in-hand with the
writing of the program itself. We doubt that doing so is productive. On the
other hand, there are those who say that proof has no place in the discipline of
programming. The slogan “if you are not sure your program is correct, run it
and see” is commonly offered by this camp.


--- Page 22 ---
6 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

Our position is between these two extremes. Testing programs is surely
essential. However, testing goes only so far, since you cannot try your program
on every input. More importantly, if your program is complex -~ say a tricky
recursion or iteration — then if you don’t understand what is going on as you
go around a loop or call a function recursively, it is unlikely that you will write
the cede correctly, When your testing tclls you the code is incorrect, you still
need to get it right.

To make your iteration or recursion correct, you need to set up an inductive
hypothesis, and it is helpful to reason, formally or informally, that the hypoth-
esis is consistent with the iteration or recursion. This process of understanding
the workings of a correct program is essentially the same as the process of prov-
ing theorems by induction. Thus, in addition to giving you wodels that are
useful for certain types of software, it has becomle traditional for a course on
automata theory to cover methodologies of formal proof. Perhaps more than
other core subjects of computer science, automata theory lends itself to natural
and interesting proofs, both of the deductive kind (a sequence of justified steps)
and the inductive kind (recursive proofs of a parameterized statement that use
the statement iiself with “lower” values of the parameter).

1.2.1 Deductive Proofs

As mentioned above, a deductive proof consists of a sequence of statements
whose truth leads us fromm some initial statement, called the hypothesis or the
given siatement({s), to a conclusion statement. Each step in the proof must
follow, by some accepted logical principle, from either the given facts, or some
of the previous statements in the deductive proof, or a combination of these.

The hypothesis may be true or false, typically depending on values of its
parameters. Often, the hypothesis consists of several independent statements
connected by a logical AND. In those cases, we talk of each of these statements
as a hypothesis, or as a given statement.

The theorem that is proved when we go from a hypothesis H to a conclusion
Cis the statement “if H then C.” We say that C is deduced from H. An example
theorem of the form “if H then C” will illustrate these points.

Theorem 1.3: If z > 4, then 27 >2°. O

Tt is not hard to convince ourselves informally that Theorem 1.3 is true,
although a formal proof requires induction and will be left for Example 1.17.
First, notice that the hypothesis # is “az > 4.” This hypothesis has a parameter,
x, and thus is neither true nor false. Rather, its truth depends on the value of
the parameter x; e.g., H is true for « = 6 and false for + = 2.

Likewise, the conclusion C is “27 > 2°.” This statement also uses parameter
x and is true for certain values of « and not others. For example, € is false for
x = 3, since 2% = 8, which is not as large as 34 = 9. On the other hand, C is
true for « = 4, since 24 = 4? = 16. For « = 5, the statement is also true, since
2° = 32 is at least as large as 5? = 25.



--- Page 23 ---
1.2. INTRODUCTION TO FORMAL PROOF 7

Perhaps you can see the intuitive argument that tells us the conclusion
2° > x? will be true whenever x > 4. We already saw that it is true for x = 4.
As x grows larger than 4, the left side, 2* doubles each time x increases by

1. However, the right side, 2?, grows by the ratio (2th), If « > 4, then

(x + 1)/x cannot be greater than 1.25, and therefore (£4 y cannot be bigger

than 1.5625. Since 1.5625 < 2, each time x increases above 4 the left side 2*
grows more than the right side 2?. Thus, as long as we start from a value like
x = 4 where the inequality 2° > 2? is already satisfied, we can increase 2 as
much as we like, and the inequality will still be satisfied.

We have now completed an informal but accurate proof of Theorem 1.3. We
shall return to the proof and make it more precise in Example 1.17, after we
introduce “inductive” proots.

Theorem 1.3, like all interesting theorems, involves an infinite number of
related facts, in this case the statement “if « > 4 then 2° > 2?” for all integers
z. In fact, we do not need to assume a is an integer, but the proof talked about
repeatedly increasing x by 1. starting at « = 4, so we really addressed only the
situation where # is an integer.

Theorem 1.3 can be used to help deduce other theorems. In the next ex-
ample, we consider a complete deductive proof of a simple theorem that uses
Theorem 1.3.

Theorem 1.4: If z is the sum of the squares of four positive integers, then
QT > x2.

PROOF: The intuitive idea of the proof is that if the hypothesis is true for z,
that is, © is the sum of the squares of four positive integers, then <« must be at
least 4. Therefore, the hypothesis of Theorem 1.3 holds, and since we believe
that theorem, we may state that its conclusion is also truc for «. The reasoning
can be expressed as a sequence of steps. Each step is either the hypothesis of
the theorem to be proved, part of that hypothesis, or a statement. that follows
from one or more previous statements.

By “follows” we mean that if the hypothesis of some theorem is a previous
statement, then the conclusion of that theorem is true, and cau be written down
ag a statement of our proof. This logical rule is often called modus ponens; i.¢.,
if we know H is true, and we know “if A then C” is true, we may conclude
that C' is true. We also allow certain other logical steps to be used in creating
a statement that follows from one or more previous statements. For instance,
if A and B are two previous statements, then we can deduce and write down
the statement “A and 8.”

Figure 1.3 shows the sequence of statements we need to prove Theorem 1.4.
While we shall not generally prove theorems in such a stylized form, it helps to
think of proofs as very explicit lists of statements, each with a precise justifica-
tion. In step (1), we have repeated one of the given statements of the theorem:
that x is the sum of the squares of four integers: It often helps in proofs if we
name quantitics that are referred to but not named, and we have done so here,
giving the four integers the names a. 6, ¢, and d.


--- Page 24 ---
2) CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

Statement | Justification
lj @=erthP+e +d Given
2+a2tlb>le>i;d>1 Given
3.] a? >1,;8 >1;¢ >1;d? >1 | (2) and properties of arithmetic
4.| 2>4 (1), (3), and properties of arithmetic
5. | 2% > x? (4) and Theorem 1.3

Figure 1.3: A formal proof of Theorem 1.4

In step (2), we put down the other part of the hypothesis of the theorem:
that the values being squared are each at least 1. Technically, this statement
represents four distinct statements, one for each of the four integers involved.
Then, in step (3) we observe that if a number is at least 1, then its square is
also at least 1. We use as a justification the fact that statement (2) holds, and
“properties of arithmetic.” That is, we assume the reader knows, or can prove
simple statements about how inequalities work, such as the statement “if y > 1,
then y? > 1.”

Step (4) uses statements (1) and (3). The first statement tells us that x is
the sum of the four squares in question, and statement, (3) tells us that each of
the squares is at least 1. Again using well-known properties of arithmetic, we
conclude that « is at least 1+1+1+1, or 4.

At the final step (5), we use statement (4), which is the hypothesis of Theo-
rem 1.3. The theorem itself is the justification for writing down its conclusion,
since its hypothesis is a previous statement. Since the statement (5) that is
the conclusion of Theorem 1.3 is also the conclusion of Theorem 1.4, we have
now proved Theorem 1.4. That is, we have started with the hypothesis of that
theorem, and have managed to deduce its conclusion. 0

1.2.2 Reduction to Definitions

In the previous two theorems, the hypotheses used terms that should have
been familiar: integers, addition, and multiplication, for instance. In many
other theorems, including many from automata theory, the terms used in the
statement may have implications that are less obvious. A useful way to proceed
in many proofs is:

e If you are not sure how to start a proof, convert all terms in the hypothesis
to their definitions.

Here is an example of a theorem that is simple to prove once we have ex-
pressed its statement in elementary terms. It uses the following two definitions:

1. A set S is finite if there exists an integer m such that S' has exactly n
elements. We write |{S|| =n, where ||S|| is used to denote the number


--- Page 25 ---
1.2. INTRODUCTION TO FORMAL PROOF 9

of elements in a set S. If the set S is not finite, we say S is infinite.
Intuitively, an infinite set is a set that contains more than any integer
number of clements.

3. If S and T are both subsets of some set U', then T is the complement of S
(with respect to U) if SUT =U and $™T = $. That is, each element
of U is in exactly one of S$ and T; put another way, T consists of exactly
those elements of U that are not in S.

Theorem 1.5: Let S be a finite subset of some infinite sect U'. Let T be the
complement of .S with respect to U. Then 7 is infinite.

PROOF: Intuitively, this theorem says that if you have an infinite supply of
something (U), and you take a finite amount. away (5), then you still have an
infinite amount left. Let us begin by restating the facts of the theorem as in
Fig. 1.4.

| Original Statement
S is finite

U is infinite

New Statement |

There is a integer n
such that ||5|| =
For no integer p

is [Ul = p

| T is the complement of $

SUT=U and SnT =?

Figure 1.4: Restating the giveus of Theorem 1.5

We are still stuck, so we need to use a common proof technique called “proof
by contradiction.” In this proof method, to be discussed further in Section 1.3.3,
we assume that the conclusion is false. We then use that assumption, together
with parts of the hypothesis, to prove the opposite of one of the given statements
of the hypothesis. We have then shown that it is impossible for all parts of the
hypothesis to be true and for the conclusion to be false at the same time.
The only possibility that remains is for the conclusion to be true whenever the
hypothesis is true. That is, the theorem is true.

In the case of Theorem 1.5, the contradiction of the conclusion is “T' is
finite.” Let us assume T is finite, along with the statement of the hypothesis
that says S is finite; ie., ||S|| = for some integer n. Similarly, we can restate
the assumption that T is finite as |/Z"|| = m for some integer mm.

Now one of the given statements tells us that SUT =U, and SOT = @.
That is, the elements of U are exactly the elements of S and T. Thus, there
must be 2 +m elements of U'. Since n + m is an integer, and we have shown
[U7 || = n+ m, it follows that U is finite. More precisely, we showed the number
of elements in U is some integer, which is the definition of “finite.” But the
statement that U is finite contradicts the given statement that U is infinite. We
have thus used the contradiction of our conclusion to prove the contradiction


--- Page 26 ---
10 CHAPTER I. AUTOMATA: THE METHODS AND THE MADNESS

of one of the given statements of the hypothesis, and by the principle of “proof
by contradiction” we may conclude the theorem is true. O

Proofs do not have to be so wordy. Having seen the ideas behind the proof,
let us reprove the theorem in a few lines.

PROOF: (of Theorem 1.5) We know that SUT =W and S and TF are disjoint,
su || S|] + [IPI] = WU]. Since S is finite, ||S]] = 2 for some integer n, and since U
is infinite, there is no integer p such that ||U|| =p. So assume that T is finite;
that is, {|7'|| = m for some integer m. Then ||U'|| = |S] + |[T |] = 2+ m, which
contradicts the given staternent that there is no integer p equal to ||U||. 0

1.2.3. Other Theorem Forms

The “if-then” form of theorem is most common in typical areas of mathematics.
However, we see other kinds of statements proved as theorems also. In this
section, we shall examine the most common forms of statement and what we
usually need to do to prove them.

Ways of Saying “IfThen”

First, there are a number of kinds of theorem statements that look different
from a simple “if H then C” form, but are in fact saying the same thing: if
hypothesis H is true for a given value of the parameter(s), then the conclusion
C is true for the same value. Here are some of the other ways in which “if H
then C” might appear.

1. H implies C.

2. A only if C.

3. Cif A.

4, Whenever 7 holds, C follows.

We also see many variants of form (4), such as “if A holds, then C follows,” or
“whenever H holds, C holds.”

Example 1.6: The statement of Theorem 1.3 would appear in these four forms
as:

1. 2 > 4 implies 2* > 2.
2, 2 >4 only if 2" > 27.
3. 2 > 2 ifs > 4.

4. Whenever x > 4, 2" > x? follows.


--- Page 27 ---
12. INTRODUCTION TO FORMAL PROOF 11

Statements With Quantifiers

Many theorems involve statements that use the quantifiers “for all” and
“there exists,” or similar variations, such as “for every” instead of “for all.”
The order in which these quantifiers appear affects what the statement
means. It is often helpful to see statements with more than one quantifier
as a “game” between two players — for-all and there-exists — who take
turns specifying values for the parameters mentioned in the theorem. “For-
all” must consider all possible choices, so for-all’s choices are generally left
as variables. However, “there-exists” only has to pick one value, which
may depend on the values picked by the players previously. The order in
which the quantifiers appear in the statement determines who gocs first.
If the last player to make a choice can always find some allowable value,
then the statement is truc.

For example, consider an alternative definition of “infinite set”: set S
is infinite if and only if for all integers n, there exists a subset T of S with
exactly n members. Here, “for-all” precedes “there-exists,” so we must
consider an arbitrary integer n. Now, “there-exists” gets to pick a subset
T, and may use the knowledge of n to do so. For instance, if S were the
set, of integers, “there-exists” could pick the subset T = {1,2,....n} and
thereby succeed regardless of n. That is a proof that the set of integers is
infinite.

The following statement looks like the definition of “infinite,” but is
incorrect because it reverses the order of the quantifiers: “there exists a
subset 7 of set S such that for all 1, set T has exactly nm members.” Now,
given a set S such as the integers, player “there-exists” can pick any set
T; say {1,2,5} is picked. For this choice, player “for-all” must show that
T has n members for every possible n. However, “for-all” cannot do so.
For instance, it is false for n — 4, or in fact for any n # 3.

In addition, in formal logic one often sees the operator + in place of “if
then.” That is, the statement “if H then C” could appear as H > C in some
mathematical literature; we shall not use it here.

Tf-And-Only-If Statements

Sometimes, we find a statement of the form “A if and only if B.” Other forms
of this statement are “A iff B,”!' “A is equivalent to B,” or “A exactly when
B.” This statement is actually two if-then statements: “if A then B,” and “if
B then A.” We prove “4 if and only if B” by proving these two statements:

lift, short for “if and only if,” ig a non-word that is used in some mathematical treatises
for succinctness.


--- Page 28 ---
12 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

How Formal Do Proofs Have to Be?

The answer to this question is not easy. The bottom line regarding proofs
is that their purpose is to convince someone, whether it is a grader of your
classwork or yourself, about the correctness of a strategy you are using in
your code. If it is convincing, then it is enough; if it fails to convince the
“consumer” of the proof, then the proof has left out too much.

Part. of the uncertainty regarding proofs comes from the different
knowledge that the consumer may have. Thus, in Theorem 1.4, we as-
sumed you knew all about arithmetic, and would believe a statement like
“ify > 1 then y? > 1.” If you were not familiar with arithmetic, we would
have to prove that statement by some steps in our deductive proof.

However, there are certain things that are required in proofs, and
omitting them surcly makes the proof inadequate. For instance, any de-
ductive proof that uses statements which are not justified by the given or
previous statenicats, carmot be adequate. When doing a proof of an “if
and only if” statement, we must surely have one proof for the “if” part and
another proof for the “only-if? part. As an additional example, inductive
proofs (discussed in Section 1.4) require proofs of the basis and induction
parts.

1. The ¢f pert: “if B then A,” and

2. The only-if part: “if A then B,” which is often stated in the equivalent
form “4 only if B.”

The proofs can be presented in either order. In many theorems, one part is
decidedly easier than the other, and it is customary to present the easy direction
first and get it out of the way.

Tn formnal logic, onc may see the operator © or = to denote an “if-and-only-
if” statement. That is, 4 = B and A @ B mean the same as “A if and only if
BS

When proving an if-and-only-if statement, it is important to remember that
you must prove both the “if” and “only-if” parts. Sometimes, you will find it
helpful to break an if-and-only-if into a succession of several equivalences. That
is. to prove “4 if and only if B,” you might first prove “4 if and only if C,” and
then prove “C if and only if B.” That method works, as long as you remember
that each if-and-only-if step must be proved in both directions. Proving any
one step in only one of the directions invalidates the entire proof.

The following is an cxample of a simple if-and-only-if proof. It uses the
notations:

1. |x], the floor of real mimber <, is the greatest integer equal to or less than
Rin


--- Page 29 ---
1.3. ADDITIONAL FORMS OF PROOF 13

2. [e|, the ceiling of real number «, is the least integer equal to or greatcr
than 2.

Theorem 1.7: Let z be areal number. Then |z] = [x] if and only if x ts an
integer.

PROOF: (Only-if part) In this part, we assume |z| = [2] and try to prove 2 is
an integer. Using the definitions of the floor and ceiling, we notice that |z| < 2,
and [x] > x. However, we are given that |x| = [z]. Thus, we may substitute
the floor for the ceiling in the first inequality to conclude [z] < 2. Since
both [x] < z and [xz] > g hold, we may conclude by properties of arithmetic
inequalities that [z] = z. Since [x] is always an integer, x must also be an
integer in this case.

(If part) Now, we assume x is an integer and try to prove |x| = [«]. This part
is easy. By the definitions of floor and ceiling, when x is an integer. both ||
and {2] are equal to x, and therefore equal to each other. O

1.2.4 Theorems That Appear Not to Be If-Then
Statements

Sometimes, we encounter a theorem that appears not to have a hypothesis. An
example is the well-known fact from trigonometry:

Theorem 1.8: sin’? @+cos?@=1. O

Actually, this statement does have a hypothesis, and the hypothesis consists
of all the statements you need to know to interpret the statement. In particular,
the hidden hypothesis is that @ is an angle, and therefore the functions sine
and cosine have their usual meaning for angles. From the definitions of these
terms, and the Pythagorean Theorem (in a right triangle, the square of the
hypotenuse equals the sum of the squares of the other two sides), you could
prove the theorem. In essence, the if-then form of the theorem is really: “if @
is an angle, then sin? ¢ + cos? @ = 1.”

1.3 Additional Forms of Proof

In this section, we take up several additional topics concerning how to construct
proofs:

1. Proofs about sets.
2. Proofs by contradiction.

3. Proofs by counterexample.


--- Page 30 ---
14 CHAPTER 1, AUTOMATA: THE METHODS AND THE MADNESS

1.3.1 Proving Equivalences About Sets

In automata theory, we are frequently asked to prove a theorem which says that
the sets constructed in two different ways are the same sets. Often, these sets
are sets of character strings, and the sets are called “languages,” but in this
section the nature of the sets is unimportant. If & and F are two expressions
representing sets, the statement # = F means that the two sets represented
are the same. More precisely, every element in the set represented by # is in
the set represented by F’, and every element in the set represented by F is in
the set represented by E.

Example 1.9: The commutative law of union says that we can take the union
of two sets R and S in either order. That is, RUS = SUR. In this case, F is
the expression # U S and F is the expression S UR. The commutative law of
union says that =F. O

We can write a set-equality & = F as an if-and-only-if statement: an element
xis in & if and only if ¢ is in F. As a consequence, we see the outline of a
proof of any statement that asserts the equality of two sets HE = F: it follows
the form of any if-and-only-if proof:

1. Proof that if # is in &, then z is in F.
2. Prove that if ¢ isin F, then x isin £.

As an example of this proof process, let us prove the distributive law of
union over intersection:

Theorem 1.10: RU(SONT)=(RUS)N(RUT)}.
PROOF: The two set-expressions involved are FE = RU(SMT) and
F=(RUS)N(RUT)

We shall prove the two parts of the theorem in turn. In the “if” part we assume
clement z is in # and show it is in F, This part, summarized in Fig. 1.5, uses
the definitions of union and intersection, with which we assume you are familiar.

Then, we must prove the “only-if” part of the theorem. Here, we assume
is in # and show it is in &. The steps are summarized in Fig. 1.6. Since we
have now proved both parts of the if-and-only-if statement, the distributive law
of union over intersection is proved. O

1.3.2 The Contrapositive

Every if-then statement has an equivalent form that in some circumstances is
easier to prove. The contrapositive of the statement “if H then C” is “if not C
then not 4." A statement and its contrapositive are either both true or both
false, so we can prove either to prove the other.

To see why “if H then C” and “if not C then not 4” are logically equivalent,
first observe that there are four cases to consider:


--- Page 31 ---
1.3. ADDITIONAL FORMS OF PROOF 15

| Statement Justification

.| visin RU(SNT) Given

2.| eisin Rorvisin SMT | (1) and definition of union

3. | visin & or x is in (2) and definition of intersection
both $ and F

tisn RUS (3) and definition of union

gisin RUT (3) and definition of union

risin(RUS)A(RUT) | (4), (5), and definition

of intersection

Figure 1.5: Steps in the “if” part of Theorem 1.10

|_| Statement | Justification
gisin{RUS)N(RUT) | Given
zgisin RUS (1) and definition of intersection
zisin RUT (1) and definition of intersection
zis in for is in (2), (3), and reasoning
both 5 and T about unions

(4) and definition of intersection
(5) and definition of union

cisin Rorzvisin SOT
risin RU(SNT}

Figure 1.6: Steps in the “only-if” part of Theorem 1.10

1. H and C both true.
2. HA true and C’' false.
3. C true and # false.

4. H and C both false.

There is only one way to make an if-then statement false; the hypothesis must
be true and the conclusion false, as in case (2}. For the other three cases,
including case (4) where the conclusion is false, the ifthen statement itself is
true.

Now, consider for which cases the contrapasitive “if not C then not H” is
false. In order for this statement to be false, its hypothesis (which is “not C”)
must be true, and its conclusion (which is “not H%) must be false. But “not
C” is true exactly when C is false, and “not H” is false exactly when # is true.
These two conditions are again case (2), which shows that in each of the four
cases, the original statement and its contrapositive are either both true or both
false; ie., they are logically equivalent.


--- Page 32 ---
16 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

Saying “If-And-Only-If” for Sets

As we mentioned, theorems that state equivalences of expressions about
sets are if-and-only-if statements. Thus, Theorem 1.10 could have been
stated: an element 2 is in RU (SMT) if and only if z is in

(RUS)N(RUT}

Another common expression of a set-equivaience is with the locution
“all-and-only.” For instance, Theorem 1.10 could as well have been stated
“the elements of R U (ST) are all and only the elements of

(RUSIN(RUT)

The Converse

Do not confuse the terms “contrapositive” and “converse.” The converse
of an if-then statement is the “other direction”; that is, the converse of “if
Af then C” is “if C then H.” Unlike the contrapositive, which is logically
equivalent to the original, the converse is not equivalent to the original
statement. In fact, the two parts of an if-and-only-if proof are always
some statement and its converse.

Example 1.11: Recall Theorem 1.3, whose statement was: “if c > 4, then
2* > 27.” The contrapositive of this statement is “if not 27 > x? then not
2 > 4," In more colloquial terms, making use of the fact that “not a > b” is
the same as a < 5, the contrapositive is “if 2* < a? thenx <4.” O

When we are asked to prove an if-and-only-if theorem, the use of the con-
trapositive in one of the parts allows us several options. For instance, suppose
we want to prove the set equivalence E = F. Instead of proving “if x is in #
then «isin F and if zis in F then z is in &,” we could also put one direction
in the contrapositive. One equivalent proof form is:

e Jfzisin & then z isin F, and if x is not in F then z is not in F.

We could also interchange £ and F in the statement above.

1.3.3. Proof by Contradiction

Another way to prove a statement of the form “if H then C” is to prove the
statement.


--- Page 33 ---
1.3. ADDITIONAL FORMS OF PROOF 17

e “FH and not C implies falsehood.”

That is, start by assuming both the hypothesis H and the negation of the
conclusion C. Complete the proof by showing that something known to be
false follows logically from H and not C. This form of proof is called proof by
contradiction.

Example 1.12: Recall Theorem 1.5, where we proved the if-then statement
with hypothesis H = “U is an infinite set, S is a finite subset of U, and 7 is
the complement of S with respect to U.” The conclusion C' was “T is infinite.”
We proceeded to prove this theorem by contradiction. We assumed “not C”;
that is, we assumed 7 was finite.

Our proof was to derive a falsehood from H and not C. We first showed
from the assumptions that S and T are both finite, that U also must be finite.
But since U is stated in the hypothesis H to be infinite, and a set cannot be
both finite and infinite, we have proved the logical statement “false.” In logical
terms, we have both a proposition p (U is finite) and its negation, not p (U
is infinite). We then use the fact that “p and not p” is logically equivalent to
“false.” O

To see why proofs by contradiction are logically correct, recall from Scc-
tion 1.3.2 that there are four combinations of truth values for H and C. Only
the second case, H true aud C false, makes the statement “if H then C” false.
By showing that H and not C leads to falsehood, we are showing that case 2
cannot occur. Thus, the only possible combinations of truth values for H and
C are the three combinations that make “if WH then C” true.

1.3.4 Counterexamples

In real life, we are not told to prove a theorem, Rather, we are faced with some-
thing that seems true-- a strategy for implementing a program for example - -
and we need to decide whether or not the “theorem” is true. To resolve the
question, we may alternately try to prove the theorem, and if we cannot, try to
prove that its statement is false.

Theorems generally are statements about an infinite number of cases, per-
haps all values of its parameters. Indeed, strict mathematical convention will
only dignify a statement with the title “theorem” if it has an infinite number
of cases; statements that have no parameters, or that apply to only a finite
number of values of its parameter(s) are called observations. It is sufficient to
show that an alleged theorem is false in any one case in order to show it is not a
theorem. The situation is analogous to programs, since a program is generally
considered to have a bug if it fails to operate correctly for even one imput on
which it was expected to work.

It often is easier to prove that a statement is not a theorem than to prove
it ig a theorem. As we mentioned, if S is any statement. then the statement
“S is not a theorem” is itself a statement without parameters, and thus can


--- Page 34 ---
18 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

be regarded as an observation rather than a theorem. The following are two
examples, first of an obvious nontheorem, and the second a statement that just
misses being a theorem and that requires some investigation before resolving
the question of whether it is a theorem or not.

Alleged Theorem 1.13: All primes are odd. (More formally, we might say:
if integer x is a prime, then =z is odd.)

DISPROOF: The integer 2 is a prime, but Ziseven. O

Now, let us discuss a “theorem” involving modular arithmetic. There is an
essential definition that we must first establish. If @ and 6 are positive integers,
then a mod bis the remainder when a is divided by 6, that is, the unique integer
r between 0 and 6 — 1 such that a = gb+ r for some integer g. For example,
& mod 3 = 2, and 9 mod 3 = 0. Our first proposed theorem, which we shall
determine to be false, is:

Alleged Theorem 1.14: There is no pair of integers a and 6 such that

amodé=5bmode

When asked to do things with pairs of objects, such as a and 6 here, it is
often possible to simplify the relationship between the two by taking advantage
of symmetry. In this case, we can focus on the case where a < 6, since if b <a
we can swap @ and 6 and get the same equation as in Alleged Theorem 1.14.
we must be careful, however, not to forget the third case, where a = 6. This
case turns out to be fatal to our proof attempts.

Let us assume a < 8. Then a mod b = a, since in the definition of a mod b
we have g = 0 andr =a. That is, when a < b we havea =~ Ox b+a. But
b mod a < a, since anything mod @ is between 0 and a — 1. Thus, when a < 8,
binod a < @ mod 4, so a mod 6 = 6 mod a is impossible. Using the argument
of symmetry above, we also know that @ mod b # 6 mod a when b < a.

However, consider the third case: a = 6. Since x mod z = 0 for any integer
x, we do have a mod 6 = b mod a if a = b. We thus have a disproof of the
alleged theorem:

DISPROOF: (of Alleged Theorem 1.14) Let a = 5 = 2. Then
amod6=bmoda=0
im

In the process of finding the counterexample, we have in fact discovered the
exact conditions under which the alleged theorem holds. Here is the correct
version of the theorem, and its proof.

Theorem 1.15: a mod b = 6 mod a if and only ifa = 5.


--- Page 35 ---
14. INDUCTIVE PROOFS 19

PROOF: (If part) Assume a = 6, Then as we observed above, x mod x = 0 for
any integer z. Thus, a mod 6 = 6 mod a = 0 whenever @ = 0.

(Only-if part) Now, assume a mod b = 6 mod a. The best technique is a
proof by contradiction, so assume in addition the negation of the conclusion;
that is, assume a # b. Then since a = 6 is eliminated, we have only to consider
the cases a < b and b < a.

We already observed above that when « < b, we have a mod b = a and
bh mod a < a. Thus, these statements, in conjunction with the hypothesis
a mod 6 = 5 mod a iets us derive a contradiction.

By symmetry, if b < a then b mod a = b and a mod b < 6. We again derive
a contradiction of the hypothesis, and conclude the only-if part is also true. We
have now proved both directions and conclude that the theorem is true. O

1.4 Inductive Proofs

There is a special form of proof, called “inductive,” that is essential when dealing
with recursively defined objects. Many of the most familiar inductive proofs
deal with integers, but in automata theory, we also need inductive proofs about
such recursively defined concepts as trees and expressions of various sorts, such
as the regular expressions that were mentioned briefly in Section 1.1.2. In this
section, we shall introduce the subject of inductive proofs first with “simple”
inductions on integers. Then, we show how to perform “structural” inductions
on any recursively defined concept.

1.4.1 Inductions on Integers

Suppose we are given a statement S(n), about an integer n, to prove. One
common approach is to prove two things:

1. The basis, where we show S(2) for a particular integer 7. Usually, 7 = 0
or i = 1, but there are examples where we want to start at some higher
i, perhaps because the statement S is false for a few small integers.

2. The inductive step, where we assume n > i, where 7 is the basis integer,
and we show that “if S(n} then S(n + 1).”

Intuitively, these two parts should convince us that S{r) is true for every
integer n that is equal to or greater than the basis integer 7. We can argue as
follows. Suppose S(n) were false for one or more of those integers. Then there
would have to be a smallest value of n, say j, for which $(j) is false, and yet
j > i, Now j could not be i, because we prove in the basis part that S(z) is
true. Thus, j must be greater than i. We now know that 7 —1 > i, and S(j— 1)
1s true.

However, we proved in the inductive part that ifn 2 1, then S(n) implies
S(n +1). Suppose we let mn = j — 1. Then we know from the inductive step
that S(j —1) implies S{j). Since we also know S(j — 1), we can conclude 5(j).


--- Page 36 ---
20 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

We have assumed the negation of what we wanted to prove; that is, we
assumed S(j) was false for some j > 7. In each case, we derived a contradiction,
so we have a “proof by contradiction” that S(n) is true for all n > 7.

Unfortunately, there is a subtle logical flaw in the above reasoning. Our
assumption that we can pick a least 7 > i for which S(7) is false depends on
our believing the principle of induction in the first place. That is, the only way
to prove that we can find such a 7 is to prove it by a method that is essentially
an inductive proof. However, the “proof” discussed above makes good intuitive
sense, and matches our understanding of the real world. Thus, we generally
take as un integral part of our logical reasoning system:

® The Induction Principle: If we prove S(i) and we prove that for all n > 4,
S(n) implies $(n +1), then we may conclude $(n) for all n > 2.

The following two examples illustrate the use of the induction principle to prove
theorems about integers.

Theorem 1.16: For ail n > 0:

PROOF: The proof js in two parts: the basis and the inductive step; we prove
each in turn.

BASIS: For the basis, we pick n = 0. It might seem surprising that the theorem
even makes sense for n = 0, since the left side of Equation (1.1) is a when
nm = 0. However, there is a general principle that when the upper limit of a sum
(0 in this case) is jess than the lower limit (1 here), the sum is over no terms
and therefore the sum is 0. That is, 7}_, 2? = 0.

The right side of Equation (1.1) is also 0, since 0 x (0+1) x (2x0+1)/6 = 0.
Thus, Equation (1.1) is true when n = 0.

INDUCTION: Now, assume nm > 0. We must prove the inductive step, that
Equation (1.1) implies the same formula with mn + 1 substituted for n. The
latter formula is

S a _ mtM(m ++ Qn ++) (1.2)

6

We may simplify Equations (1.1) and (1.2) by expanding the sums and products
on the right sides. These equations become:

> ? = (2n® + 3n? 4+ n)/6 (1,3)

i=1


--- Page 37 ---
1.4. INDUCTIVE PROOFS 21

mel

So? = (2n’ + 9n? + 13n + 6)/6 (1.4)

i=1
We need to prove (1.4} using (1.3), since in the induction principle, these are
statements S(m+ 1) and S(n}, respectively. The “trick” is to break the sum
to n+ 1 on the right of (1.4) into a sum to 7 plus the (n + Ist term. In that
way, we can replace the sum to n by the left side of (1.3) and show that (1.4)
is truce. These steps are as follows:

(S ) +(n+1)? = (2n? + 9n’ + 13n + 6)/6 (1.5)

t=1

(2n3 + 38n? + n)/6 + (n? + 2n +1) = (2n3 + On? + 13n + 6)/6 (1.6)

The final verification that (1.6) is true requires only simple polynomial algebra
on the left side to show it is identical to the right side. O

Example 1.17: In the next example, we prove Theorem 1.3 from Section 1.2.1.
Recall this theorem states that if z > 4, then 2° > x”. We gave an informal
proof based on the idea that the ratio «?/27 shrinks as « grows above 4. We
can make the idea precise if we prove the statement 27 > z? by induction on
z, starting with a basis of « = 4. Note that the statement is actually false for
a<4.

BASIS: If « = 4, then 27 and x? are both 16. Thus, 2* > 4? holds.

INDUCTION: Suppose for some x > 4 that 27 > z*. With this statement as
the hypothesis, we need to prove the same statement, with z + 1 in place of z,
that is, 2!*+1) > [2 + i]?. These are the statements S(z) and S(x +1) in the
induction principle; the fact that we are using z instead of n as the parameter
should not be of concern; 2 or 7 is just a local variable.

As in Theorem 1.16, we should rewrite S{z + 1) so it can make use of S(x).
In this case, we can write 27+ as 2 x 2°. Since S(z) tells us that 2* > 2, we
can conclude that 2°+! = 2 x 2% > 22’.

But we need something different; we need to show that 27t! > (2 + 1)*.
One way to prove this statement is to prove that 22? > {2 + 1)? and then use
the transitivity of > to show 27+! > 22? > (2 + 1)*. In our proof that

2x” > (x +1)? (1.7)
we may use the assumption that 2 > 4. Begin by simplifying (1.7):

g? > Qet] (1.8)
Divide (1.8) by a, to get:


--- Page 38 ---
22 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

Integers as Recursively Defined Concepts

We mentioned that inductive proofs are useful when the subject matter is
recursively defined. However, our first examples were inductions on inte-
gers, which we do not normally think of as “recursively defined.” However,
there is a natural, recursive definition of when a number is a nonnegative
integer, and this definition does indeed match the way inductions on inte-
gers proceed: from objects defined first, to those defined later.

BASIS: 0 is an integer.

INDUCTION: If n is an integer, then so is m+ 1.

r>2te (1.9)
a

Since z > 4, we know 1/z < 1/4. Thus, the left side of (1.9) is at least
4, and the right side is at most 2.25. We have thus proved the truth of (1.9).
Therefore, Equations (1.8) and (1.7) are also true. Equation (1.7) in turn gives
us 227 > (2 +1)? for x > 4 and lets us prove statement S(z + 1), which we
recall was 2°7' > (7 +1)?. O

1.4.2 More General Forms of Integer Inductions

Sometimes an inductive proof is made possible only by using a more general
scheme than the one proposed in Section 1.4.1, where we proved a statement $
for one basis value and then proved that “if S(m) then S(m+1).” Two important
generalizations of this scheme are:

1. We can use several basis cases. That is, we prove S(z), S(¢+ 1),.-., 8G)
for some 7} > i.

2, In proving S(n + 1), we can use the truth of all the statements
Si), SG+1),...,5(n)

rather than just using S(n). Moreover, if we have proved basis cases up
to S(j), then we can assume n > j, rather than just n > 4.

The conclusion to be made from this basis and inductive step is that S(7) is
true for all n > i,

Example 1.18: The following example will illustrate the potential of both
principles. The statement S(m} we would like to prove is that ifn > 8, then n
can be written as a sum of 3's and 5's. Notice, incidentally, that 7 cannot be
written as a sum of 3’s and 5’s.


--- Page 39 ---
1.4. INDUCTIVE PROOFS 23

BASIS: The basis cases are S(8), S(9), and S(10}. The proofs are 8 = 3+ 4,
9=3+3+3, and 10=5-+ 5, respectively.

INDUCTION: Assume that 2 > 10 and that 5(8),S(9),...,S(m)} are true. We
must prove S(n + 1) from these given facts. Our strategy is to subtract 3 from
n+ 1, observe that this number must be writable as a sum of 3's and 5’s, and
add one more 3 to the sum to get a way to write n+ 1.

More formally, observe that n — 2 > 8, so we may assume S(n — 2). That
is, n — 2 = 3a + 5b for some integers a and &. Then » + 1 = 3+ 3a +4 58, so
n+ 1 can be written as the sum of a+ 1 3’s and 6 5’s. That proves S(n + 1)
and concludes the inductive step. O

1.4.3 Structural Inductions

In automata theory, there are several recursively defined structures about. which
we need to prove statements. The familiar notions of trees and expressions
are important examples. Like inductions, all recursive definitions have a basis
case, where one or more elementary structures are defined, and an inductive
step, where more complex structures are defined in terms of previously defined
structures.

Example 1.19: Here is the recursive definition of a tree:
BASIS: A single node is a treo, and that node is the root of the tree.

INDUCTION: If T), 72,..., 7% are trees, then we can form a new tree as follows:

1. Begin with a new node N, which is the root of the tree.
2, Add copies of all the trees T), 7 %,..., 7}.
3. Add edges from node N to the roots of each of the trees T,, T2,..., Tr.

Figure 1.7 shows the inductive construction of a tree with root N from & smaller
trees. O

Example 1.20: Here is another recursive definition. This time we define
expressions using the arithmetic operators + and *, with both numbers and
variables allowed as operands.

BASIS: Any number or letter {i-e., a variable) is an expression.
INDUCTION: If E and F are expressions, then so are B+ F, E+ F, and (£2).

For example, both 2 and z are expressions by the basis. The inductive step
tells us 2 +2, (2 +2), and 2 * {z + 2) are all expressions. Notice how each of
these expressions depends on the previous ones being expressions. O


--- Page 40 ---
24 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

W)

Figure 1.7: Inductive construction of a tree

Intuition Behind Structural Induction

We can suggest informally why structural induction is a valid proof
method. Imagine the recursive definition establishing, one at a time, that
certain structures X,, X2,... meet the definition. The basis elements come
first, and the fact that X; is in the defined set of structures can only depend
ou the membership in the defined sct of structures that precede X; on the
list. Viewed this way, a structural induction is nothing but an induction
on integer n of the statement S(X,). This induction may be of the gen-
eralized form discussed in Section 1.4.2, with multiple basis cases and an
inductive step that uses all previous instances of the statement. However,
we should remember, as explained in Section 1.4.1, that this intuition is
not a formal prouf, and in fact, we must assume the validity this induction
principle as we did the validity of the original induction principle of that
section.

When we have a recursive definition, we can prove theorerns about it using
the following proof form, which is called structural induction. Let S(X) be a
statement about the structures X that are defined by some particular recursive
definition.

1. As a basis, prove S(X) for the basis structure(s) X.

2. For the inductive step, take a structure Y that the recursive defini-
tion says is formed from Yi,¥3,...,¥,. Assume that the statements
5(¥1), 5(¥2),...,S(¥,), and use these to prove S(X).

Our conclusion is that S(X) is true for all X¥. The next two theorems are
examples of facts that can be proved about trees and expressions.

Theorem 1.21: Every tree has one more node than it has edges.


--- Page 41 ---
14. INDUCTIVE PROOFS 25

PROOF: The formal statement 5S(J) we need to prove by structural induction
is: “if T is a tree, and T has n nodes and e edges, then n =e + 1.”

BASIS: The basis case is when T is a single node. Then n = 1 and e = 0, so
the relationship n = ¢ + 1 holds.

INDUCTION: Let T be a tree built by the inductive step of the definition,
from root node N and & smaller trees 1), 72,...,7;. We may assume that the
statements S(T;) hold for i= 1,2,...,%. That is, let 7; have mn, nodes and e;
edges; then nj =e; +1.

The nodes of T are node N and all the nodes of the T;’s. There are thus
1L+n, +o +---+ mn, nodes in T. The edges of T are the & edges we added
explicitly in the inductive definition step, plus the edges of the J;’s. Hence, T
has

Rte tegt-:+ ter (1.10)

edges. If we substitute e; + 1 for n,; in the count of the number of nodes of T
we find that T has

L+fer.+1)+leetij+---+le. +4) (1.11}

nodes. Since there are & of the “+1” terms in (1.10), we can regroup (1.11) as

k+tlterptent-:-+e (1.12)

This expression is exactly 1 more than the expression of (1.10) that was given
for the number of edges of T. Thus, T' has one more node than it has edges.
Oo

Theorem 1.22: Every expression has an equal number of left and right paren-
theses.

PROOF: Formally, we prove the statement S(G) about any expression G that
is defined by the recursion of Example 1.20: the numbers of left and right
parentheses in G are the same.

BASIS: If G is defined by the basis, then G is a number or variable. These
expressions have 0 left parentheses and 0 right parentheses, so the numbers are
equal.

INDUCTION: There are three rules whereby expression G may have been con-
structed according to the inductive step in the definition:

1 G=E+F.
3. G=(E).


--- Page 42 ---
96 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

We may assume that S(£) and S(/) are true; that is, B has the same number
of left and right parentheses, say n of each, and F likewise has the same number
of left and right parentheses, say m of each. Then we can compute the numbers
of left and right parentheses in G for each of the three cases, as follows:

1. G = E+ F, then G has n +m left parentheses and n + m right
parentheses; n of each come from £ and m of each come from F’.

2. If G = Ex F, the count of parentheses for G is again n + m of each, for
the same reason as in case (1).

3. IfG = (#), then there are n+] left parentheses in G — onc left parenthesis
is explicitly shown, and the other are present in &, Likewise, there are
n +t right parentheses in G; one is explicit and the other n are in &.

In each of the three cases, we see that the numbers of left and right parentheses
in G are the same. This observation completes the inductive step and completes
the proof. 0

1.4.4 Mutual Inductions

Sometimes, we cannot prove a single statement by induction, but rather need
to prove a group of statements $1(m),52(m),.-.,S(m) together by induction
on n. Automata theory provides many such situations. In Example 1.23 we
sample the common situation where we need to explain what an automaton
does by proving a group of statements, one for each state. These statements
tell under what sequences of inputs the automaton gets into cach of the states.

Strictly speaking, proving a group of statements is no different from proving
the conjunction (logical AND) of all the statements. For instance, the group
of statements S;(n), S2(n),...,5,(n) could be replaced by the single statement
$1 (n) AND So(n) AND --- AND S,(n). However, when there are really several inde-
pendent statements to prove, it is generally less confusing to keep the statements
separate and to prove them all in their own parts of the basis and inductive
steps. We call this sort of proof mutual induction. An example will illustrate
the necessary steps for a mutual recursion.

Example 1.23: Let us revisit the on/off switch, which we represented as an
automaton in Example 1.1. The automaton itself is reproduced as Fig. 1.8.
Since pushing the button switches the state between on and off, and the switch
starts out in the off state, we expect that the following statements will together
explain the operation of the switch:

$1(n): The automaton is in state off after n pushes if and only if n is even.

So{n): The automaton is in state on after n pushes if and only if n is odd.


--- Page 43 ---
14. INDUCTIVE PROOFS 27

Push

Sen) Cw)

Push

Figure 1.8: Repeat of the automaton of Fig. 1.1

We might suppuse that S; implies $8, and vice-versa, since we know that
a number 7 cannot be both even and odd. However, what is not always true
about an automaton is that it is in one and only one state. It happens that
the automaton of Fig. 1.8 is always in exactly one state, but that fact must be
proved as part of the mutual induction.

We give the basis and inductive parts of the proofs of statements 5S) (n) and
52(n)} below. The proofs depend on several facts about odd and even integers:
if we add or subtract 1 from an even integer, we get an odd integer, and if we
add or subtract 1 from an odd integer we get an even integer.

BASIS: For the basis, we choose n = 0. Since there are two statements, each of
which must be proved in both directions (because 5, and S2 are each “if-and-
only-if” statements), there are actually four cases to the basis, and four cases
to the induction as well.

1. [5\; If] Since 0 is in fact even, we must show that after 0 pushes, the
automaton of Fig. 1.8 is in state off. Since that is the start state, the
automaton is indeed in state off after 0 pushes.

2. [S}; Only-if] The automaton is in state off after 0 pushes, so we must
show that @ is even. But f is even by definition of “even,” so there is
nothing more to prove.

3. [So; If] The hypothesis of the “if” part of S2 is that 0 is odd. Since this
hypothesis H is false, any statement of the form “if H then C” is true, as
we discussed in Section 1.3.2. Thus, this part of the basis also holds.

4, [S2; Only-if] The hypothesis, that the automaton is in state on after 0
pushes, is also false, since the only way to get to state on is by following
an arc labeled Push, which requires that the button be pushed at least
once. Since the hypothesis is false, we can again conclude that the if then
statement is true.

INDUCTION: Now, we assume that S(m) and S2(n) are true, and try to prove
Si(n +1) and S2(n +1). Again, the proof separates into four parts.


--- Page 44 ---
28

CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

1. [S\(m + 1); If] The hypothesis for this part is that n+ 1 is even. Thus,

nis odd. The “if” part of statement S (nm) says that after 2 pushes, the
automaton is in state on. The arc from on to off labeled Push tells us
that the (n+ 1)st push will cause the automaton to enter state off That
completes the proof of the “if” part of Sy(n+ 1).

2. [Si{m + 1}; Only-if] The hypothesis is that the automaton is in state off

after n +1 pushes. Inspecting the automaton of Fig. 1.8 tells us that the
only way to get to state off after one or more moves is to be in state on and
receive an input Push. Thus, if we are in state off after n + 1 pushes, we
must have been in state on after m pushes. Then, we may use the “only-if”
part of statement S2(n) to conclude that n is odd. Consequently, 2 + 1 is
even, which is the desired conclusion for the only-if portion of S,(n + 1).

3. [So{2+1); If] This part is essentially the same as part (1), with the roles of

statements S; and Sz exchanged, and with the roles of “odd” and “even”
exchanged. The reader should be able to construct this part of the proof
easily.

4. [S2(r +1); Only-if] This part is essentially the same as part (2}, with the

roles of statements 5S, and Sy exchanged, and with the roles of “odd” and
“even” exchauged.

We can abstract from Example 1.23 the pattern for all mutual inductions:

« Each of the statements must be proved separately in the basis and in the
inductive step.

« If the statements are “if-and-only-if,” then both directions of each state-
ment must be proved, both in the basis and in the induction.

1.5 The Central Concepts of Automata Theory

In this section we shall introduce the most important definitions of terms that
pervade the theory of automata. These concepts include the “alphabet” (a sct
of symbols), “strings” (a list of symbols from an alphabet), and “language” (a
set of strings from the same alphabet).

1.5.1 Alphabets

An alphabet is a finite, nonempty set of symbols. Conventionally, we use the
symbol © for an alphabet. Common alphabets include:

1, © = {0,1}, the binary alphabet.


--- Page 45 ---
1.5. THE CENTRAL CONCEPTS OF AUTOMATA THEORY 29

2. S= {a,d,...,z}, the set of all lower-case letters.

3. The set of all ASCII characters, or the set of all printable ASCIT charac-
ters.

1.5.2 Strings

A string (or sometimes word) is a finite sequence of symbols chosen from some
alphabet. For example, 01101 is a string from the binary alphabet © = {6,1}.
The string 111 is another string chosen from this alphabet.

The Empty String

The empty string is the string with zero occurrences of symbols. This string,
denoted ¢, is a string that may be chosen from any alphabet whatsoever.

Length of a String

It is often useful to classify strings by their length, that is, the number of
positions for symbols in the string. For instance, 01101 has length 3. It is
common to say that the length of a string is “the number of symbols” in the
string; this statement is colloquially accepted but not strictly correct. Thus,
there are only two symbols, 0 and 1, in the string 01101, but there are five
positions for symbols, and its length is 5. However, you should generally expect
that “the number of symbols” can be used when “number of positions” is meant.

The standard notation for the length of a string w is |w|. For example,
|O11| = 3 and |e| = 0.

Powers of an Alphabet

If © is an alphabet, we can express the set of all strings of a certain length from
that alphabet by using an exponential notation. We define E* to be the set of
strings of length k, each of whose symbols is in ©.

Example 1.24: Note that 5° = {e}, regardless of what alphabet & is. That
is, € is the only string whose length is 0.
if © = {0,1}, then Zt = {0,1}, ©? = {00,01, 10, 11},

E5 = {000, 001, 010, O11, 100, 101, 110, 111}

and so on. Note that there is a slight confusion between © and U!. The former
is an alphabet; its members 0 and 1 are symbols. The latter is a set of strings;
its members are the strings 0 and 1, each of which is of length 1. We shall not
try to use separate notations for the two sets, relying on context to make it
clear whether {0,1} or similar sets are alphabets or sets of strings. O


--- Page 46 ---
30 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

Type Convention for Symbols and Strings

Commonly, we shall use lower-case letters at the beginning of the alphabet
(or digits} to denote symbols, and lower-case letters near the end of the
alphabet, typically w, z, y, and z, to denote strings. You should try to get
used to this convention, to heip remind you of the types of the elements
being discussed.

The set of all strings over an alphabet © is conventionally denoted E*. For
instance, {0,1}* = {e,0,1, 00,01, 10, 11,000,...}. Put another way,

wr=rfPurluDu..--

Sometimes, we wish to exclude the empty string from the set. of strings. The
set of nonempty strings from alphabet ¥ is denoted £+. Thus, two appropriate
equivalences are:

e Sta] 'uYrreuriu---.

e S* = Dt u fe}.

Concatenation of Strings

Let x and y be strings. Then zy denotes the concatenation of x and y, that
is, the string formed by making a copy of # and following it by a copy of y.
More preciscly, if z is the string composed of ¢ symbols # = a1ag +--+ a; and y is
the string composed of j symbols y = 6,4. ---6;, then zy is the string of length
d+: ey = aydy--- abby ree Dy,

Example 1.25: Let s = 01101 and y = 110. Then wy = 01101110 and
ye = 11001101. For any string w, the equations ew = we = w hold. That is,
€ is the identity for concatenation, since when concatenated with any string it
yields the other string as a result (analogously to the way 0, the identity for
addition, can be added to any number =z and yields x as a result). ©

1.5.3 Languages

A set of strings all of which are chosen from some &*, where © is a particular
alphabet, is called a language. If © is an alphabet, and £ C d*, then Lisa
language over . Notice that a language over © need not include strings with
all the symbols of E, so once we have established that ZL is a language over ©,
we also know it is a language over any alphabet that is a superset of L.

The choice of the term “language” may seem strange. However, common
languages can be viewed as sets of strings. An example is English, where the


--- Page 47 ---
1.5. THE CENTRAL CONCEPTS OF AUTOMATA THEORY 3]

collection of legal English words is a set of strings over the alphabet that consists
of all the letters. Another example is C, or any other programming language,
where the legal programs are a subset of the possible strings that can be formed
from the alphabet of the language. This alphabet is a subset of the ASCII
characters. The exact alphabet may differ slightly among different programming
languages, but generally includes the upper- and lower-case lotters, the digits,
punctuation, and mathematical symbols.

However, there are also many other languages that appear when we study
automata. Some are abstract examples, such as:

1. The language of all strings consisting of n 0's followed by n 1’s, for some
n> 0: {e,01,0011,000111,...}.

2. The set of strings of 0’s and 1’s with an equal number of each:

fe, 01, 10,0011, 0101, 1001, .. .}

3. The set of binary numbers whose value is a prime:

{10, 11,101,114, 1011,..}
4. ©" is a language for any alphabet ©.
5. 6, the empty language, is a language over any alphabet.

6. {e}, the language consisting of only the empty string, is also a language
over any alphabet. Notice that @ 4 {e}; the former has no strings and
the latter has one string.

The only important constraint on what can be a language is that ali alphabets
are finite. Thus languages, although they can have an infinite number of strings,
are restricted to consist of strings drawn from one fixed, finite alphabet.

1.5.4 Problems

In automata theory, a problem is the question of deciding whether a given string
is a member of some particular language. It turns out, as we shall see, that
anything we more colloquially call a “problem” can be expressed as membership
in a language. More precisely, if © is an alphabet, and L is a language over ¥,
then the problem ZL is:

e Given a string w in &*, decide whether or not w is in L.

Example 1.26: The problem of testing primality can be expressed by the
language L, consisting of all binary strings whose value as a binary number is
a prime. That is, given a string of 0’s and 1’s, say “yes” if the string is the
binary representation of a prime and say “no” if not. For some strings, this


--- Page 48 ---
32 CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

Set-Formers as a Way to Define Languages

it is common to describe a language using a “set-former”:
{w | something about w}

This expression is read “the set of words w such that (whatever is said
about w to the right of the vertical bar).” Examples are:

1. {w | w consists of an equal number of 0’s and 1's }.
2. {w | w is a binary integer that is prime }.
3. {w }| w is a syntactically correct C program }.

It is also common to replace w by some expression with parameters and
describe the strings in the language by stating conditions on the parame-
ters. Here are some examples; the first with parameter n, the second with
parameters 7 and 7:

1. {071" |n > 1}. Read “the set of 0 to the n 1 to the m such that n
is greater than or equal to 1,” this language consists of the strings
{01,0011,000111,...}. Notice that, as with alphabets, we can raise
a single symbol to a power n in order to represent n copies of that
symbol,

. {0°17 | 0 <i < j}. This language consists of strings with some 0’s
(possibly none) followed by at least as many 1’s.

decision is easy. For instance, 0011101 cannot be the representation of a prime,
for the simple reason that every integer except 0 has a binary representation
that begins with 1. However, it is less obvious whether the string 11101 belongs
to Ly, so any sohition to this problem will have to use significant computational
resources of some kind: time and/or space, for example. O

One potentially unsatisfactory aspect of our definition of “problem” is that
one commonly thinks of problems not as decision questions (is or is not the
following true?) but as requests to compute or transform some input (find the
best way to do this task). For instance, the task of the parser in a C compiler
can be thought of as a problem in our formal sense, where one is given an ASCII
string and asked to decide whether or not the string is a member of L,, the set
of valid C programs. However, the parser does more than decide. It produces a
parse tree, entries in a symbol table and perhaps more. Worse, the compiler as
a whole solves the problem of turning a C program into object code for some


--- Page 49 ---
1.5. THE CENTRAL CONCEPTS OF AUTOMATA THEORY 33

Is It a Language or a Problem?

Languages and problems are really the same thing. Which term we prefer
to use depends on our point of view. When we care only about strings for
their own sake, e.g., in the set {071" {| 2 > 1}, then we tend to think of
the set of strings as a language. In the last chapters of this book, we shall
tend to assign “semantics” to the strings, ¢.g., think of strings as coding
graphs, logical expressions, or even integers. In those cases, where we care
more about the thing represented by the string than the string itself, we
shall tend to think of a set of strings as a problem.

machine, which is far from simply answering “yes” or “no” about the validity
of a program.

Nevertheless, the definition of “problems” as languages has stood the test
of time as the appropriate way to deal with the important questions of com-
plexity theory. In this theory, we are interested in proving lower bounds on
the complexity of certain problems. Especially important are techniques for
proving that certain problems cannot be solved in an amount of time that is
less than exponential in the size of their input. It turns out that the yes/no
or language-based version of known problems arc just as hard in this sense, as
their “solve this” versions.

That is, if we can prove it is hard to decide whether a given string belongs to
the language Ly of valid strings in programming language X, then it stands to
reason that it will not be easier to translate programs in language X to object
code. For if it were easy to generate code, then we could run the translator, and
conclude that the input was a valid member of Ly exactly when the translator
succeeded in producing object code. Since the final step of determining whether
object code has been produced cannot be hard, we can use the fast algorithm
for generating the object code to decide membership in Lx efficiently. We thus
contradict the assumption that testing membership in Lx is hard. We have a
proof by contradiction of the statement “if testing membership in Lx is hard,
then compiling programs in programming language X is hard.”

This technique, showing one problem hard by using its supposed efficient
algorithm to solve efficiently another problem that is already known to be hard,
is called a “reduction” of the second problem to the first. It is an essential tool
in the study of the complexity of problems, and it is facilitated greatly by our
notion that problems are questions about membership in a language, rather
than more general kinds of questions.


--- Page 50 ---
34

1.6

+

CHAPTER 1. AUTOMATA: THE METHODS AND THE MADNESS

Summary of Chapter 1

Finite Automata: Finite automata involve states and transitions among
states in response to inputs. They are useful for building several different.
kinds of software, including the lexical analysis component of a compiler
and systems for verifying the correctness of circuits or protocols, for ex-
ample.

Regular Expressions: These are a structural notation for describing the
same patterns that can be represented by finite antomata. They are used
in many common types of software, including tools to search for patterns
in text or in file names, for instance.

Contezt-Free Grammars: These are an important notation for describing
the structure of programming languages and related sets of strings; they
are used to build the parser component of a compiler.

Turing Machines: These are automata that model the power of real com-
puters. They allow us to study decidabilty, the question of what can or
cannot be done by a computer. They also let us distinguish tractable
problems — those that can be solved in polynomial time — from the
intractable problems — those that cannot.

Deductive Proofs: This basic method of proof proceeds by listing state-
ments that are either given to be true, or that follow logically from some
of the previous statements.

Proving if-Then Statements: Many theorems are of the form “if (some-
thing) then (something else).” The statement or statements following the
“if” are the hypothesis, and what. follows “then” is the conclusion. Deduc-
tive proofs of if-then statements begin with the hypothesis, and continue
with statements that follow logically from the hypothesis and previous
statements, until the conclusion is proved as one of the statements.

Proving If And-Only-If Statements: There are other theorems of the form
“(something) if and only if (something else).” They are proved by showing
ifthen statements in both directions. A similar kind of theorem claims
the equality of the sets described in two different ways; these are proved
by showing that each of the two sets is contained im the other.

Proving the Contrapositive: Sometimes, it is easier to prove a statement
of the form “if H then C” by proving the equivalent statement: “if not
C then not #.” The latter is called the contrapositive of the former.

Proof by Contradiction: Other times, it is more convenient to prove the
statement “if H then C” by proving “if H and not C then (something
known to be false).” A proof of this type is called proof by contradiction.


--- Page 51 ---
1.7. REFERENCES FOR CHAPTER 1 35

+ Counterexamples: Sometimes we are asked to show that a certain state-
ment is not true. If the statement has one or more parameters, then we
can show it is false as a generality by providing just one counterexam-
ple, that is, one assignment of values to the parameters that makes the
statement false.

+ Inductive Proofs: A statement that has an integer parameter n can offen
by proved by induction on n. We prove the statement is true for the
basis, a finite number of cases for particular values of 7, and then prove
the inductive step: that if the statement is true for values up to n, then
it is true for n+ 1.

+ Structural inductions: In some situations, including many in this book,
the theorem to be proved inductively is about some recursively defined
construct, such as trees. We may prove a theorem about the constructed
objects by induction on the number of steps used in its construction. This
type of induction is referred to as structural.

+ Alphabets: An alphabet is any finite set of symbols.
+ Strings: A string is a finite-length sequence of symbols.

+ Languages and Problems: A language is a (possibly infinite) set of strings,
all of which choose their symbols from some one alphabet. When the
strings of a language are to be interpreted in some way, the question of
whether a string is in the language is sometimes called a problem.

1.7 References for Chapter 1

For extended coverage of the material of this chapter, including mathematical
concepts underlying Computer Science, we recommend [1].

1. A.V. Aho and J. D. Ullman, Foundations of Computer Science, Computer
Science Press, New York, 1994.


--- Page 52 ---


--- Page 53 ---
Chapter 2

Finite Automata

This chapter introduces the class of languages known as “regular languages.”
These languages are exactly the ones that can be described by finite automata,
which we sampled briefly in Section 1.1.1. After an extended exaniple that will
provide motivation for the study to follow, we define finite automata formally.

As was mentioned earlier, a finite automaton has a set of states, and its
“control” moves from state to state in response to external “inputs.” One of
the crucial distinctions among classes of finite automata is whether that con-
trol is “deterministic,” meaning that the automaton cannot be in more than
one state at any one time, or “nondeterministic,” meaning that it may be in
several states at once. We shall discover that adding nondeterminism does
not let us define any language that cannot be defined by a deterministic finite
automaton, but there can be substantial efficiency in describing an application
using a nondeterministic automaton. In effect, nondeterminism allows us to
“program” solutions to problems using a higher-level language. The nondeter-
ministic finite automaton is then “compiled,” by an algorithm we shall learn
in this chapter, into a deterministic automaton that can be “executed” on a
conventional computer.

We conclude the chapter with a study of an extended nondeterministic aut-
omaton that has the additional choice of making a transition from one state to
another spontancously, i.e., on the empty string as “input.” These automata
also accept nothing but the regular languages. However, we shall find them
quite important in Chapter 3, when we study regular expressious and their
equivalence to automata.

The study of the regular languages continues in Chapter 3. There, we in-
troduce another important way to describe regular languages: the algebraic
notation known as regular expressions. After discussing regular expressions,
and showing their equivalence to finite automata, we use both automata and
regular expressions as tools in Chapter 4 to show certain important properties
of the regular languages. Examples of such properties are the “closure” proper-
ties, which allow us to claim that one language is regular because one or more

37


--- Page 54 ---
38 CHAPTER 2. FINITE AUTOMATA

other languages are known to be regular, and “decision” properties. The latter
are algorithms to answer questions about automata or regular expressions, 6.2;
whether two automata or expressions represent the same language.

2.1 An Informal Picture of Finite Automata

In this section, we shall study an extended cxample of a real-world problem
whose solution uses finite automata in an important role. We investigate pro-
tocols that support “electronic money” - files that a customer can use to pay
for goods on the internet, and that the seller can receive with assurance that
the “money” is real. The seller must know that the file has not been forged,
nor has it been copied and seut to the seller, while the customer retains a copy
of the same file to spend again.

The nonforgeability of the file is something that must be assured by a bank
and by a cryptography policy. That is, a third player, the bank, must issue and
encrypt the “money” files, so that forgery is not a problem. However, the bank
has a second important job: it must keep a database of all the valid money
that it has issued, so that it can verify to a store that the file it has received
represents real money and can be credited to the store’s account. We shall not
address the cryptographic aspects of the problem, nor shall we worry about
how the bank can store and retrieve what could be billions of “electronic dollar
bills.” These problems are not likely to represent long-term impediments to the
concept of electronic money, and examples of its small-scale use have existed
since the late 1990°s.

However, in order to use electronic money, protocols need to be devised to
allow the manipulation of the money in a variety of ways that the users want.
Because monetary systems always invite fraud, we must verify whatever policy
we adopt regarding how money is used. That is, we need to prove the only
things that can happen are things we intend to happen -—~— things that do not
allow an unscrupulous user to steal from others or to “manufacture” moncy.
In the balance of this section, we shall introduce a very simple example of a
(bad) clectronic-money protocol, model it with finite automata, and show how
constructions on automata can be used to verify protocols (or, in this case, to
discover that the protocol has a bug).

2.1.1 The Ground Rules

There are three participants: the customer, the store, and the bank. We assume
for simplicity that there is only one “money” file in existence. The customer
may decide to transfer this money file to the store, which will then redeem the
file from the bank (i.e., get the bank to issue a new money file belonging to the
store rather than the customer) and ship goods to the customer. In addition,
the customer has the option to cancel the file. That is, the customer may ask
the bank to place the money back in the customer’s account, making the money


--- Page 55 ---
2.1. AN INFORMAL PICTURE OF FINITE AUTOMATA 39

no longer spendable. Interaction among the three participants is thus limited
to five events:

1. The customer may decide to pay. That is, the customer sends the money
to the store.

9. The customer may decide to cancel. The money is sent to the bank with
a message that the value of the money is to be added to the customer’s
bank account.

3. The store may ship goods to the customer.

4. The store may redeem the money. That is, the money is sent to the bank
with a request that its value be given to the store.

5. The bank may transfer the money by creating a new, suitably encrypted
money file and sending it to the store,

2.1.2 The Protocol

The three participants must design their behaviors carefully, or the wrong things
may happen. In our example, we make the reasonable assumption that the
customer cannot be relied wpon to act responsibly. In particular, the customer
may try to copy the money file, use it to pay several times, or both pay and
cancel the money, thus getting the goods “for free.”

The bank must behave responsibly, or it cannot be a bank. In particular, it
must make sure that two stores cannot both redeem the same money file, and
it must not allow money to be both canceled and redeemed. The store should
be careful as well. In particular, it should not ship goods until it is sure it has
been given valid money for the goods.

Protocols of this type can be represented as finite automata. Each state
represents a situation that one of the participants could be in. That is, the state
“ramembers” that certain important events have happened and that others have
not yet happened. Transitions between states occur when one of the five events
described above occur. We shall think of these events as “external” to the
automata representing the three participants, even though each participant is
responsible for initiating one or more of the events. It turns out that what is
important about the problem is what sequences of events can happen, not who
is allowed to initiate them.

Figure 2.1 represents the three participants by automata. In that diagram,
we show only the events that affect a participant. For example, the action pay
affects only the customer and store. The bank does not know that the money
has been sent by the customer to the store; it discovers that fact only when the
store executes the action redeem.

Let us examine first the automaton (c) for the bank, The start state is
state 1; it represents the situation where the bank has issued the money file in
question but. has not been requested either to redeem it or to cancel it. Ifa


--- Page 56 ---
40 CHAPTER 2. FINITE AUTOMATA

Start pay redeem transfer
a
ship
{a} Store
redeem transfer
cancel
cancel
redeem transfer
Start Start
(b) Customer (c) Bank

Figure 2.1: Finite automata representing a customer, a store, and a bank

cancel request is sent to the bank by the customer, then the bank restores the
money to the customer’s account and enters state 2. The latter state represents
the situation where the money has been cancelled. The bank, being responsible,
will not leave state 2 once it is entered, since the bank must not allow the same
money to be cancelled again or spent by the customer.’

Alternatively, when in state 1 the bank may receive a redeem request from
the store. If so, it goes to state 3, and shortly sends the store a transfer message,
with a new money file that now belongs to the store. After sending the transfer
message, the bank goes to state 4. In that state, it will neither accept cancel or
redeem requests nor will it perform any other actions regarding this particular
money file.

Now, let us consider Fig. 2.1(a), the automaton representing the actions of
the store, While the bank always does the right thing, the store’s system has
some defects. Imagine that the shipping and financial operations are done by

1You should remember that this entire discussion is about one single money file. The bank
will in fact be running the same protocol with a large number of electronic pieces of money,
but the workings of the protocol are the same for each of thern, so we can discuss the problem
as if there were only one piece of electronic money in existence.


--- Page 57 ---
2.1. AN INFORMAL PICTURE OF FINITE AUTOMATA 4}

separate processes, so there is the opportunity for the ship action to be done
either before, after, or during the redemption of the electronic money. That
policy allows the store to get. into a situation where it has already shipped the
goods and then finds out the money was bogus.

The store starts out in state a. When the customer orders the goods by
performing the pay action, the store enters state b. In this state, the store
begins both the shipping and redemption processes. If the goods are shipped
first, then the store enters state ¢, where it must still redeem the money from
the bank and receive the transfer of an equivalent money file from the bank.
Alternatively, the store may send the redeem message first, entcring state d.
From state d, the store might next ship, entering state ¢, or it might next
receive the transfer of money from the bank, entering state f. From state f, we
expect that the store will eventually ship, putting the store in state g, where the
transaction is complete and nothing more will happen. In state e, the store is
waiting for the transfer from the bank. Unfortunately, the goods have already
been shipped, and if the transfer never occurs, the store is out of luck.

Last, observe the automaton for the customer, Fig. 2.1(b). This automaton
has only one state, reflecting the fact that the customer “can do anything.”
The customer can perform the pay and cancel actions any number of times, in
any order, and stays in the lone state after each action.

2.1.3 Enabling the Automata to Ignore Actions

While the three automata of Fig. 2.1 reflect the behaviors of the three partici-
pants independently, there are certain transitions that are missing. For example,
the store is not affected by a cancel message, so if the cancel action is performed
by the customer, the store should remain in whatever state it is in. However, in
the formal definition of a finite automaton, which we shall study in Section 2.2,
whenever an input X is reccived by an automaton, the automaton must follow
an arc labeled X from the state it is in to some new state. Thus, the automaton
for the store needs an additional arc from each state to itself, labeled cancel.
Then, whenever the cancel action is executed, the store automaton can make a
“transition” on that input, with the effect that it stays in the same state it was
in. Without these additional arcs, whenever the cancel action was executed the
store automaton would “die”; that is, the autamaton would be in no state at
all, and further actions by that automaton would be impossible.

Another potential problem is that one of the participants may, intentionally
or erroneously, send an unexpected message, and we do not want this action to
cause oue of the automata to dic. For instance, suppose the customer decided
to execute the pay action a second time, while the store was in state e. Since
that state has no arc out with label pay, the store’s automaton would die before
it could receive the transfer from the bank. In summary, we must add to the
automata of Fig. 2.1 loops on certain states, with labels for all those actions
that must be ignored when in that state; the complete automata are shown
in Fig. 2.2, To save space, we combine the labels onto one arc, rather than


--- Page 58 ---
42 CHAPTER 2. FINITE AUTOMATA

showing several arcs with the same heads and tails but different labels. The
two kinds of actions that must. be ignored are:

cancel pay,cancel pay,cancel pay,cancel

pay.cancel pay,cancel pay,cancel

pay, ship

ship. redeem, transfer, pay,redeem, pay,redeem,

pay, cancel cancel cancel, ship cancel, ship
A ry. (C1)
ship redeem —transfer
Start Start
(b) Customer (c) Bank

Figure 2.2: The complete sets of transitions for the three automata

1. Actions that are irrelevant to the participant involved. As we saw, the
only irrelevant action for the store is cancel, so each of its seven states
has a loop labeled cancei. For the bank, both pay and ship are irrelevant,
so we have put at each of the bank’s states an arc labeled pay, ship. For
the customer, skip, redeem and transfer are all irrelevant, so we add arcs
with these labels. In effect, it stays in its one state on any sequence of
inputs, so the customer automaton has no effect on the operation of the
overall system. Of course, the customer is still a participant, since it is
the customer who initiates the pay and cancel actions. However, as we
mentioned, the matter of who initiates actions has nothing to do with the
behavior of the automata.

2. Actions that must not be allowed to kill an autematon. As mentioned, we
must not allow the customer to kill the store’s automaton by executing pay


--- Page 59 ---
2.4. AN INFORMAL PICTURE OF FINITE AUTOMATA 43

again, so we have added loops with label pay to all but state a (where the
pay action is expected and relevant). We have also added loops with labels
cancel to states 3 and 4 of the bank, in order to prevent the customer from
killing the bank’s automaton by trying to cancel money that has already
been redeemed. The bank properly ignores such a request. Likewise,
states 3 and 4 have loops on redeem. The store should not try to redeem
the same money twice, but if it does, the bank properly ignores the second
request.

2.1.4 The Entire System as an Automaton

While we now have models for how the three participants behave, we do not
yet have a representation for the interaction of the three participants. As men-
tioned, because the customer has no constraints on behavior, that automaton
has only one state, and any sequence of events lets it stay in that state; ie., it is
not possible for the system as a whole to “die” because the customer automaton
has no response to an action. However, both the store and bank behave in a
complex way, and it is not immediately obvious in what combinations of states
these two automata can be.

The normal way to explore the interaction of automata such as these is to
construct the preduct automaton. That automaton’s states represent a pair of
states, one from the store and one from the bank. For instance, the state (3. d)
of the product automaton represents the situation where the bank is In state
3, and the store is in state d. Since the bank has four states and the store has
seven, the product automaton has 4 x 7 = 28 states.

We show the product automaton in Fig. 2.3. For clarity, we have arranged
the 28 states in an array. The row corresponds to the state of the bank and
the column to the state of the store. To save space, we have also abbreviated
the labels on the arcs, with /, 8, C, R, and T standing for pay, ship, cancel,
redeem, and transfer, respectively.

To construct the arcs of the product autormaton, we need to run the bank
and store automata “in parallel.” Each of the two compouents of the product
automaton independently makes transitions on the various inputs. However, it
is important to notice that if an input action is received, and one of the two
automata has no state to go to on that input, then the product automaton
“dies”; it has no state to go to.

To make this rule for state transitions precise, suppose the product automa-
ton is in state (i,2). That state corresponds to the situation where the bank
is in state 7 and the store in state s. Let 2 be one of the input actions. We
look at the automaton for the bank, and see whether there is a transition out
of state 7 with label Z. Suppose there is, and it leads to state 7 (which might
be the same as i if the bank loops on input Z). Then, we look at the store and
see if there is an arc labeled Z leading to some state y. If both 7 and y exist,
then the product automaton has an arc from state (¢, x) to state (j,y), labeled
Z. Ef either of states 7 or y do not exist (because the bank or store has no arc


--- Page 60 ---
44 CHAPTER 2, FINITE AUTOMATA

Cc PC PC PC PC PC PC

Figure 2.3: The product automaton for the store and bank

out of i or x, respectively, for input Z), then there is no arc out of (t,x) labeled
Z.

We can now see how the ares of Fig. 2.3 were selected. For instance, on
input pay, the store goes from state a to b, but stays put if it is in any other
state besides a. The bank stays in whatever state it is in when the input is
pay, because that action is irrelevant to the bank. This observation explains
the four arcs labeled P at the left ends of the four rows in Fig. 2.3, and the
loops labeled P on other states.

For another example of how the arcs are selected, consider the input redeem.
If the bank receives a redeem message when in state 1, it goes to state 3. If in
states 3 or 4, it stays there, while in state 2 the bank automaton dies; ie., it has
nowhere to go. The store, on the other hand, can make transitions from state
b to d or from ¢ to e when the redeem input is received. In Fig. 2.3, we see six
arcs labeled redeem, corresponding to the six combinations of three bank states
and two store states that have outward-bound arcs labeled R. For example, in
state (1,8), the arc labeled & takes the automaton to state (3,d), since redeem
takes the bank from state 1 to 3 and the store from b to d. As another example,
there is an arc labeled A from (4,c) to (4, e), since redeem takes the bank from
state 4 back to state 4, while it takes the store from state ¢ to state e.


--- Page 61 ---
2.2. DETERMINISTIC FINITE AUTOMATA 45

2.1.5 Using the Product Automaton to Validate the
Protocol

Figure 2.3 tells us some interesting things. For instance, of the 28 states, only
ten of them can be reached from the start state, which is (1,a} — the cambi-
nation of the start states of the bank and store automata. Notice that states
like (2,e) and (4,d} are not aceessibdle, that is, there is no path to them from
the start state. Inaccessible states need not be included in the automaton. and
we did so in this example just to be systematic.

However, the real purpose of analyzing a protocol such as this one using
automata is to ask and answer questions that mean “can the following type
of error occur?” In the example at hand, we might ask whether it is possible
that the store can ship goods and never get paid. That is, can the produet
automaton get into a state in which the store has shipped (that is, the state is
in column c¢, ¢, or g}, and yet no transition on input T was ever made or will
be made?

For instance, in state (3,e), the goods have shipped, but there will eventu-
ally be a transition on input T to state (4,g). In terms of what the bank is
doing, once it has gotten to state 3, it has received the redeem request. and pro-
cessed it. That means it must have been iu state | before receiving the redeem
and therefore the cancel message had not been received and will be ignored if
received in the future. Thus, the bank will eventually perform the transfer of
money to the store.

However, state (2,c) is a problem. The state is accessible, but the only arc
out leads back to that state. This state corresponds to a situation where the
bank received a cance! message before a redeem message. However, the store
received a pay Iucssage: i.¢., the customer was being duplicitous and has both
spent and canceled the same money. The store foolishly shipped before trying
to redeem the money, and when the store does execute the redeem: action, ihe
bank will not even acknowledge the message, because it is in state 2, where it
has canceled the money and will not process a redeem request.

2.2 Deterministic Finite Automata

Now it is time to prescut the formal notion of a finite automaton, so that we
may start to make precise some of the informal arguments and descriptions that
we saw in Sections 1.1.1 and 2.1. We begin by introducing the formalisin of a
deterministic finite automaton, one that is in a single state after reading any
sequence of inputs. The term “deterministic” refers to the fact that on each
input there is one and only one state to which the automaton can transition from
its current state. In contrast, “nondeterministic” nite automata, the subject of
Section 2.3, can be in several states at once. The term “finite automaton” will
refer to the deterministic variety, although we shall use “deterministic” or the
abbreviation DFA normally, to remind the reader of which kind of automaton
we are talking about.


--- Page 62 ---
46 CHAPTER 2. FINITE AUTOMATA

2.2.1 Definition of a Deterministic Finite Automaton
A deterministic finite automaton consists of:

i. A finite set of states, often denoted Q.

2. A finite set of input symbols, often denoted ©.

3. A transition function that takes as arguments a state and an input symbol
and returns a state. The transition function will commonly be denoted 6.
In our informal graph representation of automata, 6 was represented by
arcs between states and the labels on the arcs. If g is a state, and a is an
input symbol, then 6(g, a) is that state p such that there is an arc labeled
a from g to p.?

4. A start state, one of the states in Q.
5. A set of final or accepting states F. The set F is a subset of Q.

A deterministic finite automaton will often be referred to by its acronym: DFA.
The most succinct representation of a DFA is a listing of the five components
above. In proofs we often talk about a DFA in “five-tuple” notation:

where 4 is the name of the DFA, Q@ is its set of states, © its input symbols, 6
its transition function, go its start state, and F its set of accepting states.

2.2.2 How a DFA Processes Strings

The first thing we need to understand about a DFA is how the DFA decides
whether or not to “accept” a sequence of input symbols. The “language” of
the DFA is the set of all strings that the DFA accepts. Suppose @1G2---Gn iS a
sequence of input symbols. We start out with the DFA in its start state, go. We
consult the transition function 6, say d(go,a1) = q to find the state that the
DFA A enters after processing the first input symbol a,. We process the next
input symbol, az, by evaluating 4{q1,@2); let us suppose this state is gq. We
continue in this manner, finding states g3,q4,.-..G, such that d(gj_-1,@;) = q
for each i. If g, is a member of F, then the input a;a2---a@, is accepted, and
if not then it is “rejected.”

Example 2.1: Let us formally specify a DFA that accepts all and only the
strings of 0’s and 1’s that have the sequence 01 somewhere in the string. We
can write this language L as:

fw | w is of the form z01y for some strings
xz and y consisting of 0’s and 1’s only}

“More accurately, the graph is a picture of some transition function 6, and the arcs of the
graph are constructed to reflect the transitions specified by 4.


--- Page 63 ---
2.2. DETERMINISTIC FINITE AUTOMATA 17

Another equivalent description, using parameters « and y to the left of the
vertical bar, is:

fx0ly| a and y are any strings of 0’s and 1’s}

Examples of strings in the language include 01, 11010, and 100011. Examples
of strings med in the language include e, 0, and 111000.

What do we know about an automaton that can accept this language L?
First, its input alphabet is © = {0,1}. It has some set of states, Q, of which
one, say go, is the start state. This automaton has to remember the important
facts about what inputs it has seen so far. To decide whether (1 is a substring
of the input, A needs to remember:

1. Has it already seen U1? Tf so, then it accepts every sequence of further
inputs; i.e., it will only be in accepting states from uow on.

2. Has it never seen (1, but its most recent input was 0, so if it now sees a
1, it will have seen Q1 and can accept everything it sces from here on?

3. Has it never seen 01, but its last input was either nonexistent (it just
started) or it last saw a 1? In this case, A cannot accept until it first sees
a 0 and then sees a 1 iminediately after.

These three conditions can each be represented by a stare. Condition (3) is
represented by the start state, go. Surely, when just starling, we need to see
a0 and then a 1. But if in state gg we next sce al, then we are no closer to
seeing 01, and so we must stay in state gg. That is, d(go, 1) = qu-

However, if we are in state go and we next sec a 0, we are in condition (2).
That is, we have never seen 01, but we have our 0. Thus, let us use gs to
represent condition (2). Our transition from gg on input 0 is 6(qu.0) = ge.

Now, let us consider the transitions from state gz. Tf we see a 0, we are no
better off than we were, but no worse either. We have not seen 01, bui 0 was
the last symbol, so we are still waiting for a 1, State go describes this situation
perfectly, so we want 4(q2,0) = qz. If we are in state g: and we see a 1 input,
we now know there is a 0 followed by a1. We can go to an accepting state,
which we shall call g;, and which corresponds to condition (1} above. That. is.

Finally, we must design the transitions for state q,. In this state, we have
already seen a O01 sequence, so regardless of what happens, we shall still be in
a situation where we've seen OL. That is, 6(q,,0) = d(¢1,1) =m.

Thus, Q@ = {qo,¢1.¢2}. As we said, gy is the start state, and the only
accepting slate is qi: that is, F = {gq}. The complete specification of the
automaton A that accepts the language F of strings that have a 01 substring,
is

A= ({qo.91. 42}, {0.1}. 4. go. {a

where 6 is the transition function described above. CI


--- Page 64 ---
48 CHAPTER 2. FINITE AUTOMATA

2.2.3 Simpler Notations for DFA’s

Specifying a DFA as a five-tuple with a detailed description of the 6 transition
function is both tedious and hard to read. There are two preferred notations
for describing automata:

1. A transition diagram, which is a graph such as the ones we saw in Sec-
tion 2.1.

2. A transition table, which is a tabular listing of the 6 function, which by
implication tells us the set. of states and the input alphabet.

Transition Diagrams

A transition diagram for a DFA A = (Q,»,6, qo, F) is a graph defined as follows:
a) For each state in @ there is a node.

b) For each state ¢ in @ and cach input symbol a in ¥, let 6{g,a) = p.
Then the transition diagram has an arc from node g to node p, labeled
a. If there are several input symbols that cause transitions from g to p,
then the transition diagram can have one arc, labeled by the list of these
symbols.

c) There is an arrow into the start state gu, labeled Start. This arrow does
not Originate at any node.

d) Nodes corresponding to accepting states (those in F) are marked by a
double circle. States not in F have a single circle.

Example 2.2: Figure 2.4 shows the transition diagram for the DFA that we
designed in Example 2.1. We see in that diagram the three nodes that cor-
respond to the three states. There is a Start arrow entering the start state,
go, and the one accepting state, gq, is represented by a double circle. Out of
each state is one arc labeled 0 and one are labeled i (although the two arcs
are combined into one with a double label in the case of gq). The arcs each
correspond to one of the 6 facts developed in Example 2.1. 0

1 0

Start 0 , Op 01

Figure 2.4: The transition diagram for the DFA accepting all strings with a
substring 01


--- Page 65 ---
2.2. DETERMINISTIC FINITE AUTOMATA 49

Transition Tables

A transition table is a conventional, tabular representation of a function like 6
that takes two arguments and returns a value. The rows of the table correspond
to the states, and the columns correspond to the inputs. The entry for the row
corresponding to state g and the column corresponding to input a is the state

6(q, @).

Example 2.3: The transition table corresponding to the function 6 of Ex-
ample 2.1 is shown in Fig. 2.6. We haye also shown two other features of a
transition table. The start state is marked with an arrow, and the accepting
states are marked with a star. Since we can deduce the sets of states and in-
put symbols by looking at the row and column heads, we can now read from
the transition table all the information we need to specify the finite automaton
uniquely. O

Figure 2.5: Transition table for the DFA of Example 2-1

2.2.4 Extending the Transition Function to Strings

We have explained informally that the DFA defiues a language: the set of all
strings that result in a sequence of state transitions from the start state to an
accepting state. In terms of the transition diagram, the language of a DFA
is the set of labels along all the paths that lead from the start state lo any
accepting state.

Now, we need to make the notion of the language of a DFA precise. To do
go, we define an extended transition function that describes what happens when
we start in any state and follow any sequence of inputs. If é is our transition
function, then the extended transition function constructed from 6 will be called
§. The extended transition function is a function that takes a state g and a
string w and returns a state p — the state that the automaton reaches when
starting in state g and processing the sequence of inputs w. We define 4 by
juduction on the length of the input string, as follows:

BASIS: 6 (g,¢) = gq. That is, if we are in state g and read no inputs, then we
are still m state q.


--- Page 66 ---
50 CHAPTER 2. FINITE AUTOMATA

INDUCTION: Suppose w is a string of the form ze; that is, @ is the last symbol
of w, and a is the string consisting of all but the last symbol.* For example,
w = 1101 is broken into z = 110 and a@ = 1. Then

5(g,w) = 6(5(¢, 2), 2) (2.1)

Now (2.1) may seem like a lot to take in, but the idea is simple. To compute
6(q, w}, first compute 6(g, x), the state that the automaton is in after processing
all but the last symbol of w. Suppose this state is p; that is, 6(g,2) =p. Then

6(g,w) is what we get by making a transition from state p on input a, the last
symbol of w. That is, 6(g,w) = 6(p, a).

Example 2.4: Let us design a DFA to accept. the language
LE = {w | w has both an even number of 0’s and an even number of 1’s}

It should not be surprising that the job of the states of this DFA is to count
both the number of 0’s and the number of 1’s, but count them modulo 2. That
is, the state is used to remember whether the number of 0's seen so far is even or
odd, and also to remember whether the number of 1’s seen so far is even or odd.
There are thus four states, which can be given the following interpretations:

go: Both the number of 0's seen so far and the number of 1’s seen so far arc
even.

gi: The number of 0's seen so far is even, but the number of 1’s scen so far is
odd.

gz: The number of 1's seen so far is even, but the number of 0’s seen so far is
odd.

ga: Both the number of 0's secn so far and the number of 1’s seen so far arc
odd.

State go is both the start state and the lone accepting state. It is the start
state, Decause before reading any inputs, the numbers of 0’s and 1’s seen so
far arc both zero, and zero is even. It is the only accepting state, because it
describes exactly the condition for a sequence of 0’s and 1’s to be in language
£.

We now know almost how to specify the DFA for language EL. It is

A = ({qgo, 41; 2,43}, {0,1}, 6, go, {¢0})

“Recall our convention thai letters at the beginning of the alphabet are symbols, and those
near the end of the alphabet are strings. We need that convention to make sense of the phrase
“of the form xa.”


--- Page 67 ---
22. DETERMINISTIC FINITE AUTOMATA 51

Figure 2.6: Transition diagram for the DFA of Example 2.4

where the transition function é is described by the transition diagram of Fig. 2.6.
Notice how each input 0 causes the state to cross the horizontal, dashed line.
Thus, after seeing an even number of 0’s we are always above the line, in state
go or q1 while after seeing an odd number of 0’s we are always below the line,
in state g2 or gj. Likewise, every 1 causes the state to cross the vertical, dashed
line. Thus, after seeing an even number of 1’s, we are always to the left, in state
go OF G2, while after seeing an odd number of 1’s we are to the right, in state q1
or g3. These observations are an informal proof that the four states have the
interpretations attributed to them. However, one could prove the correctness
of our claims about the states formally, by a mutual induction in the spirit of
Example 1.23.

We can also represent this DFA by a transition table. Figure 2.7 shows this
table. However, we are not just concerned with the design of this DFA; we
want to use it to illustrate the construction of 6 from its transition function 6.
Suppose the input is 110101. Since this string has even numbers of 0's and i’s
both, we expect it is in the language. Thus, we expect that 6(go, 110101} = qo,
since qo is the only accepting state. Let us now verify that claim.

|| 0 | 1

* —> Go || G2 | a

7 d3 | @o
gz || go | @
a3 || G1 | 42

Figure 2.7: Transition table for the DFA of Example 2.4

The check involves computing 8 (qo, w) for each prefix w of 110101, starting
at « and going in increasing size. The summary of this calculation is:


--- Page 68 ---
52 CHAPTER 2. FINITE AUTOMATA

Standard Notation and Local Variables

After reading this section, you might imagine that our customary notation
is required; that is, you must use 6 for the transition function, use A for
the name of a DFA, and so on. We tend to use the same variables to
denote the same thing across all examples, because it helps to remind you
of the types of variables, much the way a variable 7 in a program is almost
always of integer type. However, we are free to call the components of an
automaton, or anything else, anything we wish. Thus, you are free to call
a DFA M and its transition function T if you like.

Moreover, you should not be surprised that the same variable means
different things in different contexts. For example, the DFA’s of Examples
2.1 and 2.4 both were given a transition function called 6. However, the
two transition functions are each local variables, belonging only to their
examples. These two transition functions are very different and bear no
relationship to one another.

© 5(g0,€) = go.

° 5(qo, 1) = 6(8(go,€), 1) = 6(g0, 1) = a1.

@ 5(qo, 11) = 6(5(go, 1), 1) = dat, 1) = 40.

« 5(go, 110) = 6(5(go, 11), 0) = 6(q0,0) = a.

@ 8(qo, 1101) = 5(6(go, 110), 1) = 5(g2, 1) = as.

© d(go, 11010) = 6(8(go, 1101), 0) = d(q3,0) =a.

© (go, 110101) = 6(6(go, 11010), 1) = 4(q;,1) = a.

Oo

2.2.5 The Language of a DFA

Now, we can define the language of a DFA A = (@,5,6,qo, F). This language
is denoted L(A), and is defined by

L(A) = {w | (go, w) is in F}

That is, the language of A is the set of strings w that take the start state go to
one of the accepting states. If £ is L(A) for some DFA A, then we say £ is a
regular language.


--- Page 69 ---
2.2. DETERMINISTIC FINITE AUTOMATA 53

Example 2.5: As we mentioned earlier. if A is the DFA of Example 2.1, then
L(A) is the set of all strings of 0’s and 1’s that contain a substring 01. if A is
instead the DFA of Example 2.4, then L(4) is the set of all strings of 0’s and
1’s whose numbers of 0’s and 1’s are both even. O

2.2.6 Exercises for Section 2.2

Exercise 2.2.1: In Fig. 2.8 is a marble-rolling toy. A marble is dropped at
Aor B. Levers 71, 2, and «3 cause the marble to fall either to the left or to
the right. Whenever a marble encounters a lever, it causes the lever to reverse
after the marble passes, so the next marble will take the opposite branch.

A B
x
x

Cc dD

Figure 2.8: A marble-rolling toy

* 3) Model this toy by a finite automaton. Let the inputs A and B represent
the input into which the marble is dropped. Let acceptance correspond
to the marble exiting at D: nonacceptance represents a marble exiting at

C.
!b) Informally describe the language of the automaton.

c) Suppose that instead the levers switched before allowing the marble to
pass. How would your answers to parts (a) and (b) change?

*! Exercise 2.2.2: We defined 6 by breaking the input string into any string
followed by a single symbol (in the inductive part, Equation 2.1). However, we
informally think of 6 as describing what happens along a path with a certain


--- Page 70 ---
54 CHAPTER 2. FINITE AUTOMATA

string of labels, and if so, then it should not matter how we break the input
string in the definition of 6. Show that in fact, 5(q, ry) = 6(8 (q,2),y) for any
state g and strings x and y. Hint: Perform an induction on |y|.

Exercise 2.2.3: Show that for any state q, string 2, and input symbol a,
6(q.az) = 6(6(q,a),z). Hint: Use Exercise 2.2.2.

Exercise 2.2.4: Give DFA’s accepting the following languages over the alpha-
bet. {0,1}:

* a} The set of all strings ending in 00.

b) The set of all strings with three consecutive 0's (not necessarily at the
end).

c) The set of strings with 011 as a substring.

Exercise 2.2.5: Give DFA’s accepting the following languages over the alpha-
bet {0, 1}:

a) The set of all strings such that each block of five consecutive symbols
contains at least two 0’s.

b) The set of all strings whose tenth symbol from the right end is a 1.

c} The set of strings that either begin or end (or both) with 01.

d) The set of strings such that the number of 0’s is divisible by five, and the
number of 1’s is divisible by 3.

'! Exercise 2.2.6: Give DFA’s accepting the following languages over the alpha-
bet {0, 1}:

* a) The set of all strings beginning with a 1 that, when interpreted as a binary
integer, is a multiple of 5. For example, strings 101, 1010, and 1111 are
in the Janguage; 0, 100, and 111 are not.

b) The set of all strings that, when interpreted tn reverse as a binary inte-
ger, is divisible by 5. Exampics of strings in the language are 0, 10011,
1001100, and 0101.

Exercise 2.2.7: Let A be a DFA and g a particular state of A, such that
5(q,a) = q for all input symbols a. Show by induction on the length of the
input that for all input strings w, é(g,w) = q.

Exercise 2.2.8: Let A be a DFA and a a particular input symbol of A, such
that for all states q of 4 we have d(g,a) = q.

a) Show by induction on 7 that for all n > 0, 6(g,a") = g, where a” is the
string consisting of n a’s.


--- Page 71 ---
*

23. NONDETERMINISTIC FINITE AUTOMATA

baa |
aa

b) Show that either {a}* C L(A) or {a}" N L(A) = 0.

Exercise 2.2.9: Let 4 = (Q,2,46,qy, {gy}) be a DFA, and suppose that. for all
ain © we have d(qo,a) = d(gy. a).

a) Show that for all w 4 « we have 8{qy, #) = S(qp.w).

b) Show that if # is a nonempty string in L(A), then for all & > 0, x® (ie.,
x written & times) is also in L(A).

Exercise 2.2.10: Consider the DFA with the following transition table:

| 0 | 1
+All A] SB
Bl BA

Informally deseribe the language accepted by this DFA, and prove by induction
on the length of an input string that your description is correct. Hint: When
setting up the inductive hypothesis, it is wise to make a statement about what
inputs get you to each state, not. just what inputs get you to the accepting
state.

Exercise 2.2.11: Repeat Exercise 2.2.10 for the following transition table:

2.3 Nondeterministic Finite Automata

A “nondeterministie” [finite antormaton (NFA) las the power to be in several
states at once. This ability is often expressed as au ability to “guess” something
about its input. For instance, when the automaton is used to search for certain
sequences of characters {c.z., keywords) in a long text string, it is helpfil to
“miess” that we are at the beginning of one of those strings and use a sequence of
states to do nothing but check that the string appears, character by character.
We shall see an example of this type of application in Section 2-4.

Before examining applications, we need to define nondeterministic finite
automata and show that each one accepts a language that is also accepted by
some DFA. That is, the NFA’s accept exactly the regular languages, just as
DFA’s do. However, there are reasons to think about NFA’s. They are often
More succinct and easier to design than DFA’s, Moreover, while we can always
convert an NFA to a DFA, the latter may have exponentially more states than
the NFA; fortunately. cases of this type are rare.


--- Page 72 ---
56 CHAPTER 2. FINITE AUTOMATA

2.3.1 An Informal View of Nondeterministic Finite
Automata

Like the DFA, an NFA has a finite set of states, a finite set of input symbols,
one start state and a set of accepting states. It also has a transition function,
which we shall commonly call 6. The difference between the DFA and the NFA
is in the type of 6. For the NFA, 6 is a function that takes a state and input
symbol as arguments (like the DFA’s transition function), but returns a set
of zero, one, or more states (rather than returning exactly one state, as the
DFA must). We shall start with an example of an NFA, and then make the
definitions precise.

Example 2.6: Figure 2.9 shows a nondeterministic finite automaton, whose
job is to accept all and only the strings of 0’s and 1’s that end in 01. State
go is the start state, and we can think of the automaton as being in state go
(perhaps among other states) whenever it has not yet “euessed” that the final
O1 has begun. It is always possible that the next symbol does not begin the
final 01, even if that symbol is 0. Thus, state gg may transition to itself on both
0 and 1.

0, 1
S t~ 0 1
tart (%) @)

Figure 2.9: An NFA accepting all strings that end in 01

However, if the next symbol is 0, this NFA also guesses that the final 01 has
begun. An arc labeled 0 thus leads from go to state qr. Notice that there are
two arcs labeled 0 out of gy. The NFA has the option of going either to go or
to q,, and in fact it does both, as we shall see when we make the definitions
precise. In state q,, the NFA checks that the next symbol is 1, and if so, it goes
to state gz and accepts.

Notice that there is no arc out of g, labeled 0, and there are no ares at all
out of go. In these situations, the thread of the NFA’s existence corresponding
to those states simply “dies,” although other threads may continue to exist.
While a DFA has exactly one are out of each state for each input symbol, an
NFA has no such constraint; we have seen in Fig. 2.9 cases where the number
of arcs is zero, one, and two, for example.

Figure 2.10 suggests how an NFA processes inputs. We have shown what
happens when the automaton of Fig. 2.9 receives the input sequence 00101. It
starts in only its start state, go. When the first 0 is read, the NFA may go to
either state gg or state g), so it does both. These two threads are suggested by
the second column in Fig. 2.10.

Then, the second 0 is read. State gg may again go to both gq and q.
However, state g, has no transition on 0, so it “dies.” When the third input, a


--- Page 73 ---
23. NONDETERMINISTIC FINITE AUTOMATA 57

% a Cee Gee
4| 4 MN 4
(stuck) NN SN
ty tp
(stuck)
0 0 1 0 ]

Figure 2.10: The states an NFA is in during the processing of input sequence
00101

1, occurs, we must consider transitions from both gg and q,;. We find that go
goes only to gp on 1, while q goes only to gg. Thus, after reading 001, the NFA
is in states gy and gz. Since ge is an accepting state, the NFA accepts 001.

However, the input is not finished. The fourth input, a 0, causes q2's thread
to die, while gy goes to both gy and q. The last input, a 1, sends go to qo and
@, to gz. Since we are again in an accepting state, 00101 is accepted. O

2.3.2 Definition of Nondeterministic Finite Automata

Now, let us introduce the formal notions associated with nondeterministic finite
automata. The differences between DFA’s and NFA's will be pointed out as we
do. An NFA is represented essentially like a DFA:

A = (Q,2,4.g0, F)
where:
1. @ is a finite set of states.
. is a finite sect of input symbols.

. Gg, a member of Q, is the start state.

mem 3 bo

. F,a subset of Q, is the set of final (or accepting) states.

5. 6, the transition function is a function that takes a state in @ and an
input symbol in © as arguments and returns a subset of @. Notice that
the only difference between an NFA and a DFA is in the type of value
that 6 returns: a sct of states in the case of an NFA and a single state in
the case of a DFA.

Example 2.7: The NFA of Fig. 2.9 can be specified formally as

({do, 71,92}, {9,1},9, 40, {¢2})


--- Page 74 ---
38 CHAPTER 2. FINITE AUTOMATA

Figure 2.11: Transition table for an NFA that accepts all strings ending in O1

where the transition function 6 is given by the transition table of Fig. 2.11. ©

Notice that transition tables can be used to specify the transition function
for aun NFA as well as for a DFA. The only difference is that each entry in the
table for the NFA is a set, even if the set is a singleton (has one member). Also
notice that when there is no transition at all from a given state on a given input.
symbol, the proper entry is 4, the empty set.

3.3.3. The Extended Transition Function

As for DFA’s, we need to extend the transition function d of an NFA to a
function 6 that takes a state g and a string of input symbols w, and returns the
set of states that the NFA is in if it starts in state g and processes the string w.
The idea was suggested by Fig. 2.10; in essence 6{g, w) is the column of states
found after reading w, if q is the lone state in the first column. For instance,
Fig. 2.10 suggests that &(qo, 001) = {go, g2}. Formaily, we define § for an NFA’s
transition function 6 by:

BASIS: 4(g,€) = {gq}. That is, without reading any input symbols, we are only
in the state we began in.

INDUCTION: Suppose w is of the form w = xa, where a is the final symbol of
w and x is the rest of w. Also suppose that 4(q, x) = {p1,p2,..-,pe}. Let

k
U 5(pi, @) = {7 1, 72;+++5Tm}
i=1

Then S(¢,w) = {ri,r2,.-.;7m}. Less formally, we compute 5(q,w) by first
computing 4(g,z), and by then following any transition from any of these states
that is labeled a.

Example 2.8: Let us use 5 to describe the processing of input 00101 by the
NFA of Fig. 2.9. A summary of the steps is:

1. d(q,€) = {¢0}-
2. 8(Go,0) = 5(qo,0) = {90,91}:


--- Page 75 ---
2.3. NONDETERMINISTIC FINITE AUTOMATA 39

3. 5(go, 00) = 5(go, 0) U 5(q1,0) = {90,41} UO = {g0,.a1}-

4, 8(qa, 001) = 5(go, 1) U d(q1, 1) = {90} U {42} = {40, 92}-
5. 5(go,0010) = 6(go,0) U §(g2,0) = {go,¢1} UO = {90,0 }-
6. 5(qo,00101) = 5(qo, 1) U 8(a1, 1) = {go} U {a2} = {40, @2}-

Line {1) is the basis rule. We obtain line (2) by applying 6 to the lone state, go,
that is in the previous set, and get {go,q:} as a result. Line (3) is obtained by
taking the union over the two states in the previous set of what we get when we
apply 6 to them with input 0. That is, 6(go,0) = {go, qi}, while 6(q,0) = @.
For line {4), we take the union of 6(gg, 1) = {qo} and 6(q1,1) = {gz}. Lines (5)
and (6) are similar to lines (3) and (4). D

2.3.4 The Language of an NFA

As we have suggested, an NFA accepts a string w if it is possible to make any
sequence of choices of next state, while reading the characters of w, and go from
the start state to any accepting state. The fact that other choices using the
input symbols of w lead to a nonaccepting state, or do not lead to any state at
all (i.e., the sequence of states “dies”), does not prevent w from being accepted
by the NFA as a whole. Formally, if A = (Q,5,6,¢o, F) is an NFA, then

L(A) = {w | 6(qo,w) NF 4 0}

That is, L(A) is the set of strings w in £* such that (go, w) contains at least
one accepting state.

Example 2.9: As an example, let us prove formally that the NFA of Fig. 2.9
accepts the language L = {w | w ends in 01}. The proof is a mutual induction
of the following three staternents that characterize the three states:

1. 8(go, w) contains go for every w.

,

2. é(go,w)} contains q, if and only if w ends in 0.

8(go,w) contains qy if and only if w ends in O1.

To prove these statements, we need to consider how A can reach cach state; Le.,
what was the last input symbol, and in what state was A just before reading
that symbol? .

Since the language of this automaton is the set of strings w such that 6(go, w)
contains go {because go is the only accepting state), the proof of these three
statements, in particular the proof of (3), guarantees that the language of this
NEA is the set. of strings ending in 01. The proof of the theorem is an induction
on ||, the length of w, starting with length 0.


--- Page 76 ---
60 CHAPTER 2. FINITE AUTOMATA

BASIS: If || = 0, then w = e. Statement (1) says that d(qo,€) contains go,
which it does by the basis part of the definition of 6. For statement (2), we
know that ¢ does not end in 0, and we also know that 5(go,€) does not contain
gi, again by the basis part of the definition of 6. Thus, the hypotheses of both
directions of the if-and-only-if statement are false, and therefore both directions
of the statement are true. The proof of statement (3) for w = ¢ is essentially
the same as the above proof for statement (2).

INDUCTION: Assume that w = xa, where a is a symbol, either 0 or 1. We
may assume statements (1) through (3) hold for z, and we need to prove them
for w. That is, we assume |w| = +1, so |z| = 7. We assume the inductive
hypothesis for n and prove it for n+ 1.

1. We know that 5(qo, x) contains gy. Since there are transitions on both
0 and 1 from go to itself, it follows that 4(go,w) also contains go, so
statement (1) is proved for w.

2. (If) Assume that w ends in 0; 1e.,@ = 0. By statement (1) applied to z,
we know that d{qg,2) contains gg. Since there is a transition from go to
gq on input 0, we conclude that 4(¢g9, w) contains q1.

(Only-if} Suppose 4(q,w) contains q. If we look at the diagram of
Fig. 2.9, we see that the only way to get into state q, is if the input
sequence w is of the form 20. That is enough to prove the “only-if”
portion of statement (2).

3. (If) Assume that w ends in 01. Then if w = za, we know that a@ = 1 and
« ends in 0. By statement (2) applied to z, we know that 4(qo, £) contains
gi. Since there is a transition from gq, to gz on input 1, we conclude that
d(go, w) contains ge.

(Only-if} Suppose (qo, w) contains gz. Looking at the diagram of Fig. 2.9,
we discover that the only way to get to state q is for w to be of the form
v1, where 4(gj,x) contains g;. By statement (2) applied to 2, we know
that x ends in 0. Thus, w ends in 01, and we have proved statement (3).

0

2.3.5 Equivalence of Deterministic and Nondeterministic
Finite Automata

Although there are many languages for which an NFA is easier to construct
than a DFA, such as the language (Example 2.6) of strings that end in 61, it is
a surprising fact that every language that can be described by some NFA can
also be described by some DFA. Moreover, the DFA in practice has about as
many states as the NFA, although it often has more transitions. In the worst
case, however, the smallest DFA can have 2” states while the smallest NFA for
the same language has only 7 states.


--- Page 77 ---
2.3. NONDETERMINISTIC FINITE AUTOMATA 61

The proof that DFA’s can do whatever NFA’s can do involves an important
“construction” called the subset construction because it involves constructing all
subsets of the set of states of the NFA. In general, many proofs about automata
involve constructing one automaton from another. It is important for us to
observe the subset construction as an example of how one formally describes one
automaton in terms of the states and transitions of another, without knowing
the specifics of the latter automaton.

The subset construction starts from an NFA N = (Qw,5,6n,40,Fn). Its
goal is the description of a DFA D = (Qp,2, 4p, {go}, Fp) such that L(D) =
L(N). Notice that the input alphabets of the two automata are the same, and
the start state of D is the set containing only the start state of N. The other
components of D are constructed as follows.

° Qn is the set of subsets of Qn; i-¢., Qp is the power set of Qn. Note
that if @~ has n states, then Qn will have 2" states. Often, not all these
states are accessible from the start state of Qn. Inaccessible states can
be “thrown away,” so effectively, the number of states of D may be much
smaller than 2”.

Fp is the set of subsets S of Qn such that SM Fw # #. That is, Fp is
all sets of N’s states that include at least one accepting state of N.

*

For each set § C Qy» and for each input symbol a in &,
bo(S,a)= LJ dn@p.2)
pins

That is, to compute dp(S,a) we look at all the states p in S, see what
states N goes to from p on input a, and take the union of all those states.

Jo |r
0

@ || @
— {ao} |] {40,41} | {40}
{qi} |] @ {q2}
*{q} || 0 p

{go, 1} |} {ao,41} | {90,42}
*{4Go, 42} || {0.4} | {90}
*{qo, 41,42} |] {40.0} | {40,42}

Figure 2.12: The complete subset construction from Fig. 2.9

Example 2.10: Let N be the automaton of Fig. 2.9 that accepts ali strings
that end in 01. Since N’s set of states is {go.q1,q2}, the subset construction


--- Page 78 ---
62 CHAPTER 2. FINITE AUTOMATA

produces a DFA with 2? = 8 states, corresponding to all the subsets of these
three states. Figure 2.12 shows the transition table for these eight states; we
shall show shortly the details of how some of these entries are computed.

Notice that this transition table belongs to a deterininistic finite automaton.
Even though the entries in the table are sets, the states of the constructed DFA
are sets. To make the point clearer, we can invent new names for these states,
e.g., A for #, B for {go}, and so on. The DFA transition table of Fig 2.13 defines
exactly the same automaton as Fig. 2.12, but makes clear the point that the
entries in the table arc single states of the DFA.

Soma ool

We ty bye & bye

Figure 2.13: Renaming the states of Fig. 2.12

Of the cight states in Fig. 2.13, starting in the start state B, we can only
reach states B, E, and F. The other five states are inaccessible from the start
state and may as well not be there. We often can avoid the exponential-time step
of constructing transition-table entries for every subset of states if we perform
“lazy evaluation” on the subsets, as follows.

BASIS: We know for certain that the singleton set consisting only of N’s start
State is accessible.

INDUCTION: Suppose we have determined that set S of states is accessible.
Then for each input symbol a, compute the set of states dp(5,a); we know that
these sets of states will also be accessible.

For the example at hand, we know that {go} is a state of the DFA D. We
find that ép{{go}.0) = {¢o,4¢.} and 6n({qo},1) = {go}. Both these facts are
established by looking at the transition diagram of Fig. 2.9 and observing that.
on 0 there are arcs out of gg to both go and q,, while on 1 there is an are only
to go. We thus have one row of the transition table for the DFA: the second
row in Fig. 2.12.

One of the two sets we computed is “old”; {gp} has already been considered.
However, the other — {9,41} -- is new and its transitions must be computed.
We find dn({qo.91},0) = {9o.a1} and dp({90,0},1) = {¢o,92}. For instance,
to see the latter calculation, we know that


--- Page 79 ---
2.3. NONDETERMINISTIC FINITE AUTOMATA 63

dp({go.m }, 1) = (qo, 1) U dn (qi, b) = {40} U {92} = {40,42}

We now have the fifth row of Fig. 2.12, and we have discovered one new
state of D, which is {qo,q2}. A similar calculation tells us

8p ({¢0, q2},0) = dn (qu, 0) U dv (G2, 0} = {go,n} U8 = {a0,01}
dp({ao, 42}; 1) = bn (go, 1) U Sn (42,1) = {go} UB = {40}

These calculations give us the sixth row of Fig. 2.12, but it gives us only sets
of states that we have already seen.

Thus, the subset construction has converged; we know all the accessible
states and their transitions. The entire DFA is shown in Fig. 2.14. Notice that
it has only three states, which is, by comeidence, exactly the same number of
states as the NFA of Fig, 2.9, from which it was constructed. However, the DFA
of Fig. 2.14 has six transitions, compared with the four transitions in Fig. 2.9.
i

(_)

Start

1

Figure 2.14: The DFA constructed from the NFA of Fig 2.9

We need to show formally that the subset construction works, although
the intuition was suggested by the examples. After reading sequence of input
symbols w, the constructed DFA is in one state that is the set of NFA states
that the NFA would be in after reading w. Since the accepting states of the
DFA are those sets that include at least one accepting state of the NFA, and the
NFA also accepts if it gets into at least one of its accepting states, we may then
conclude that the DFA and NFA accept exactly the same strings, and therefore
accept the same language.

Theorem 2.11: If D = (Qn,¥,6p,{q0}, Fp) is the DFA constructed from
NFA N = (Qy,¥,6n, 40) Fx) by the subset construction, then L(D) = EN).

PROOF: What we actually prove first, by induction on |w|, is that
dn {go}. w) = dn (qo, w)

Notice that each of the é functions returns a set of states from Qa, but é Dp
interprets this set as one of the states of Qp (which is the power set of Qa;).
while é6y interprets this set as a subset of Qy.


--- Page 80 ---
64 CHAPTER 2. FINITE AUTOMATA

BASIS: Let |w| = 0; that is, w =e. By the basis definitions of 6 for DFA’s and
NFA’s, both én({go}, €} and dn(go,€) are {go}.
INDUCTION: Let w be of length n+ 1, and assume the statement for length

n. Break w up as w = a where a is the final symbol of w. By the induc-
tive hypothesis, dp( ({go},2) = = bn (qo, z). Let both these sets of N’s states be

{P1, P2,--->Pe}- .
The inductive part of the definition of 6 for NFA’s tells us

bn (go, w) = UY avtonsa) (2.2)

i=l]

The subset construction, on the other hand, tells us that

k
Sn({mi,p2,---,Pe}a)= J év(pi, 4) (2.3)
i=1
Now, let us use (2.3) and the fact that dp({go}, 2) = {p1,p2,..-, pe} in the
inductive part of the definition of d for DFA’s:

k
dn({qo}, w) = 0p (n({¢0}, 2), @) = 6p({Pi, Pa, - . Pk}, @) = U dn (pi, @)

mga)
Thus, Equations (2.2) and (2.4) demonstrate that dp( ({go},w) = ou (a, w
When we observe that D and N both accept w if and only if ren
én (qo, w), respectively, contain a state in Fy, we have a complete proof that
E(D)=L(N). a
Theorem 2.12: A language £ is accepted by some DFA if and only if Z is
accepted by some NFA.

PROOF: (If) The “if” part is the subset construction and Theorem 2.11.

(Only-if} This part is easy; we have only to convert a DFA into an identical NFA.
Put intuitively, if we have the transition diagram for a DFA, we can alse inter-
pret it as the transition diagram of an NFA, which happens to have exactly one
choice of transition in any situation. More formally, let D = (Q,2,ép,q0,F)
be a DFA. Define N = (Q,2,6n,90, F) to be the equivalent NFA, where dy is
defined by the rule:

* If dp(g,a) = p, then dy(q,a) = {p}.
It is then easy to show by induction on _ that if dp(qo,w) = p then

dv( (go. w = {p}

We leave the proof to the reader. As a consequence, w is accepted by D if and
only if it is accepted by N; ie., L(D) = L(N). O


--- Page 81 ---
2.3. NONDETERMINISTIC FINITE AUTOMATA 65

2.3.6 A Bad Case for the Subset Construction

In Example 2.16 we found that the DFA had no more states than the NFA.
As we mentioned, it is quite common in practice for the DFA to have roughly
the same number of states as the NFA from which it is constructed. However,
exponential growth in the number of states is possible; all the 2” DFA states
that we could construct from an m-state NFA may turn out to be accessible. The
following example does not quite reach that bound, but it is an understandable
way to reach 2” states in the smallest DFA that is equivalent to an n + L-state
NFA.

Example 2.13: Consider the NFA N of Fig. 2.15. E(N) is the set of all strings
of 0’s and 1’s such that the nth symbol from the end is 1. Intuitively, a DFA
D that accepts this language must remember the last symbols it has read.
Since any of 2” subsets of the last n symbols could have been 1, if D has fewer
than 2” states, then there would be some state g such that D can be in state q
after reading two different sequences of n bits, say ayaz2---@_ and oo

Since the sequences are different, they must differ in some position, say
a; # 6;. Suppose (by symmetry) that a; = 1 and b; = 0. If i = 1, then g
must be both an accepting state and a nonaccepting state, SINCE @103-°-*Gy 18
accepted {the nth symbo! from the end is 1) and hy) by---h, is not. If: > 1,
then consider the state p that D enters after reading 7 — 1 0’s. Then p must
be both accepting and nonaccepting, since a;a;41 ~*~ a, 00+--0 is accepted and
bpbi41 -- +b, 00---0 is not.

0, |

be l i) 0, 1 ou - "ae “+@)

Start

Figure 2.15: This NFA has no equivalent DFA with fewer than 2” states

Now, let us see how the NFA N of Fig. 2.15 works. There is a state go that
the NFA is always in, regardless of what inputs have been read. If the next
input is 1, N may also “guess” that this 1 will be the nth symbol! from the end,
so it goes to state q, as well as gg. From state q), any input takes NV to qo,
the next input takes it to q3, and so on, until » — 1 inputs later, it is in the
accepting state gn. The formal statement of what the states of N do is:

1. N is in state go after reading any sequence of inputs w.

2. N is in state qj, fori =1,2,...,, after reading mput sequence w if and
only if the ith symbol from the end of w is 1; that is, w is of the form
zlaya2-+-a;_1, where the a;’s are each input symbols.

We shall not prove these statements formally; the proof is an easy induction
on |w|, mimicking Example 2.9. To complete the proof that the automaton


--- Page 82 ---
66 CHAPTER 2. FINITE AUTOMATA

The Pigeonhole Principle

In Example 2.13 we used an important reasoning technique called the
pigeonhole principle. Colloquially, if you have more pigeons than pigeon-
holes, and each pigeon flies into some pigeonhole, then there must be at
least. one hole that has more than one pigeon. In our example, the “pi-
geons” are the sequences of n bits, and the “pigeonholes” are the states.
Since there are fewer states than sequences, one state must be assigned
two sequences.

The pigeonhole principle may appear obvious, but it actually depends
on the number of pigeonholes being finite. Thus, it works for finite-state
automata, with the states as pigeonholes, but does not apply to other
kinds of automata that have an infinite number of states.

To see why the finiteness of the number of pigeonholes is essential,
consider the infinite situation where the pigeonholes correspond to integers
1,2,.... Number the pigeons 0,1,2,..., so there is one more pigeon than
there are pigeonholes. However, we can send pigeon 7 to hole 7 + 1 for all
¢ >. Then each of the infinite number of pigeons gcts a pigeonhole, and
no two pigeons have to share a pigeonhole.

accepts exactly those strings with 4 1 in the nth position from the end, we
consider statement (2) with « = n. That says N is in state q, if and only if
the nth symbol from the end is 1. But q, is the only accepting state, so that
condition also characterizes exactly the set of strings accepted by V. 0

2.3.7 Exercises for Section 2.3
* Exercise 2.3.1: Convert to a DFA the following NFA:



--- Page 83 ---
2.3. NONDETERMINISTIC FINITE AUTOMATA 67

Dead States and DFA’s Missing Some Transitions

We have formally defined a DFA to have a transition from any state,
on any input symbol, to exactly one state. However, sometimes, it is
more convenient to design the DFA to “die” in situations where we know
it is impossible for any extension of the input sequence to be accepted.
For instance, observe the automaton of Fig. 1.2, which did its job by
recognizing a single keyword, then, and nothing else. Technically, this
automaton is not a DFA, because it lacks transitions on most symbols
from cach of its states.

However, such an automaton is an NFA. If we use the subset construc-
tion to convert it to a DFA, the automaton looks almost the sare, but it
includes a dead state, that is, a nonaccepting state that goes to itself on
every possible input symbol. The dead state corresponds to §, the erpty
set of states of the automaton of Fig. 1.2.

In general, we can add a dead state to any automaton that has no
more than one transition for any state and input symbol. Then, add a.
transition to the dead state from each other state g, on all mput symbols
for which g has no other transition. The result will be a DFA in the strict
sense. Thus, we shall sometimes refer to an automaton as a DFA if it has
at most one transition out of any state on any symbol, rather than if it
has ezactly one transition.

! Exercise 2.3.3: Convert the following NFA to a DFA and informally describe
the language it accepts.

t Exercise 2.3.4: Give nondeterministic finite automata to accept the following
languages. Try to take advantage of nondeterminism as much as possible.
* a) The set of strings over alphabet {0,1,...,9} such that the final digit has
appeared before.
b) The set of strings over alphabet {0,1,...,9} such that the final digit has
not appeared before.
c) The set of strings of 0°s and i*s such that there are two 0’s separated by
a number of positions that is a multiple of 4. Note that 0 is an allowable
multiple of 4.


--- Page 84 ---
—

68 CHAPTER 2. FINITE AUTOMATA

Exercise 2.3.5: In the only-if portion of Theorem 2.12 we omitted the proof
by induction on |w| that if dp(go,w) = p then én(qo.w) = {p}. Supply this
proof.

Exercise 2.3.6: In the box on “Dead States and DFA’s Missing Some Tran-
sitions,” we claim that if N is an NFA that has at most one choice of state for
any state and input symbol (i.e., 6(g, a) never has size greater than 1), then the
DFA PD constructed from N by the subset construction has exactly the states
and transitions of NV plus transitions to a new dead state whenever NV is missing
a transition for a given state and input symbol. Prove this contention.

Exercise 2.3.7: In Example 2.13 we claimed that the NFA NV is in state q,
fori = 1,2,...,n, after reading input sequence w if and only if the ith symbol
from the end of w is 1. Prove this claim.

2.4 An Application: Text Search

In this section, we shall see that the abstract study of the previous section,
where we considered the “problem” of deciding whether a sequence of bits ends
in 01, is actually an excellent model for several real problems that appear in
applications such as Web search and extraction of information from text.

2.4.1 Finding Strings in Text

A cominon problem in the age of the Web and other on-line text repositories
is the following. Given a set of words, find all documents that contain one
for all) of those words. A search engine is a popular example of this process.
The search engine uses a particular technology, called inverted indexes, where
for each word appearing on the Web (there are 100,000,000 different words),
a list of all the places where that word occurs is stored. Machines with very
large amounts of main memory keep the most common of these lists available,
allowing many people to search for documents at once.

Inverted-index techniques do not make use of finite automata, but they also
take very large amounts of time for crawlers to copy the Web and set up the
indexes. There are a number of related applications that are unsuited for in-
verted indexes, but are good applications for automaton-based techniques. The
characteristics that make an application suitable for searches that use automata
are:

1. The repository on which the search is conducted is rapidly changing. For
example:

(a) Every day, news analysts want to search the day’s on-line news arti-
cles for relevant topics. For example, a financial analyst might search
for certain stock ticker symbols or names of companies.


--- Page 85 ---
2.4, AN APPLICATION: TEXT SEARCH 69

(b) A “shopping robot” wants to search for the current prices charged
for the items that its clients request. The robot will retrieve current
catalog pages from the Web and then search those pages for words
that suggest a price for a particular item.

2. The documents to be searched cannot be cataloged. For example, Ama-
zon.com does not make it easy for crawlers to find all the pages for all the
books that the company sells. Rather, these pages are generated “on the
fly” in response to queries. However, we could send a query for books on
a certain topic, say “finite automata,” and then search the pages retrieved
for certain words, ¢.g., “excellent” in a review portion.

2.4.2 Nondeterministic Finite Automata for Text Search

Suppose we are given a set of words, which we shall call the keywords, and we
want to find occurrences of any of these words. In applications such as these, a
useful way to proceed is to design a nondeterministic finite automaton, which
signals, by entering an accepting state, that it has seen one of the keywords.
The text of a document is fed, one character at a time to this NFA, which then
recognizes occurrences of the keywords in this text. There is a simple form to
an NFA that recognizes a set of keywords.

l. There is a start state with a transition to itself on every input symbol,
e.g. every printable ASCII character if we are examining text. Intuitively,
the start state represents a “guess” that we have not yet begun to see one
of the keywords, even if we have seen some letters of onc of these words.

2. For each keyword a@1a@2 ---@,, there are k states, say g1,g2,--..¢- There
is a transition from the start state to gq on symbol a), a transition from
gq, to G2 on symbol az, and so on. The state gy is an accepting state and
indicates that the keyword a,a2---a, has been found.

Example 2.14: Suppose we want to design an NFA to recognize occurrences
of the words web and ebay. The transition diagram for the NFA designed using
the rules above is in Fig. 2.16. State 1 is the start state, and we use £ to stand
for the set of all printable ASCII characters. States 2 through 4 have the job
of recognizing web, while states 5 through 8 recognize ebay. O

Of course the NFA is not a program. We have two major choices for an
implementation of this NFA.

1. Write a program that simulates this NFA by computing the set of states
it is in after reading each input symbol. The simulation was suggested in
Fig. 2.10.

2. Convert the NFA to an equivalent DFA using the subset construction.
Then simulate the DFA directly.


--- Page 86 ---
70 CHAPTER 2. FINITE AUTOMATA

Figure 2.16: An NFA that searches for the words web and ebay

Some text-processing programs, such as advanced forms of the UNIX grep
command (egrep and fgrep) actually use a mixture of these two approaches.
However, for our purposes, conversion to a DFA is easy and is guaranteed not
to increase the number of states.

2.4.3 A DFA to Recognize a Set of Keywords

We can apply the subset construction to any NFA. However, when we apply that
construction to an NFA that was designed from a set. of keywords, according to
the strategy of Section 2.4.2, we find that the number of states of the DFA is
never greater than the number of states of the NFA. Since in the worst case the
number of states exponentiates as we go to the DFA, this observation is good
news and explains why the method of designing an NFA for keywords and then
constructing a DFA from it is used frequently. The rules for constructing the
set of DFA states is as follows.

a) If go is the start. state of the NFA, then {go} is one of the states of the
DFA.

b) Suppose p is one of the NFA states, and it is reached from the start state
along a path whose symbols are a,a2--:a,,. Then one of the DFA states
is the set of NFA states consisting of:

1. qo.

2. p.

3, Every other state of the NFA that is reachable from go by following
a path whose labels are a suffix of a1aq+--am, that is, any sequence
of symbols of the form ajaj41 +++ @m.-

Note that in general, there will be one DFA state for each NFA state p. However,
in step (b), two states may actually yield the same set of NFA states, and thus
become one state of the DFA. For example, if two of the keywords begin with
the same letter, say a, then the two NFA states that are reached from gp by an


--- Page 87 ---
2.4. AN APPLICATION: TEXT SEARCH 71

arc labeled a will yield the same set of NFA states and thus get merged in the
DFA.

Figure 2.17: Conversion of the NFA from Fig. 2.16 to a DFA

Example 2.15: The construction of a DFA from the NFA of Fig. 2.16 is shown
in Fig. 2.17. Each of the states of the DFA is located in the same position as
the state p from which it is derived using rule (b) above. For example, consider
the state 135, which is our shorthand for {1,3,5}. This state was constructed
from state 3. It inclides the start state, 1, because every set of the DFA states
does. It. also includes state 5 because that state is reached from state 1 by a
suffix, e, of the string we that reaches state 3 in Fig. 2.16.

The transitions for each of the DFA states may be calculated according to
the subset construction. However, the rule is simple. From any set of states that
includes the start state go and some other states {p1,p2,--.,Pn}, determine, for
each symbo! x, where the p;’s go in the NFA, and let this DFA state have a
transition labeled 2 to the DFA state consisting of go and all the targets of the


--- Page 88 ---
72 CHAPTER 2. FINITE AUTOMATA

pi’s on symbol a. On all symbols x such that there are no transitions out of
any of the p;’s on symbol 2, let this DFA state have a transition on x to that
state of the DFA consisting of go and all states that are reached from go in the
NFA following an are labeled =.

For instance, consider state 135 of Fig. 2.17. The NFA of Fig. 2.16 has
transitions on symbol b from states 3 and 5 to states 4 and 6, respectively.
Therefore, on symbol 6, 135 goes to 146. On symbol e, there are no transitions
of the NFA out of 3 or 5, but there is a transition from 1 to 5. Thus, in the
DFA, 135 goes to 15 on input ¢. Similarly, on input w, 135 goes to 12.

On every other symbol 2, there are no transitions out or 3 or 5, and state 1
goes only to itself. Thus, there are transitions from 135 to 1 on every symbol
in © other than 6, ¢, and w. We use the notation L — 6 ~ e — w to represent
this set, and use similar representations of other sets in which a few symbols
are removed from =. 0

2.4.4 Exercises for Section 2.4
Exercise 2.4.1: Design NFA’s to recognize the following sets of strings.
* a) abc, abd, and aacd. Assume the alphabet is {a, b,¢, a}.

b) 0101, 101, and 0114.

c) ab, be, and ca. Assume the alphabet is {a, 6, c}.

Exercise 2.4.2: Convert each of your NFA’s from Exercise 2.4.1 to DFA’s.

2.5 Finite Automata With Epsilon-Transitions

We shall now introduce another extension of the finite automaton. The new
“feature” is that we allow a transition on ¢, the empty string. In effect, an
NFA is allowed to make a transition spontaneously, without receiving an input
symbol. Like the nondeterminism added in Section 2.3, this new capability does
not. expand the class of languages that can be accepted by finite automata, but it
does give us some added “programming convenience.” We shall also see, when
we take up regular expressions in Section 3.1, how NFA’s with ¢-transitions,
which we call «-NFA’s, are closely related to regular expressions and useful
in proving the equivalence between the classes of languages accepted by finite
automata and by regular expressions.

2.5.1 Uses of e-Transitions

We shall begin with an informal treatment of e-NFA’s, using transition diagrams
with « allowed as a label. In the examples to follow, think of the automaton
as accepting those sequences of labels along paths from the start state to an
accepting state. However, each ¢ along a path is “invisible”; ie., it contributes
nothing to the string along the path.


--- Page 89 ---
2.5. FINITE AUTOMATA WITH EPSILON-TRANSITIONS 73

Example 2.16: In Fig. 2.18 is an «NFA that accepts decimal numbers con-
sisting of:

1. An optional + or — sign,
2. A string of digits,
3. A decimal point, and

4, Another string of digits. Either this string of digits, or the string (2) can
be empty, but at least one of the two strings of digits must be nonempty.

Start C)
> (%)

Figure 2.18: An «-NFA accepting decimal numbers

Of particular interest is the transition from go to q, on arly of €, +, or -.
Thus, state g, represents the situation in which we have seen the sign if there
is one, and perhaps some digits, but not the decimal point. State qo represents
the situation where we have just seen the decimal point, and may or may not
have seen prior digits. In gy we have definitely seen at least one digit, but
not the decimal point. Thus, the interpretation of qj is that we have seen a
decimal point and at least one digit, either before or after the decimal poiut.
We may stay in gg reading whatever digits there are, and also have the option
of “guessing” the string of digits is complete and going spontaneously to gs, the
accepting state. O

Example 2.17: The strategy we outlined in Example 2.14 for building an
NFA that recognizes a set of keywords can be simplified further if we allow
é-transitions. For instance, the NFA recognizing the keywords web and ebay,
which we saw in Fig. 2.16, can also be implemented with ¢-transitions as in
Fig. 2.19. In general, we construct a complete sequence of states for each
keyword, as if it were the only word the automaton needed to recognize. Then,
we add a new start state (state 9 in Fig. 2.19), with e-transitions to the start-
states of the automata for each of the keywords. O


--- Page 90 ---
74 CHAPTER 2. FINITE AUTOMATA

Start

Figure 2.19: Using ¢-transitions to help recognize keywords

2.5.2 The Formal Notation for an «-NFA

We may represent an «NFA exactly as we do an NFA, with one exception: the
transition function must include information about transitions on ¢. Formally,
we represent an «-NFA 4A by A = (Q,=,46,qo,F), where all components have
their same interpretation as for an NFA, except that 6 is now a function that
takes as arguments:

1. A state in Q, and
2. A member of Z U fe}, that is, either an input symbol, or the symbol! e.

We require that ¢, the symbol for the empty string, cannot be a member
of the alphabet £, so no confusion results.

Example 2.18: The e-NFA of Fig. 2.18 is represented formally as
B= (140, Giy-+- 195}; {., +, —, 0, 1, o -, 9},4, do, {g5})

where 6 is defined by the transition table in Fig. 2.20. O

| € | +,-|- 0,1,...,9
go || {ax} | in} | @ G
gy || b ) {qz} | {q.¢4}
go || & Q a {¢3}
a3 {gs} 8 6 {q3}
a4 p 0 {a3} )
gs || @ 0 1) )

Figure 2.20: Transition table for Fig. 2.18


--- Page 91 ---
nas |
a

2.5. FINITE AUTOMATA WITH EPSILON-TRANSITIONS

2.5.3 Epsilon-Closures

We shall proceed to give formal definitions of an extended transition function for
€-NFA’s, which leads to the definition of acceptance of strings and languages by
these automata, and eventually lets us explain why «-NFA‘s can be simulated by
DFA’s. However, we first need to learn a central definition, called the ¢-closurc
of a state. Informally, we e-close a state g by following all transitions out of
q that are labeled «. However, when we gct to other states by following c, we
follow the e-transitions out of those states, and so on, eventually finding every
state that can be reached from gq along any path whose arcs are all labeled e.
Formally, we define the e-closure ECLOSE(g) recursively, as follows:

BASIS: State ¢ is in ECLOSE(g).

INDUCTION: If state p is in ECLOSE(g), and there is a transition from state p
to state r labeled e, then r is in ECLOSE{q). More precisely, if 6 is the transition
function of the e-NFA involved, and p is in ECLOSE(g), then ECLOSL{(g) also
contains all the states in d(p, e).

Example 2.19: For the automaton of Fig. 2.18, cach state is its own «-closure,
with two exceptions: ECLOSE(go) = {90,9} and ECLOSE(qs3) = {9¢3,95}. The
reason is that there are only two e-transitions, one that adds qy to ECLOSE(gu)
and the other that adds ys to ECLOSE(g;).

A more complex example is given in Fig. 2.21. For this collection of states,
which may be part of some e-NFA, we can conclude that

ECLOSE(1) = {1,2,3, 4, 6}

Each of these states can be reached from state 1 along a path exclusively labeled
e. For example, state 6 is reached by the path 1 + 2 + 3 — 6. State 7 is not
in ECLOSE(1), since although it is reachable from state 1, the path must use
the arc 4 > 5 that is not labeled ¢. The fact that state 6 is also reached from
state 1 alonga path 14 4 4 5 > 6 that has non-e transitions is untmportant.
The existence of one path with all labels ¢ is sufficient to show state 6 is in
ECLOSE(1}. O

Figure 2.21: Some states and transitions


--- Page 92 ---
76 CHAPTER 2, FINITE AUTOMATA

2.5.4 Extended Transitions and Languages for c-NFA’s

The e-closure allows us to explain easily what the transitions of an ¢-NFA look
like when given a sequence of (non-e) inputs. From there, we can define what.
it means for an e-NFA to accept its input. .

Suppose that E = (Q, £, 6. qo, F) isan e-NFA. We first define 4, the extended
transition function, to reflect what happens on a sequence of inputs. The intent
is that 6(q, w) is the set of states that can be reached along a path whose labels,
when concatenated, form the string w. As always, e’s along this path do not
contribute to w. The appropriate recursive definition of 6 is:

BASIS: 8(q, €) = ECLOSE(q). That is, if the label of the path is e, then we can
follow only ¢-labcled arcs extending from state g; that is exactly what ECLOSE
does.

INDUCTION: Suppose w is of the form za, where a is the last symbol of w.
Note that a@ is a member of E; it cannot be ¢, which is not in &£, We compute

6(q,w) as follows:

1. Let {p.,p2,..., De} be 8(q, z). That is, the p;’s are all and only the states
that we can reach from gq following a path labeled «. This path may end
with one or more trausitions labeled ¢, and may have other ¢-transitions,
as well.

2. Let Ula 5(p;,a) be the set {r1,r2,..-)7m}- That is, follow all transitions
labeled a from states we can reach from g along paths labeled z. The
rj’s are some of the states we can reach from g along paths labeled w.
The additional states we can reach are found from the r;’s by following
e-labeled arcs in step (3), below.

3. Then 8(g,w) = Use ECLOSE(r;). This additional closure step includes
all the paths from q labeled w, by considering the possibility that there
are additional e-labcled ares that we can follow after making a transition
on the final “real” symbol, a.

Example 2.20: Let us compute 8(go, 5.6) for the «NFA of Fig. 2.18. A
summary of the steps needed are as follows:

* 8(4o. Ey= ECLOSE(go} = {qa,qi}-
e Compute 6(go,5) as follows:

1. First compute the transitions on input 5 from the states go and ™
that we obtained in the calculation of &(q0;€)s above. That is, we
compute 6(gg,5) U 5(q1,5) = {q1,%}-

2. Next, e-close the members of the set computed in step (1). We get
ECLOSE(q) U ECLOSE(gs) = {a1} U {aa} = {91,94}. That set is
) (qo,5). This two-step pattern repeats for the next two symbols.


--- Page 93 ---
2.5. FINITE AUTOMATA WITH EPSILON-TRANSITIONS 7

a |

« Compute 6(qo, 8.) as follows:

1. First compute 6(g,, .) U 6fga, .) = {a2} U {ga} = {92.43}.
2. Then compute

5(qu,5-) = ECLOSE(g2) U ECLOSE(g3) = {92} U {93.95} = {42.43.48}
* Compute 4{q0,5.6) as follows:

1. First compute 6(g2,6) U 4(q3,6) U 6(g5,6) = {as} U fas} UB =
{qs}-

2. Then compnte 4(go,5.6) = ECLOSE(gs) = {43.45}.

Now, we can define the language of an e-NFA E = (Q.2.d6,go.) in the
expected way: L(E) = {wu | d(go.w) OF # O}. That is, the language of £ is
the set. of strings w that take the start state to at least one accepting state. For
instance, we saw in Example 2.20 that 8(qo, 5.6) coutains the accepting state
gs, 80 the string 5.6 is in the language of that «-NFA.

2.5.5 Eliminating e-Transitions

Given any e-NFA E, we can find a DFA D that accepts the same language as £.
The construction we use is very close to the subset construction, as the states of
D are subsets of the states of H. The only difference is that we must incorporate
é-transitions of BZ. which we do through the mechanism of the e-closure.

Let BE = (Qy, ©, 66,90, £). Then the equivalent DFA

D=(Qp.¥.6p.¢p, Fp}
is defined as follows:

1. Qp is the set of subsets of Qe. More precisely, we shall find that. all
accessible states of D are e-closed subsets of Qp, that is, sets S C Op
such that S$ = ECLOSE(S). Put another way, the e-closed sets of states 5
are those such that any ¢-transition out of one of the states in S leads to
a state that is also in S. Note that @ is an e-closed set.

2. qo = ECLOSE(qy); that is, we get the start state of D by closing the set
consisting of only the start state of E. Note that this rule differs from
the original subset construction, where the start state of the constructed
automaton was just the set containing the start state of the given NTA.

3. Fp is those sets of states that contain at least onc accepting state of F.
That is, Fp = {$ | S isin Qp and SN Fy F 9}.

4, 5p(S,a) is computed, for all a in © and sets 5 in Qn by:


--- Page 94 ---
78 CHAPTER 2. FINITE AUTOMATA

(b}) Compute UL, $n(p;, 0); let this set be {r1,r2,-.-, 7m}.
(c) Then dp(S,a) = U_, BCLOSE(r;).

Example 2.21: Let us eliminate etransitions from the e-NFA of Fig. 2.18,
which we shall call & in what follows. From FE, we construct an DFA D, which
is shown in Fig. 2.22. However, to avoid clutter, we omitted from Fig. 2.22 the
dead state 9 and all transitions to the dead state. You should imagine that for
each state shown in Fig. 2.22 there are additional transitions from any state to
# on any input symbols for which a transition is not indicated. Also, the state
@ has transitions to itself on all input symbols.

0,1,...,9

0,1,...,9
a,

0,1,....9

Figure 2.22: The DFA D that eliminates e-transitions from Fig. 2.18

Since the start state of F is gg, the start state of D is ECLOSE(qg), which
is {go,4¢1}. Our first job is to find the successors of gg and g, on the various
symbols in 4; note that these symbols are the plus and minus signs, the dot,
and the digits 0 through 9. On + and —, g goes nowhere in Fig. 2.18, while
go goes to qi. Thus, to compute ép({go,91}, +) we start with {g} and «close
it. Since there are no ¢-transitions out of gi, we have ép({go,q1},+) = {ar}.
Similarly, do({¢o,4¢1},—) = {a1}. These two transitions are shown by one arc
in Fig. 2.22.

Next, we need to compute dp({go,qi}, -}. Since go goes nowhere on the
dot, and gq, goes to q2 in Fig. 2.18, we must e-close {go}. As there are no
e-transitions out of gz, this state is its own closure, so dp({go, qi}, -) = {ge}.

Finally, we must compute dp({go, gi}, 0), as an example of the transitions
from {go,q1} on all the digits. We find that go goes nowhere on the digits, but
q, goes to both g, and qy. Since neither of those states have ¢-transitions out,
we conclude én({qo, 91}, 9) = {41, ga}, and likewise for the other digits.


--- Page 95 ---
2.5, FINITE AUTOMATA WITH EPSILON-TRANSITIONS 79

We have now explained the arcs out of {go,g,} in Fig. 2.22. The other
transitions are computed similarly, and we leave them for you to check. Since
gs is the only accepting state of E, the accepting states of D are those accessible
states that contain gs. We see these two sets {g3,q5} and {¢2,93,¢5} indicated
by double circles in Fig. 2.22. 0

Theorem 2.22: A language L is accepted by some e-NFA if and only if Z is
accepted by some DFA.

PROOF: (If) This direction is easy. Suppose L = L(D) for some DFA. Turn
D into an e-DFA E by adding transitions 6(q,¢) = @ for all states q of D.
Technically, we must also convert the transitions of D on input symbols, e.g.,
6p(q,a) = p into an NFA-transition to the set containing only p, that is
Se(q,a) = {p}. Thus, the transitions of E and D are the same, but & ex-
plicitly states that there are no transitions out of any state on €.

(Only-if) Let E = (Qz,=,6e,90,Fe) be an «NFA. Apply the modified
subset construction described above to produce the DFA

We need to show that L{D) = E(B), and we do so by showing that the extended
transition functions of FE and D are the same. Formally, we show dg(go,w) =
dp(qp,w) by induction on the length of w.

BASIS: If |w] = 0, then w = €. We know dz(g0,€) = ECLOSE(qg). We also
know that gp = ECLOSE(qo), because that is how the start state of D is defined.
Finally, for a DFA, we know that i(p, €) = p for any state p, so in particular,
6n(qp,€) = ECLOSE(go). We have thus proved that ér(go.€) = dp(qn,€).

INDUCTION: Suppose w = xa, where a is the final symbol of w, and assume
that the statement holds for #. That is, by: (qo. 2) = dp{gn.x). Let both these
sets of states be {py,po,..-,pe}-

By the definition of 4 for «-NFA’s, we compute be(go,w) by:

1. Let {ri.r2,..-.?m} be Ut, d@ (pi, @).
2. Then é¢(qo,w) = Uy ECLOSE(?;).

If we examine the construction of DFA D in the modified subset construction
above, we see that dp({p1, po,.-.,pe},a) is constructed by the same two steps
(1) and (2) above. Thus, dn(qp,w), which is dp({p1,p2,---.Px},@) is the same
set as be (40, w). We have now proved that on(qo.2) = bn(gp.w) and completed
the inductive part. O



--- Page 96 ---
80 CHAPTER 2. FINITE AUTOMATA

3.5.6 Exercises for Section 2.5

* Exercise 2.5.1: Consider the following «-NFA.

a) Compute the e-closure of each state.
b) Give all the strings of length three or less accepted by the automaton.

c) Convert the automaton to a DFA.

Exercise 2.5.2: Repeat Exercise 2.5.1 for the following «-NFA:

Je |e |? |e

spl far} | @
q || 0 {p} | {r} | tp, a}
xr || @ 6 af 8

Exercise 2.5.3: Design «-NFA’s for the following languages. Try to use e-
transitions to simplify your design.

a) The set. of strings consisting of zero or more a’s followed by zero or more
b’s, followed by zero or more e's.

1b) The set of strings that consist of either 01 repeated one or more times or
010 repeated one or more times.

tc) The set of strings of 0’s and 1's such that at least one of the last ten
positions is a 1.

2.6 Summary of Chapter 2

+ Deterministic Finite Automata: A DFA has a finite set of states and a
finite set of input symbols. One state is designated the start state, and
zero or more states are accepting states. A transition function determines
how the state changes each time an input symbol is processed.

¢ Transition Diagrams: It is convenient to represent. automata by a graph
in which the nodes are the states, and arcs are labeled by input symbols,
indicating the transitions of that automaton. The start state is designated
by afi arrow, and the accepting states by double circles.


--- Page 97 ---
27.

REFERENCES FOR CHAPTER 2 81

+ Language of an Automaton: The automaton accepts strings. A string is

accepted if, starting in the start state, the transitions caused by processing
the symbols of that string one-at-a-time lead to an accepting state. Tn
terms of the transition diagram, a string is accepted if it is the label of a
path from the start state to some accepting state.

Nondeterministic Finite Automate: The NPA differs from the DFA in
that the NFA can have any number of transitions (including zero) to next
states from a given state on a given input symbol.

The Subset Construction: By treating sets of states of an NFA as states
of a DFA, it is possible to convert any NFA to a DFA that accepts the
same language.

é- Transitions: We can extend the NFA by allowing transitions on an
empty input, i-e., no input symbol at all. These extended NFA’s can be
converted to DFA’s accepting the same language.

Tert-Searching Applications: Nondeterministic finite automata are a usc-
ful way to represent a pattern matcher that scans a large body of text for
one or more keywords. These automata are either simulated directly in
software or are first converted to a DFA, which is then simulated.

2.7 References for Chapter 2

The formal study of finite-state systems is generally regarded as originating
with [2]. However, this work was based on a “neural nets” model of computing,
rather than the finite automaton we know today. The conventional DFA was
independently proposed, in several similar variations, by [1], [3], and [4]. The
nondeterministic finite automaton and the subset construction are from [5].

1. D. A. Huffman, “The synthesis of sequential switching circuits,” J. Frank-

lin Inst. 257:3-4 (1954), pp. 161-190 and 275-308.

_ W.S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent

in nervious activity,” Bull, Math. Biophysics 5 (1943), pp. 115-133.

. GH. Mealy, “A method for synthesizing sequential circuits,” Bell System

Technical Journal 34:5 (1955), pp. 1045-1079.

_ E. F. Moore, “Gedanken experiments on sequential machines,” in [6],

pp. 129-153.

_ M.O. Rabin and D. Scott, “Finite automata and their decision problems,”

IBM J. Research and Development 3:2 (1959), pp. 115-125.

_C. E. Shannon and J. McCarthy, Automata Studies, Princeton Univ.

Press, 1956.


--- Page 98 ---


--- Page 99 ---
Chapter 3

Regular Expressions and
Languages

We begin this chapter by introducing the notation called “regular expressions.”
These expressions are another type of language-defining notation, which we
sampled briefly in Section 1.1.2. Regular expressions also may be thought of as
a “programming language,” in which we express some important applications,
such as text-seatch applications or compiler components. Regular expressions
are closely related to nondeterministic finite automata and can be thought of
as a “user-friendly” alternative to the NFA notation for describing software
components.

In this chapter, after defining regular expressions, we show that they arc
capable of defining all and only the regular languages. We discuss the way
that regular expressions are used in several software systems. Then, we exam-
ine the algebraic laws that apply to regular expressions. They have significant
resemblance to the algebraic laws of arithmetic, yet there are also some im-
portant differences between the algebras of regular expressions and arithmetic
expressions.

3.1 Regular Expressions

Now, we switch our attention from machine-like descriptions of languages —
deterministic and nondeterministic finite automata — to an algebraic descrip-
tion: the “regular expression.” We shall find that regular expressions can define
exactly the same languages that the various forms of automata describe: the
regular languages. However, regular expressions offer something that automata
do not: a declarative way to express the strings we want to accept. Thus,
regular expressions serve as the input language for many systems that process
strings. Examples include:

83


--- Page 100 ---
84 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

1. Search commands such as the UNIX grep or equivalent commands for
finding strings that one sees in Web browsers or text-formatting systems.
These systems use a regular-expression-like notation for describing pat-
terns that the user wants to find in a file. Different search systems convert
the regular expression into either a DFA or an NFA, and simulate that.
automaton on the file being searched.

2. Lexical-analyzer generators, such as Lex or Flex. Recall that a lexical
analyzer is the component of a compiler that breaks the source program
into logical units (called tokens) of one or more characters that have a
shared significance. Examples of tokens include keywords (e.g., while),
identifiers (e.g., any letter followed by zero or more letters and/or digits},
and signs, such as + or <=. A lexical-analyzer generator accepts descrip-
tions of the forms of tokens, which are essentially regular expressions, and
produces a DFA that recognizes which token appears next on the input.

3.1.1 The Operators of Regular Expressions

Regular expressions denote languages. For a simple example, the regular ex-
pression 01* + 10* denotes the language consisting of all strings that are either
a single 0 followed by any number of 1’s or a single 1 followed by any number
of 0’s. We do not expect you to know at this point how to interpret regular
expressions, so our statement about the language of this expression must be
accepted on faith for the moment. We shortly shail define all the symbols used
in this expression, so you can see why our interpretation of this regular expres-
sion is the correct one. Before describing the regular-expression notation, we
need to learn the three operations on languages that the operators of regular
expressions represent. These operations are:

1. The union of two languages Z and M, denoted LU M, is the set of strings
that are in either L or M, or both. For example, if L = {001, 10,111} and
M = {c,004}, then LU M = {e, 10,001, 111}.

2. The concatenation of languages L and M is the set of strings that can
be formed by taking any string in E and concatenating it with any string
in M. Recall Section 1.5.2, where we defined the concatenation of a
pair of strings; one string is followed by the other to form the result of the
concatenation. We denote concatenation of languages either with a dot or
with no operator at all, although the concatenation operator is frequently
called “dot.” For example, if £ = {001, 10,111} and M = {¢,001}, then
L.M, or just LM, is (001, 10,111, 001001, 10001, 111001}. The first three
strings in LM are the strings in L concatenated with e. Since « is the
identity for concatenation, the resulting strings are the same as the strings
of L. However, the last three strings in LM are formed by taking each
string in L and concatenating it with the second string in M, which is
001. For instance, 10 from ZL concatenated with 001 from M gives us
10001 for BM.


--- Page 101 ---
3.1. REGULAR EXPRESSIONS 85

3. The closure (or star, or Kleene closure)’ of a language Z is denoted L*
and represents the set of those strings that can be formed by taking any
number of strings from L, possibly with repetitions (i.e., the same string
may be selected more than once) and concatenating all of them. For
instance, if L = {0,1}, then L* is all strings of 0’s and 1's. If L = {0,11},
then L* consists of those strings of 0’s and 1’s such that the 1’s come in
pairs, e.g., 011, 11110, and ¢, but not 01011 or 101. More formally, £* is
the infinite union Ui>y L*, where L° = {e}, L! = L, and L’, for i > 1 is
LL-+-L (the concatenation of i copies of L).

Example 3.1: Since the idea of the closure of a language is somewhat tricky,
let us study a few examples. First, let Z = {0,11}. L£° = {¢}, independent of
what language L is; the Oth power represents the selection of zero strings from
LE. £1 = L, which represents the choice of one string from £. Thus, the first
two terms in the expansion of L* give us {e,0, 11}.

Next, consider L?. We pick two strings from ZL, with repetitions allowed, so
there are four choices. These four selections give us L? = {00,011, 110, 1111}.
Similarly, L* is the set of strings that may be formed by making three choices
of the two strings in £ and gives us

{000, 0011, 0110, 1100, 01111, 11011, 11110, 111111}

To compute L”, we must compute LZ! for each i, and take the union of all these
languages. L‘ has 2' members. Although each ZL’ is finite, the union of the
infinite number of terms L' is generally an infinite language, as it is in our
example.

Now, let Z be the set of all strings of 0’s. Note that £ is infinite, unlike
our previous example, which is a finite language. However, it is not hard to
discover what L* is. L° = {e}, as always. L! = L. L? is the set of strings that
can be formed by taking one string of 0’s and concatenating it with another
string of 0’s. The result is still a string of 0's. In fact, every string of 0's
can be written as the concatenation of two strings of 0’s (don’t forget that «
is a “string of 0’s”; this string can always be one of the two strings that we
concatenate). Thus, L? = L. Likewise, L3 = L, and so on. Thus, the infinite
union L* = L°U £1U L£? U--- is Z in the particular case that the language L
the set. of all strings of 0’s.

For a final example, 0" = {e}. Note that 0° = {e}, while @*, for any i > 1,
is empty, since we can’t select any strings from the empty set. In fact, @ is one
of only two languages whose closure is notinfinite. O

3.1.2 Building Regular Expressions

Algebras of all kinds start with some elementary expressions, usually constants
and/or variables. Algebras then allow us to construct more expressions by

1The term “Kleene closure” refers to 5. C. Kleene, who originated the regular expression
notation and this operator.


--- Page 102 ---
86 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

Use of the Star Operator

We saw the star operator first in Section 1.5.2, where we applied it to an
alphabet, ¢.g., &*. That operator formed all strings whose symbols were
chosen from alphabet ©. The closure operator is essentially the same,
although there is a subtle distinction of types.

Suppose L is the language containing strings of length 1, and for each
symbol @ in © there is a string a in L. Then, although Z and © “look”
the same, they are of different types; L is a set of strings, and © is a set
of symbols. On the other hand, Z* denotes the same language as L*.

applying a certain set of operators to these elementary expressions and to pre-
viously constructed expressions. Usually, some method of grouping operators
with their operands, such as parentheses, is required as well. For instance,
the familiar arithmetic algebra starts with constants such as integers and real
numbers, plus variables, and builds more complex expressions with arithmetic
operators such as + and x.

The algebra of regular expressions follows this pattern, using constants and
variables that denote languages, and operators for the three operations of Sec-
tion 3.1.1 —union, dot, and star. We can describe the regular expressions
recursively, as follows. In this definition, we not only describe what the le-
gal regular expressions are, but for each regular expression E', we describe the
language it represents, which we denote L(E).

BASIS: The basis consists of three parts:

1. The constants « and # are regular expressions, denoting the languages {e}
and @, respectively. That is, L{e) = fe}, and L(#) = @.

2. Ife is any symbol, then a is a regular expression. This expression denotes
the language {a}. That is, L(a) = {a}. Note that we use boldface font
to denote an expression corresponding to a symbol. The correspondence,
e.g. that a refers to a, should be obvious.

3. A variable, usually capitalized and italic such as Z, is a variable, repre-
senting any language.

INDUCTION: There are four parts to the inductive step, one for each of the
three operators aud one for the introduction of parentheses.

1. If E and F are regular expressions, then & + F is a regular expression
denoting the union of £(£) and £(F). That is, L06+F) = LUE) U LF).

2. If £ and F are regular expressions, then EF is a regular expression denot-
ing the concatenation of L(E) and L(F). That is, LLB) = L(E)L(P).


--- Page 103 ---
3.4, REGULAR EXPRESSIONS 87

Expressions and Their Languages

Strictly speaking, a regular expression F is just an expression, not a lan-
guage. We should use L(£) when we want to refer to the language that E
denotes. However, it is common usage to refer to say “E” when we really
mean “L(E).” We shall use this convention as long as it is clear we are
talking about a language and not about a regular expression.

Note that the dot can optionally be used to denote the concatenation op-
erator, either as an operation on languages or as the operator in a regular
expression. For instance, 0.1 is a regular expression meaning the same as
01 and representing the language {01}. However, we shall avoid the dot
as concatenation in regular expressions.”

3. If £ is a regular expression, then E* is a regular expression, denoting the
closure of L{E). That is, L(E*") = (L(B))".

4. If F is a regular expression, then (F), a parenthesized £, is also a regular
expression, denoting the same language as E. Formally; L((£)) = L(£).

Example 3.2: Let us write a regular expression for the set of strings that
consist of alternating 0’s and 1’s. First, let us develop a regular expression
for the language consisting of the single string 01. We can then use the star
operator to get an expression for all strings of the form 0101---01.

The basis rule for regular expressions tells us that 0 and 1 are expressions
denoting the languages {0} and {1}, respectively. If we concatenate the two
expressions, we get a regular expression for the language {01}; this expression is
01. As a general rule, if we want a regular expression for the language consisting
of only the string w, we use w itself as the regular expression. Note that. in the
regular expression, the symbols of w will normally be written in boldface. but
the change of font is only to help you distinguish expressions from strings and
should not be taken as significant.

Now, to get all strings consisting of zero or more occurrences of 01, we use
the regular expression (01)*. Note that we first put parentheses around 01, to
avoid coufusing with the expression 01*, whose language is all strings consisting
of a O and any number of l’s. The reason for this interpretation is explained
in Section 3.1.38, but briefly, star takes precedence over dot, and therefore the
argument of the star is selected before performing any concatenations.

However, L((01)*) is not exactly the language that we want. It incluctes
only those strings of alternating 0’s and L's that begin with 0 and end with 1.
We also need to consider the possibility that there is a 1 at the beginning and/or

2In fact, UNIX regular expressions use the dot for an entirely different. purpose: represent.
ing any ASCII character.


--- Page 104 ---
88 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

a0 atthe end. One approach is to construct three more regular expressions that
handle the other three possibilities. That is, (10)* represents those alternating
strings that begin with 1 and end with 0, while 0(10)}* can be used for strings
that both begin and end with 0 and 1(014)* serves for strings that begin and
end with 1. The entire regular expression is

(04)* + (10)* + 0(10)" + 1(01)*

Notice that we use the + operator to take the union of the four languages that
together give us all the strings with alternating 0’s and 1's.

However, there is another approach that yields a regular expression that
looks rather different and is also somewhat more succinct. Start again with the
expression (01)*. We can add an optional 1 at the beginning if we concatenate
on the left with the expression ¢ + 1. Likewise, we add an optional 0 at the end
with the expression ¢ + 0. For instance, using the definition of the + operator:

L(e+ 1) = Lle) U £4) = {e} U {1} = fe, 1}

If we concatenate this language with any other language L, the ¢ choice gives
us all the strings in L, while the 1 choice gives us lw for every string w in L.
Thus, another expression for the set of strings that alternate 0’s and 1’s is:

(e+ 1)(01)*{e + 0)

Note that we need parentheses around each of the added expressions, to make
sure the operators group properly. O

3.1.3 Precedence of Regular-Expression Operators

Like other algebras, the regular-expression operators have an assumed order of
“precedence,” which means that operators are associated with their operands in
a particular order. We are familiar with the notion of precedence from ordinary
arithmetic expressions. For instance, we know that cy+z groups the product zy
before the sum, so it is equivalent to the parenthesized expression (xy) + z and
not to the expression z(y + z). Similarly, we group two of the same operators
from the left in arithmetic, so x — y — z is equivalent to (# — y) — z, and not to
“x --(y—z). For regular expressions, the following is the order of precedence for
the operators:

1. The star operator is of highest precedence. That is, it applies only to
the smallest sequence of symbols to its left that is a well-formed regular
expression.

2. Next in precedence comes the concatenation or “dot” operator. After
grouping all! stars to their operands, we group concatenation operators
to their operands. That is, all expressions that are juxtaposed (adjacent,
with no intervening operator) are grouped together. Since concatenation


--- Page 105 ---
3.1. REGULAR EXPRESSIONS 89

is an associative operator it does not matter in what order we group
consecutive concatenations, although if there is a choice to be made, you
should group them from the left, For instance, 012 is grouped (01)2.

3. Finally, all unions (+ operators) are grouped with their operands. Since
union is also associative, it again matters little in which order consecutive
unions are grouped, but we shall assume grouping from the left.

Of course, sometimes we do not want the grouping in a regular expression
to be as required by the precedence of the operators. If so, we are free to use
parentheses to group operands exactly as we choose. In addition, there is never
anything wrong with putting parentheses around operands that you want to
group, even if the desired grouping is implied by the rules of precedence.

Example 3.3: The expression 01* + 1 is grouped (0(1*)) + 1. The star
operator is grouped first. Since the symbol 1 immediately to its left is a legal
regular expression, that alone is the operand of the star. Next, we group the
concatenation between 0 and (1*), giving us the expression (0(1")). Finally,
the union operator connects the latter expression and the expression to its right,
which is 1.

Notice that the language of the given expression, grouped according to the
precedence rules, is the string 1 plus all strings consisting of a 0 followed by any
number of 1’s (including none). Had we chosen to group the dot before the star,
we could have used parentheses, as (01})* + 1. The language of this expression
is the string 1 and all strings that repeat 01, zero or more times. Had we wished
to group the union first, we could have added parentheses around the union to
make the expression 0(1* + 1). That expression’s language is the set of strings
that begin with 0 and have any number of 1’s following. O

3.1.4 Exercises for Section 3.1

Exercise 3.1.1: Write regular expressions for the following languages:

* a) The set of strings over alphabet {a,b,c} containing at least one a and at
least one 8b.

b) The set of strings of 0’s and 1’s whose tenth symbol from the right end is
1.

c) The set of strings of 0’s and 1’s with at most one pair of consecutive 1's.
Exercise 3.1.2: Write regular expressions for the following languages:

* a) The set of all strings of 0’s and 1’s such that every pair of adjacent 0's
appears before any pair of adjacent 1’s.

b) The set of strings of 0’s and 1’s whose number of 0’s is divisible by five.


--- Page 106 ---
*

90 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

! Exercise 3.1.3: Write regular expressions for the following languages:
a) The set of all strings of 0’s and 1’s not containing 101 as a substring.

b) The set of all strings with an equal number of 0’s and 1’s, such that no
prefix has two more 0’s than 1’s, nor two more 1’s than 0's.

c} The set of strings of 0’s and 1’s whose number of W's is divisible by five
and whose number of 1’s is even.

! Exercise 3.1.4: Give English descriptions of the languages of the following
regular expressions:

* a) (1+ €)(00"1)*0".
b) (0*1*)*000(0 + L)*.

c) (0+ 10)*1".

—

Exercise 3.1.5: In Example 3.1 we pointed out that @ is one of two languages
whose closure is finite. What is the other?

3.2 Finite Automata and Regular Expressions

While the regular-expression approach to describing languages is fundamentally
different from the finite-automaton approach, these two notations turn out to
represent exactly the same set of languages, which we have termed the “reg-
ular languages.” We have already shown that deterministic finite automata,
and the two kinds of nondeterministic finite automata —- with and without
e-transitions — accept the same class of languages. In order to show that the
regular expressions define the same class, we must show that:

1. Every language defined by one of these antomata is also defined by a
regular expression. For this proof, we can assume the language is accepted

by some DFA.

2, Every language defined by a regular expression js defined by one of these
automata. For this part of the proof, the easiest is to show that. there is
an NFA with «transitions accepting the same language.

Figure 3.1 shows all the equivalences we have proved or will prove. An arc from
class X to class Y means that we prove every language defined by class X is
also defined by class Y. Since the graph is strongly connected (i.e., we can get
from each of the four nodes to any other node) we see that all four classes are
really the same.


--- Page 107 ---
3.2, FINITE AUTOMATA AND REGULAR EXPRESSIONS 91

CeNFA \ C_NFA 2

Figure 3.1: Plan for showing the equivalence of four different notations for
regular languages

3.2.1 From DFA’s to Regular Expressions

The construction of a regular expression to define the language of any DFA is
surprisingly tricky. Roughly, we build expressions that describe sets of strings
that label certain paths in the DFA’s transition diagram. However, the paths
are allowed to pass through only a limited subset of the states. In an inductive
definition of these expressions, we start. with the simplest. expressions that de-
scribe paths that are not allowed to pass through any states (ie., they are single
nodes or single ares), and inductively build the expressions that let the paths
go through progressively larger sets of states. Finally, the paths are allowed to
go through any state; i-e., the expressions we generate at the end represent all
possible paths. These ideas appear in the proof of the following theorem.

Theorem 3.4: If Z = L(A) for some DFA A, then there is a regular expression
R such that £ = L(R).

PROOF: Let us suppose that A’s states are {1,2,...,m} for some integer n. No
matter what the states of 4 actually are, there will be n of them for some finite
n, and by renaming the states, we can refer to the states in this manner, as if
they were the first n positive integers. Our first, and most difficult, task is to
construct a collection of regular expressions that describe progressively broader
sets of paths in the transition diagram of A.

Let us use RY) as the name of a regular expression whose language is the
set of strings w such that w is the label of a path from state ¢ to state j in A,
and that path has no intermediate node whose number is greater than &, Note
that the beginning and end points of the path are not “intermediate,” so there
is no constraint that i and/or 7 be less than or equal to k.

Figure 3.2 suggests the requirement on the paths represented by RY. There,
the vertical dimension represents the state, from 1 at the bottom to n at the
top, and the horizontal dimension represents travel along the path. Notice that
in this diagram we have shown both i and j to be greater than k, but either or
both could be & or less. Also notice that the path passes through node & twice,
but never goes through a state higher than k, except at the endpoints.


--- Page 108 ---
92 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

Figure 3.2: A path whose label is in the language of regular expression RY

To construct the expressions RUN), we use the following inductive definition,
starting at k = 0 and finally reaching k = n. Notice that when k& = m, there is
no restriction at all on the paths represented, since there are no states greater
than n.

BASIS: The basis is k = 0. Since all states are numbered 1 or above, the
restriction on paths is that the path must have no intermediate states at all.
There are only two kinds of paths that meet such a condition:

1. An are from node (state) 7 to node 7.

2. A path of length 0 that consists of only some node 2.

If i # j, then only case (1) is possible. We must examine the DFA A and
find those input symbols a such that there is a transition from state i to state
j on symbol a.

a) If there is no such symbol a, then RY = t,

b) If there is exactly one such symbol a, then RY) =a,

c) If there are symbols a), @2,...,@, that label ares from state i to state j,
then RY =a, tagt+---+tay,.

However, if 7 = j, then the legal paths are the path of length 0 and all loops
from 7 to itself. The path of length 0 is represented by the regular expression
€, since that path has no symbols along it. Thus, we add € to the various
expressions devised in (a) through (c} above. That is, in case (a) [no symbol a]
the expression becomes ¢, in case (b) [one symbol a] the expression becomes e+a,
and in case (c) [multiple symbols] the expression becomes ¢ + a, + a9 +:--+a,.

INDUCTION: Suppose there is a path from state 2 to state j that goes through
no state higher than k. There arc two possible cases to consider:


--- Page 109 ---
3.2. FINITE AUTOMATA AND REGULAR EXPRESSIONS 93

1. The path does not go through state & at all. In this case, the label of the
path is in the language of REO”,

2. The path goes through state & at least once. ‘Then we can break the path
into several pieces, as suggested by Fig. 3.3. The first goes from state
i to state & without passing through &, the last piece goes from & to 7
without. passing through &, and all the pieces in the middle go from k
to itself, without passing through &. Note that if the path goes through
state k only once, then there are no “middle” pieces, just a path from «
to k and a path from & to j. The set of labels for all paths of this type
is represented by the regular expression REV REY Ry” . That: is,
the first expression represents the part of the path that gets to state k
the first time, the second represents the portion that gocs from & to itself,
zero times, once, or more than once, and the third expression represents
the part of the path that leaves & for the last time and goes to state 7.

eee ee ell

ee
(k-T] (k-i)
In Rip In Rig

Zero or more strings in R a)

Figure 3.3: A path from ¢ to j can be broken into segments at each point where
it goes through state &

When we combine the expressions for the paths of the two types above, we
have the expression

(A) pk), ply ple -Hy® ple -L)
Rye Ry + Ry Rie Vg;

for the labels of all paths from state é to state j that go through no state higher
than k. If we construct these expressions in order of increasing superscript,
then since each RY depends only on expressious with a smaller superscript,
then all expressions are available when we need them.

Eventually, we have Re for alli and j. We may assume that slate J is the
start state, although the accepting states could be any set of the states. The
regular expression for the language of the automaton is then the sum (union)
of all expressions Re such that state j is an accepting state. O

Example 3.5: Let us convert the DFA of Fig. 3.4 to a regular expression.

This DFA accepts all strings that have at Jeast one 0 in them. To see why, note

that the automaton goes from the start state 1 to accepting state 2 as soon as

it sces an input 0. The automaton then stays in state 2 on all mput sequences.
Below are the basis expressions in the construction of Theorem 3.4.


--- Page 110 ---
94 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

For instance, RY has the term ¢ because the beginning and ending states are
the same, state i. It has the term 1 because there is an arc from state 1 to state
1 on input 1. As another example, Ro is O because there is an are labeled 0
from state 1 to state 2. There is no ¢ term because the beginning and ending
states are different. For a third example, RY) = = ft, because there is no are from
state 2 to state 1.

Now, we must do the induction part, building more complex expressions
that first take into account paths that go through state 1, and then paths that
can go through states 1 and 2, i.c., any path. The rule for computing the
expressions RY are instances of the general rule given in the inductive part of
Theorem 3.4:

RY) = RP + BD RO RO (3.1)
The table in Fig. 3.5 gives first the expressions computed by direct substitution
into the above formula, and then a simplified expression that we can show, by
ad-hee reasoning, to represent the same language as the more complex expres-
sion.

| By direct substitution | Simplified

RY fe+1t (e+ let (e+)
Ry? 0+ (e+1)(e+1)*0
RS) | 0406+. 1)*(e +1)

RY e+O+1+ (6 +1)*0

ee

Figure 3.5: Regular expressions for paths that can go through only state 1

For example, consider RY, Its expression is RS) + RO (RS \*R RY, which
we get from (3.1) by substituting i = 1 and 7 = 2.

To understand the stmplification, note the general principle that if R is any
regular expression, then (€ + R)* = A*. The justification is that both sides of


--- Page 111 ---
3.2. FINITE AUTOMATA AND REGULAR EXPRESSIONS 95

the cquation describe the language consisting of any concatenation of zero or
more strings from L(R}. In our cage, we have (¢ + 1)* = 1*; notice that both
expressions denote any number of 1’s. Further, (¢+1)1" = 1". Again, it can be
observed that both expressions denote “any number of 1’s.” Thus. the original
expression RY is equivalent to 0+ 1*°0. This expression denotes the language
containing the string 0 and all strings consisting of a 0 preceded by any number
of 1’s. This language is also expressed by the simpler expression 1*0.

The simplification of RY is similar to the simplification of RY that we just
considered. The simplification of RY and RS) depends on two rules about
how @ operates. For any regular expression FR:

1. OR = RY—@. That is, is an annihilator for concatenation, it results in
itself when concatenated, either on the left or right, with any expression.
This rule makes sense, because for a string to be in the result of a concate-
nation, we must find strings from both arguments of the concatenation.
Whenever one of the arguments is @, it will be impossible to find a string
from that argument.

9 $4 R=—R+0—R. That is, @ is the identity for union; it results in the
other expression whenever it appears in a union.

As a result, an expression like @(e + 1)*(e€ + 1) can be replaced by @. The last
two simplifications should now be clear.

Now, Ict us compute the expressions RY. The inductive rule applied with
k = 2 pives us:

RD = RY + RORY) Ray (3.2)

If we substitute the simplified expressions trom Fig. 3.5 mto (3.2), we get the
expressions of Fig. 3.6. That figure also shows simplifications following the same
principles that we described for Fig. 3.5.

By direct substitution Simplified
RE] 1" 4+ 1*0(e + 0+ 1)*6 1"
RE ) 1°041°0(e +041)*(€ +041) 1*0(0 + 1)*
R@ | G4 (e+ 04+1)(c+04+1)0 0
| e4OF14 (e+ 04+1)(e+041)(€+041) | +1)"

Figure 3.6: Regular expressions for paths that cam go through any state

The final regular expression equivalent to the automaton of Fig. 3.4 is con-
structed by taking the union of all the expressions where the first. state is the
start state and the second state is accepting. In this example, with 1 as the

start state and 2 as the only accepting state, we need only the expression RY.


--- Page 112 ---
96 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

This expression is 1*°0(0 + 1)*. It is simple to interpret this expression. Its
language consists of all strings that begin with zero or more 1’s, then have a 0,
and then any string of 0’s and 1’s. Put another way, the language is all strings
of 0’s and 1’s with at least one 0. O

3.2.2 Converting DFA’s to Regular Expressions by
Eliminating States

The method of Section 3.2.1 for converting a DFA to a regular expression al-
ways works. In fact, as you may have noticed, it doesn't really depend on the
automaton being deterministic, aud could just as well have been applied to an
NFA or even an «-NFA. However, the construction of the regular expression
is expensive. Not only do we have to construct about n? expressions for an
n-state automaton, but the length of the expression can grow by a factor of 4
on the average, with each of the m inductive steps, if there is no simplification
of the expressions. Thus, the expressions themselves could reach on the order
of 4" symbols,

There is a similar approach that avuids duplicating work at some points.
For example, all of the expressions with superscript (4+ 1) in the construction
of Theorem 3.4 use the same subexpression (RO). the work of writing that
expression is therefore repeated n? times.

The approach to constructing regular expressions that we shall now learn
involves eliminating states. When we eliminate a state s, atl the paths that went
through s no longer exist in the automaton. If the tanguage of the automaton
is not to change, we must include, on an arc that goes directly from ¢ to p,
the labels of paths that went from some state g to state p, through s. Since
the label of this arc may now involve strings, rather than single symbols, and
there may even be an infinite number of such strings, we cannot simply list the
strings as a label. Fortunately, there is a simple, finite way to represent, all such
Strings: use a regular expression.

Thus, we are led to consider automata that have regular expressions as
labels. The language of the automaton is the union over all paths from the
start state to an accepting state of the language formed by concatenating the
languages of the regular expressions along that path. Note that this rule is
consistent with the definition of the language for any of the varieties of automata
we have considered so far. Each symbol a, or ¢ if it is allowed, can be thought
of as a regular expression whose language is a single string, either {a} or {e}.
We may regard this observation as the basis of a state-eliniination procedure,
which we describe next.

Figure 3.7 shows a generic state s about to be eliminated. We suppose that
the automaton of which s is a state has predecessor states ¢1,g2,....9% for s
ahd successor states p),po,--..Dm for s. It is possible that some of the q’s are
also p’s, but: we assume that s is not among the q’s or p's, even if there is a loop
froin ¢ to itself, as suggested by Fig. 3.7. We also show a regular expression on
each arc from one of the q’s to s: expression Q; labels the arc from q;. Likewise,


--- Page 113 ---
3.2. FINITE AUTOMATA AND REGULAR EXPRESSIONS 97

Figure 3.7: A state s about to be eliminated

we show a regular expression P; labeling the arc from s to pj, for all i. We show
a loop on s with label S. Finally, there is a regular expression Ajj on the are
from 9; to p;, for all i and j. Note that some of these arcs may not exist in the
automaton, in which case we take the expression on that arc to be g.

Figure 3.8 shows what happens when we eliminate state g. All arcs involving
state ¢ are deleted. To compensate, we introduce, for each predecessor qj of s
and each successor p; of s, a regular expression that represents all the paths
that start at g;, go to s, perhaps loop around s zero or more times, and finally
go to pj. The expression for these paths is QiS*P;. This expression is added
(with the union operator) to the arc from q; to p;. If there was no arc gq; —-> B;,
then first introduce one with regular expression 9.

The strategy for constructing a regular expression from a finite automaton
is as follows:

1. For cach accepting state ¢, apply the above reduction process to pro-
duce an equivalent automaton with regular-expression labels on the arcs.
Eliminate all states except q and the start state go.

2. If ¢ # qo, then we shall be left with a two-state automaton that looks like


--- Page 114 ---
98

CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

Rit 2, S77,

R,, + QO, S*P

lat

o °o
Oo 0
o o

Ry, + Q, S*F,

R,

+ 0, *P,

m

Figure 3.8: Result of climinating state s froin Fig. 3.7

Fig. 3.9. The regular expression for the accepted strings can be described
in various ways. One is (0+ SU*T)*SU*. In explanation, we can go
from the start state to itself any number of times, by following a sequence
of paths whose labels are in either L(t) or E(SU*T). The expression
SU°*T represents paths that go to the accepting state via a path in £(S),
perhaps return to the accepting state several times using a sequence of
paths with labels in Z(0/), and then return to the start state with a path
whose label is in L(7). Then we must go to the accepting state, never to
return to the start state, by following a path with a label in £(5). Once
in the accepting state, we can return to it as many times as we like, by
following a path whose label is in L(U).

Figure 3.9: A generic two-state automaton

. If the start state is also an accepting state, then we must also perform

@ state-elimination from the original automaton that gets rid of every
state but the start state. When we do so, we are left with a one-state
automaton that looks like Fig. 3.10. The regular expression denoting the


--- Page 115 ---
3.2. FINITE AUTOMATA AND REGULAR EXPRESSIONS 99

strings that it accepts is A*.

Start

Figure 3.10: A generic one-state automaton

4. The desired regular expression is the sum (union) of all the expressions
derived from the reduced automata for each accepting state, by rules (2)
and (3).

0,1

Figure 3.11: An NFA accepting strings that have a 1 either two or three posi-
tions from the end

Example 3.6: Let us consider the NFA in Fig. 3.11 that accepts all strings of
0's and 1’s such that either the second or third position from the end has a 1.
Our first step is to convert it to an automaton with regular expression labels.
Since no state elimination has been performed, all we have to do is replace the
labels “0,1” with the equivalent regular expression 0+ 1. The result is shown
in Fig. 3.12.

0+1

Figure 3.12: The automaton of Fig. 3.11 with regular-expression labels

Let us first eliminate state B. Since this state is neither accepting nor
the start state, it will not be in any of the reduced automata. Thus, we save
work if we eliminate it first, before developing the two reduced automata that
correspond to the two accepting states.

State B has one predecessor, A, and one successor, C. In terms of the
reguiar expressions in the diagram of Fig. 3.7: Q, = 1, P) =041, Ri =
(since the arc from A to C does not exist), and S = @ (because there is no


--- Page 116 ---
100 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

loop at state B), As a result, the expression on the new arc from A to C is
A+ 10"(0 +1).

To simplify, we first eliminate the initial 0, which may be ignored in a union.
The expression thus becomes 19*(0 + 1). Note that the regular expression @*
is equivalent to the regular expression €, since

LO") = {e} U LB) U LIM LB) U--

Since all the terms but the first are empty, we see that L(@*) = {e}, which
is the same as L{e), Thus, 14°(0 + 1) is equivalent to 1{0 + 1), which is the
expression we use for the are A > C in Fig. 3.18,

041

Start (_) 10+ O 0+1

Figure 3.13: Eliminating state B
Now, we must branch, eliminating states C and D in separate reductions.

To eliminate state C’, the mechanics are similar to those we performed above
to eliminate state B, and the resulting automaton is shown in Fig. 3.14.

0+1

san_f-) 10 + )(0+ 1 ~@)

Figure 3.14; A two-state automaton with states A and D

In terms of the generic two-state automaton of Fig. 3.9, the regular expres-
sions from Fig. 3.14 are: R=0O4+1, S$ =1(04+1)(0+1), T=4, and U = 4.
The expression U/* can be replaced by ¢, ie., eliminated in a concatenation;
the justification is that #* — ¢, as we discussed above. Also, the expression
SU*T is equivalent to @, since T, one of the terms of the concatenation, is 9.
The generic expression (R + SU*T)*SU™* thus simplifies in this case to R*S,
or (0 + 1)*1(0 + 1)(0 +1). In informal terms, the language of this expression
is any string ending in 1, followed by two symbols that are each either 0 or
i. That language is one portion of the strings accepted by the automaton of
Fig. 3.81: those strings whose third position from the end has a 1.

Now, we must start again at Fig. 3.13 and eliminate state D instead of C.
Since @ has no successors, an inspection of Fig. 3.7 tells us that there will be
no changes to ares, and the are from C to D is eliminated, along with state D.
The resulting two-state automaton is shown in Fig. 3.15.


--- Page 117 ---
3.2. FINITE AUTOMATA AND REGULAR EXPRESSIONS 101

Ordering the Elimination of States

As we observed in Example 3.6, when a state is neither the start state
nor an accepting state, it gets eliminated in all the derived automata.
Thus, one of the advantages of the state-climination process compared
with the mechanical generation of regular expressions thar we described
in Section 3.2.1 is that we can start by eliminating all the states that.
are neither start nor accepting, once and for all. We only have to begin
duplicating the reduction effort when we need to eliminate some accepting
states.

Even there, we can combine some of the effort. For mstance, if there
are three accepting states p, g, and r, we can eliminate p and then branch
to climinate cither g or r, thus producing the automata for accepting states
rand g, respectively. We then start again with all three accepting states
and eliminate both g and r to get the automaton for p.

Figure 3.15: Two-state automaton resulting from the elimination of 2

This antomaton is very much like that of Fig. 3.14: only the label on the are
from the start state to the accepting state is different. Thus. we can apply the
rule for two-state automata and simplify the expression to get (04 1)71(0+ 1).
This expression represents the other type of string the automaton accepts: those
with a Lin the second position from the end.

All that remains is to sum the two expressions to get the expression for the
entire automaton of Pig. 3.11. This expression is

(0+ 2)71(0 + 1) + (0+ 1)71(0 + 1)(0 +1)

q

3.2.3 Converting Regular Expressions to Automata

We shall now complete the plan of Fig. 3.1 by showing that every language £
that is ECR) for some regular expression A, is also L(F) for some e-NFA #. The
proof is a structural induction on the expression #. We stuart by showing how
to constrict amtomata for the basis expressions: single symbols, «, and @. We
then show how to combine these automata into larger antomata that accept the
union, concatenation, or closure of the language accepted by smaller antomata,


--- Page 118 ---
102 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

All of the automata we construct are ¢-NFA’s with a single accepting state.

Theorem 3.7: Every language defined by a regular expression is also defined
by a finite automaton.

PROOF: Suppose £ = L(R) for a regular expression R. We show that L = L(E)
for some «-NFA # with:

1. Exactly one accepting state.
2. No arcs into the initial state.
3. No ares out of the accepting state.

The proof is by structural induction on 2, following the recursive definition of
regular expressions that we had in Section 3.1.2.

(a)

5 ©

(b)

O (©)
(c)

Figure 3.16: The basis of the construction of an automaton from a regular
expression

BASIS: There are three parts to the basis, shown in Fig. 3.16. In part (a) we
see how to handle the expression ¢. The language of the automaton is easily
seen to be fe}, since the only path from the start state to an accepting state
is labeled €. Part (b) shows the construction for §. Clearly there are no paths
fron start state to accepting state, so @ is the language of this automaton.
Finally, part (c) gives the automaton for a regular expression a. The language
of this automaton evidently consists of the one string a, which is also L{a). It
is easy to check that these automata all satisfy conditions (1}, (2), and (3) of
the inductive hypothesis.


--- Page 119 ---
3.2. FINITE AUTOMATA AND REGULAR EXPRESSIONS 103

Figure 3.17: The inductive step in the regular-expression-to-e-NFA construction

INDUCTION: The three parts of the induction are shown in Fig. 3.17. We
assume that the statement of the theorem is true for the immediate subexpres-
sions of a given regular expression; that is, the languages of these subexpressions
are also the languages of e-NFA’s with a single accepting state. The four cases
are:

1. The expression is R + S for some smaller expressions R and S. Then the
automaton of Fig. 3.17(a) serves. That is, starting at the new start state,
we can go to the start state of either the automaton for F or the automa-
ton for S. We then reach the accepting state of one of these automata,
following a path labeled by some string in L(A) or £(S}, respectively.
Once we reach the accepting state of the automaton for R or S, we can
follow one of the «-arcs to the accepting state of the new automaton.


--- Page 120 ---
104 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

Thus, the language of the automaton in Fig. 3.17{a) is UR) U £(S).

2. The expression is #5 for some smaller expressions R and S. The automa-
ton for the concatenation is shown in Fig. 3.17(b). Note that the start
state of the first automaton becomes the start state of the whole, and the
accepting state of the second automaton becomes the accepting state of
the whole. The idea is that the only paths from start to accepting state go
first. through the automaton for A, where it must follow a path labeled by
a string in £(9), and then through the automaton for S, where it follows
a path labeled by a string in £($). Thus, the paths in the automaton of
Fig. 3.17(b) are all and only those labeled by strings in L(R)L(S).

3. The expression is R* for some smaller expression R. Then we use the
automaton of Fig. 3.17(c). That automaton allows us to go either:

(a) Directly from the start state to the accepting state along a path
labeled ¢«. That path lets us accept ¢, which is in LC") no matter
what expression # is.

(b) To the start state of the automaton for R, through that automaton
one or more times, and then to the accepting state. This set of paths
allows us to accept strings in L(A), LCR)L(R), L(R)L(R)L(R), and
so on, thus covering all strings in L(A*) except perhaps €, which was
covered by the direct arc to the accepting state mentioned in (3a).

4. The expression is (#} for some smaller expression 2. The automaton
for & also serves as the automaton for (2), since the parentheses do not
change the language defined by the expression.

It is a simple observation that the constructed automata satisfy the three con-
ditions given in the inductive hypothesis — one accepting state, with no arcs
into the initial state or out of the accepting state. O

Example 3.8: Let us convert the regular expression (0 + 1)°1(0 +4 1) to an
e-NFA. Our first step is to construct an automaton for 0 +1. We use two
automata constructed according to Fig. 3.16(c), one with label 0 on the arc
and one with label 1. These two automata are then combined using the union
construction of Fig. 3.17({a). The result is shown in Fig. 3.18(a).

Next, we apply to Fig. 3.18(a) the star construction of Fig. 3.17(c). This
automaton is shown in Fig. 3.18(b). The last two steps involve applying the
concatenation construction of Fig. 3.17(b). First, we connect the automaton of
Fig. 3.18(b) to another automaton designed to accept only the string 1. This
automaton is another application of the basis construction of Fig. 3.16(c) with
label 1 on the are. Note that we must create a new automaton to recognize 1;
we must. not use the automaton for 1 that was part of Fig. 3.18(a). The third
automaton in the concatenation is another automaton for 0+ 1. Again, we
must create a copy of the automaton of Fig. 3.18(a); we must not, use the same
copy that became part of Fig. 3.18(b}. The complete automaton is shown in


--- Page 121 ---
3.2, FINITE AUTOMATA AND REGULAR EXPRESSIONS 105

Figure 3.18: Automata constructed for Example 3.8


--- Page 122 ---
106 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

Fig. 3.18(c). Note that this «NFA, when e-transitions are removed, looks just
like the much simpler automaton of Fig. 3.15 that also accepts the strings that
have a ] in their next-to-last position. O

3.2.4 Exercises for Section 3.2

Exercise 3.2.1: Here is a transition table for a DFA:

| 0 | 1

* a) Give all the regular expressions RS). Note: Think of state ¢; as if it were
the state with integer number 3.

* b) Give all the regular expressions RY. Try to simplify the expressions as

much as possible.

oe

c) Give all the regular expressions Re Try to simplify the expressions as

ag
much as possible.

d

—

Give a regular expression for the language of the automaton.

* ¢) Coustruct the transition diagram for the DFA and give a regular expres-
sion for its language by eliminating state qo.

Exercise 3.2.2: Repeat Exercise 3.2.1 for the following DFA:

Note that solutions to parts (a), (b) and (e) are net available for this exercise.

Exercise 3.2.3: Convert the following DFA to a regular expression, using the
state-elimination technique of Section 3.2.2.

__ ie}

1
B
s
q
r

8
D
r
¢

Exercise 3.2.4: Convert the following regular expressions to NFA’s with e¢-
transitions.


--- Page 123 ---
3.2. FINITE AUTOMATA AND REGULAR EXPRESSIONS 107

* a) O1*.
b) (0+ 1)01.
c) 00(0 + 1)*.

Exercise 3.2.5: Eliminate e-transitions from your e-NFA’s of Exercise 3.2.4.
A solution to part (a) appears in the book’s Web pages.

! Exercise 3.2.6: Let A = (Q, 2,46, q0, {gy }} be an «-NFA such that there are no
transitions into gp and no transitions out of gy. Describe the language accepted
by each of the following modifications of A, in terms of L = E(.A):

* a) The automaton constructed from A by adding an e-transition from qz to
qo-

* b) The automaton constructed from A by adding an e-transition from qo
to every state reachable from go (along a path whose labels may include
symbols of © as well as €).

c) The automaton constructed from 4 by adding an e-transition to gy from
every state that can reach qs along some path.

d) The automaton constructed from 4 by doing both (b) and (c).

!! Exercise 3.2.7: There are some simplifications to the constructions of Theo-
rem 3.7, where we converted a regular expression to an e-NFA. Here are three:

1. For the union operator, instead of creating new start and accepting states,
merge the two start states into one state with all the transitions of both
start states. Likewise, merge the two accepting states, having all transi-
tions to either go to the merged state instead.

2. For the concatenation operator, merge the accepting state of the first
automaton with the start state of the second.

3. For the closure operator, simply add e-transitions fron: the accepting state
to the start state and vice-versa.

Each of these simplifications, by themselves, still yield a correct construction;
that is, the resulting «-NFA for any regular expression accepts the language of
the expression. Which subsets of changes (1), (2), and (3) may be made to the
construction together, while still yielding a correct automaton for every regular
expression?

*!

Exercise 3.2.8: Give an algorithm that takes a DFA A and computes the
number of strings of length n (for some given n, not related to the number
of states of A) accepted by A. Your algorithm should be polynomial in both
n and the number of states of A. Hint: Use the technique suggested by the
construction of Theorem 3.4.


--- Page 124 ---
108 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

3.3 Applications of Regular Expressions

A regular expression that gives a “picture” of the pattern we want to recognize
is the medium of choice for applications that search for patterns in text. The
regular expressions are then compiled, behind the scenes, into deterministic or
nondeterministic automata, which are then simulated to produce a program
that recognizes patterns in text. In this section, we shall consider two impor-
tant classes of regular-expression-based applications: lexical analyzers and text
search.

3.3.1 Regular Expressions in UNIX

Before seeing the applications, we shall introduce the UNIX notation for ex-
tended regular expressions. This notation gives us a number of additional ca-
pabilities. In fact, the UNIX extensions include certain features, especially the
ability to name and refer to previous strings that have matched a pattern, that
actually allow nonregular languages to be recognized. We shall not consider
these features here; rather we shall only introduce the shorthands that allow
complex regular expressions to be written succinctly.

The first enhancement to the regular-expression notation concerns the fact
that most real applications deal with the ASCII character set. Our examples
have typically used a small alphabet, such as {0,1}. The existence of only two
symbols allowed us to write succinct expressions like 0+ 1 for “any character.”
However, if there were 128 characters, say, the same expression would involve
listing them all, and would be highly inconvenient to write. Thus, UNIX reg-
ular expressions allow us to write character classes to represent large sets of
characters as succinctly as possible. The rules for character classes are:

« The symbol . (dot) stands for “any character.”
e The sequence Ca,d2---a,%] stands for the regular expression
@, H@o+---+ ap

This notation saves about half the characters, since we don’t have to write
the +-signs. For example. we could express the four characters used in C
comparison operators by [<>=!].

Between the square braces we can put a range of the form 2-y to mean all
the characters from x to y in the ASCII sequence. Since the digits have
codes in order, as do the upper-case letters and the lower-case letters, we
can express many of the classes of characters that we really care about
with just a few keystrokes. For example, the digits can be expressed
[0-9], the upper-case letters can be expressed [A-Z], and the set of all
letters and digits can be expressed [A-Za-z0-9]. If we want to include a
minus sign among a list of characters, we can place it first or last, so it is
not confused with its use to form a character range. For example, the set


--- Page 125 ---
3.8. APPLICATIONS OF REGULAR EXPRESSIONS 109

of digits, plus the dot, plus, and minus signs that are used to form signed
decimal numbers may be expressed [-+.0-9]. Square brackets, or other
characters that have special meanings in UNIX regular expressions can
be represented as characters by preceding them with a backslash (\).

e There are special notations for several of the most common classes of
characters. For instance:

a) [:digit:] is the set of ten digits, the same as [0-9} 3
b) [:alpha:] stands for any alphabetic character, as docs [A-Za-z].

c) [:alnum:] stands for the digits and letters (alphabetic and numeric
characters), as does [A-Za-z0-9].

In addition, there are several operators that are used in UNIX regular ex-
pressions that we have not encountered previously. None of these operators
extend what languages can be expressed, but they sometimes make it easier to
express what we want.

1. The operator | is used in place of + to denote union.

2. The operator ? means “zero or one of.” Thus, R? in UNIX is the same
as €+ Rin this book’s regular-expression notation.

3. The operator + means “one or more of.” Thus, A+ in UNIX is shorthand
for RAR* in our notation.

4. The operator {n} means “n copies of.” Thus, R{5} in UNIX is shorthand
for RRR.

Note that UNIX regular expressions allow parentheses to group subexpressions,
just as for the regular expressions described in Section 3.1.2, and the same
operator precedence is used (with 7, + and {n} treated like * as far as precedence
is concerned). The star operator * is used in UNIX (without being a superscript,
of course) with the same meaning as we have used.

3.3.2 Lexical Analysis

One of the oldest applications of regular expressions was in specifying the com-
ponent of a compiler called a “lexical analyzer.” This component scans the
source program and recognizes all tokens, those substrings of consecutive char-
acters that belong together logically. Keywords and identifiers are common
examples of tokens, but there are many others.

’'The notation [:digit:] has the advantage that should some code other than ASCII be
used, including a code where the digits did not have consecutive codes, [:digit:] would still
represent [0123456789], while [0-9] would represent whatever characters had codes betweon
the codes for 0 and 9, inclusive.


--- Page 126 ---
110 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

The Complete Story for UNIX Regular Expressions

The reader who wants to get the complete list of operators and short-
hands available in the UNIX regular-expression notation can find them
in the manual pages for various commands. There are some differences
among the various versions of UNIX, but a command like man grep will
get you the notation used for the grep command, which is fundamental.
“Grep” stands for “Global (search for) Regular Expression and Print,”
incidentally.

The UNIX command lex and its GNU version flex, accept as input a list of
regular expressions, in the UNIX style, each followed by a bracketed section of
code that indicates what the lexical analyzer is to do when it finds an instance
of that token. Such a facility is called a lezical-analyzer generator, because it
takes as input a high-level description of a lexical analyzer and produces from
it a function that is a working lexical analyzer.

Commands such as lex and flex have been found extremely useful because
the regular-expression notation is exactly as powerful as we need to describe
tokens. These commands are able to use the regular-expression-to-DFA con-
version process to generate an efficient function that breaks source programs
into tokens. They make the implementation of a lexical analyzer an afternoon’s
work, while before the development of these regular-expression-based tools, the
hand-generation of the lexical analyzer could take months. Further, if we need
to modify the lexical analyzer for any reason, it is often a simple matter to
change a regular expression or two, instead of having to go into mysterious
code to fix a bug.

Example 3.9: In Fig. 3.19 is an example of partial input to the lex command,
describing some of the tokens that are found in the language C. The first line
handles the keyword else and the action is to return a symbolic constant {ELSE
in this example) to the parser for further processing. The second line contains
a regular expression describing identifiers: a letter followed by zero or more
letters and/or digits. The action is first to enter that identifier in the symbol
table if not already there; lex isolates the token found in a buffer, so this piece
of cude knows exactly what identifier was found. Finally, the lexical analyzer
returns the symbolic constant ID, which has been chosen in this example to
represent identifiers.

The third entry in Fig. 3.19 is for the sign >=, a two-character operator.
The last example we show is for the sign =, a one-character operator. There
would in practice appear expressions describing each of the keywords, each of
the signs and punctuation symbols like commas and parentheses, and families
of constants such as numbers and strings. Many of these are very simple,
just a sequence of one or more specific characters. However, some have more


--- Page 127 ---
3.3. APPLICATIONS OF REGULAR EXPRESSIONS 111

elsa {return (ELSE) ;}

[A-Za-z] [A-Za-z0-9] * {code to enter the found identifier
in the aymbol table;
return(ID);

}
>= {return (GE) ;}

= {return (EQ) :}

Figure 3.19: A sample of lex input

of the flavor of identifiers, requiring the full power of the regular-expression
notation to describe. The integers, floating-point numbers, character strings,
and comments are other examples of sets of strings that profit from the regular-
expression capabilities of commands like lex. ©

The conversion of a collection of expressions, such as those suggested in
Fig. 3.19, to an automaton proceeds approximately as we have described for-
mally in the preceding sections. We start by building an automaton for the
union of all the expressions. This automaton in principle tells us only that
some token has been recognized. However, if we follow the construction of The-
orem 3.7 for the union of expressions, the «-NFA state tells us exactly which
token has been recognized.

The only problem is that more than one token may be recognized at once;
for instance, the string else matches not. only the regular expression else but
also the expression for identifiers. The standard resolution is for the lexical-
analyzer generator to give priority to the first expression listed. Thus, if we
want keywords like else to be reserved (not usable as identifiers), we simply
list them ahead of the expression for identifiers.

3.3.3 Finding Patterns in Text

In Section 2.4.1 we introduced the notion that automata could be used to search
efficiently for a set of words in a large repository such as the Web. While the
tools and technology for doing so are not so well developed as that for lexical
analyzers, the regular-expression notation is valuable for describing searches
for interesting patterns. As for lexical analyzers, the capability to go from
the natural, descriptive regular-expression notation to an efficient (automaton-
based) implementation offers substantial intellectual leverage.


--- Page 128 ---
112 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

The general problem for which regular-expression technology has been found
useful is the description of a vaguely defined class of patterns m text. The
vagueness of the description virtually guarantees that we shall not describe
the pattern correctly at first — perhaps we can never get exactly the right
description. By using regular-cxpression notation, it becomes easy to describe
the patterns at a high level, with little effort, and to modify the description
quickly when things go wrong. A “compiler” for regular expressions is useful
to turn the expressions we write into executable code.

Let us explore an extended example of the sort of problem that arises in
many Web applications. Suppose that we want to scan a very large number of
Web pages and detcct addresses. We might simply want to create a mailing
list. Or, perhaps we are trying to classify businesses by their location so that
we can answer queries like “find me a restaurant within 10 minutes drive of
where I arm now.”

We shall focus on recognizing street addresses in particular. What is a street
address? We'll have to figure that out, and if, while testing the software, we
find we miss some cases, we'll have to modify the expressions to capture what
we were missing. To begin, a street address will probably end in “Street” or its
abbreviation, “St.” However, some people live on “Avenues” or “Roads,” and
these might be abbreviated in the address as well. Thus, we might use as the
ending for our regular expression something like:

Street |St\. |Avenue|Ave\. |Road|Rd\,

In the above expression, we have used UNIX-style notation, with the vertical
bar, rather than +, as the union operator. Note also that the dots are escaped
with a preceding backslash, since dot has the special meaning of “any character”
in UNIX expressions, and in this case we really want only the period or “dot”
character to end the three abbreviations.

The designation such as Street must be preceded by the name of the street.
Usually, the name is a capital letter followed by some lower-case letters. We
can describe this pattern by the UNIX expression (A-Z] [a-z]*. However,
some streets have a name consisting of more than one word, such as Rhode
Island Avenue in Washington DC. Thus, after discovering that we were missing
addresses of this form, we could revise our description of street names to be

* (A-2] [a-z] *( [A-Z] [a-z]*)*?

The expression ahove starts with a group consisting of a capital and zero
or more lower-case letters. There follow zero or more groups consisting of a
blank, another capital letter, and zero or more lower-case letters. The blank
is an ordinary character in UNIX expressions, but to avoid having the above
expression look like two expressions separated by a blank in a UNIX command
line, we are required to place quotation marks around the whole expression,
The quotes are not part of the expression itself.


--- Page 129 ---
—

3.3. APPLICATIONS OF REGULAR EXPRESSIONS 113

Now, we need to include the house number as part of the address. Most
house numbers are a string of digits. However, some will have a letter follow-
ing, as in “123A Main St.” Thus, the expression we use for numbers has an
optional capital letter following: [0-9]+[A-Z]?. Notice that we use the UNIX
+ operator for “one or more” digits and the ? operator for “zero or one” capital
letter. The entire expression we have developed for street addresses is:

*[0-9)+[(A-Z]? [A-Z] [a-z]*¢ [A-Z] [a-zl*)*
(Street |5t\. |AvenuelAve\. |Road|Rd\.)?

If we work with this expression, we shall do fairly well. However, we shall
eventually discover that we are missing:

1. Streets that are called something other than a street, avenue, or road. For
example, we shall miss “Boulevard,” “Place,” “Way,” and their abbrevi-
ations.

2. Strect names that arc numbers, or partially numbers, like “42nd Street.”
3. Post-Office boxes and rural-delivery routes.

4. Street names that don’t end in anything like “Street.” An example is El
Camino Real in Silicon Valley. Being Spanish for “the royal road,” saying
“Ei Camino Real Road” would be redundant, so one has to deal with
complete addresses like “2000 El Camino Real.”

5. All sorts of strange things we can’t even imagine. Can you?

Thus, having a regular-expression compiler can make the process of slow con-
vergence to the complete recognizer for addresses much easier than if we had
to recode every change directly in a conventional programming language.

3.3.4 Exercises for Section 3.3

Exercise 3.3.1: Give a regular expression to describe phone numbers in all
the various forms you can think of. Consider international numbers as well as
the fact that different countries have different numbers of digits in arca codes
and in local phone numbers.

Exercise 3.3.2: Give a regular expression to represent salaries as they might
appear in employment advertising. Consider that salaries might be given on
a per hour, week, month, or year basis. They may or may not appear with a
dollar sign, or other unit such as “K” following. There may be a word or words
nearby that identify a salary. Suggestion: look at classified ads in a newspaper,
or on-line jobs listings to get an idea of what patterns might be useful.

Exercise 3.3.3: At the end of Section 3.3.3 we gave some examples of improve-
ments that could be possible for the regular expression that describes addresses.
Modify the expression developed there to include all the mentioned options.


--- Page 130 ---
114 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

3.4 Algebraic Laws for Regular Expressions

In Example 3.5, we saw the need for simplifying regular expressions, in order to
keep the size of expressions manageable. There, we gave some ad-hoc arguments
why one expression could be replaced by another. In all cases, the basic issue
was that the two expressions were equtvalent, m the sense that they defined
the same languages. In this section, we shall offer a collection of algebraic
laws that bring to a higher level the issue of when two regular expressions are
equivalent. Instead of examining specific regular expressions. we shall consider
pairs of regular expressions with variables as argumicnts. Two expressions with
variables are equivalent if whatever languages we substitute for the variables,
the results of the two expressions are the same language.

An example of this process in the algebra of arithmetic is as follows. It is
one matter to say that 1+2 = 2+1. That is au example of the commutative law
of addition, and it is easy to check by applying the addition operator on both
sides and getting 3 = 3. However, the commutative law of addition says more;
it says that 2+y = y +a, where x and y are variables that can be replaced
by any two numbers. That is, no matter what two numbers we add, we get the
same result regardless of the order in which we sum thet.

Like arithmetic expressions, the regular expressions have a number of laws
that work for them. Many of these are similar to the laws for arithmetic, if we
think of union as addition and concatenation as multiplication. However, there
are a few places where the analogy breaks down, and there are also some laws
that apply to regular expressions but have no analog for arithmetic, especially
when the closure operator is involved. The next sections form a catalog of the
major laws. We conclude with a discussion of how one can check whether a
proposed law for regular expressions is indeed a law; i-e., it will hold for any
languages that we may substitute for the variables.

3.4.1 Associativity and Commutativity

Commutativity is the property of an operator that says we can switch the order
of its operands and get the same result. An example for arithmetic was given
above: z +y = y +a. Associativity is the property of an operator that allows
us lo regroup the operands when the operator is applied twice. For example,
the associative law of multiplication is (2 x y) x z = 2 x (y x z). Here are three
laws of these types that hold for regular expressions:

e L+M=M+4E. This law, the commutative law for union, says that we
may take the union of two languages in cither order.

°e (L+M)+N =L4+(M+N). This law, the associative law for union,
says that we may take the union of three languages either by taking the
union of the first two initially, or taking the union of the last two initially.
Note that, together with the commmtative law for union, we conclude
that we can take the union of any collection of languages with any order


--- Page 131 ---
3.4. ALGEBRAIC LAWS FOR REGULAR EXPRESSIONS 115

and grouping, and the result will be the same. Intuitively, a string is in
£,;U£oU---U Ly if and only if it is in one or more of the £,’s.

« (LM)N = L(MN). This law, the associative lew for concatenation, says
that we can concatenate three languages by concatenating either the first
two or the last two initially.

Missing from this list is the “law” DAf = ML, which would say that con-
catenation is commutative. However, this law is false.

Example 3.10: Consider the regular expressions 01 and 10. These expres-
sions denote the languages {01} and {10}, respectively. Since the languages are
different the general law LM = ME cannot hold. If it did, we could substitute
the regular expression 0 for Z and 1 for M and conclude falsely that O01 = 10.
Oo

3.4.2 Identities and Annihilators

An identity for an operator is a value such that when the operator is applied to
the identity and some other value, the result is the other value. For instance,
0 is the identity for addition, since 0+ 2 = 2+0= 2, and 1 is the identity
for multiplication, since 1 x g = 2 x1= x. An annthilator for an operator
is a value such that when the operator is applied to the annihilator and some
other value, the result is the annihilator. For instance, 0 is an aunihilator for
multiplication, since 0 x z =z x 0=0. There is no annihilator for addition.

There are three laws for regular expressions involving these concepts; we list.
them below.

e@4+L=L+4+6= 2. This law asserts that @ is the identity for union.
e «L = Le = L. This law asserts that ¢ is the identity for concatenation.
« #£ = L@= 9. This law asserts that @ is the annihilator for concatenation.

These laws are powerful tools in simplifications. For example, if we have a
union of several expressions, some of which are, or have been simplified to 4,
then the #*s can be dropped from the union. Likewise, if we have a concatenation
of several expressions, some of which are, or have been simplified to «, we can
drop the e’s from the concatenation. Finally, if we have a concatenation of any
number of expressions, and even one of them is 9, then the entire concatenation
can be replaced by 9.

3.4.3 Distributive Laws

A distributive law involves two operators, and asserts that one operator can be
pushed down to be applied to each argument of the other operator individually.
The most common example from arithmetic is the distributive law of multipli-
cation over addition, that is, 2x (y+z)=2xy+2xz. Since multiplication is


--- Page 132 ---
116 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

commutative, it doesn’t matter whether the multiplication is on the left or right
of the sum. However, there is an analogous law for regular expressions, that. we
must state in two forms, since concatenation is not commutative. These laws
are:

e L(M+N)=2LM+4+ EN. This law, is the left distributive law of concate-
nation over union.

© (Af4 N)L = ML+ NL. This law, is the right distributive law of con-
catenation over union.

Let us prove the left distributive law; the other is proved similarly. The
proof wil] refer to languages only; it does not depend on the languages having
regular expressions.

Theorem 3.11: Uf £, 4, and N are any languages, then
E(MUN)=ELMULN

PROOF: The proof is similar to another proof about a distributive law that we
saw in Theorem 1.10. We need first to show that a string w is in L(M U N) if
and only if it isin LAY U EN.

(Only if) If w is in L(Af UN), then w = zy, where x isin Z and y is in either
M or N. Ify isin M, then cy is in LM, and therefore in LAf U LN. Likewise,
if y is in N, then zy is in LN and therefore in LM ULN.

(If) Suppose w is in LAL U LN. Then w is in either DM or in LN. Suppose
first that wisin LM. Then w = xy, where c isin 2 and y isin M. As y is in
M, it is also in MUN. Thus, ay isin 2OM UN). If w is not in EM, then it
is surely in EN, and a similar argument shows it isin Z(AY UN). O

Example 3.12: Consider the regular expression 0+ 01". We can “factor out a
0” from the union, but first we have to recognize that the expression O by itself
is actually the concatenation of 0 with something, namely «. That is, we use
the identity law for concatenation to replace 0 by Oe, giving us the expression
Oe +01*. Now, we can apply the left distributive law to replace this expression
by Ofc +1*). If we further recognize that « is in £(1*), then we observe that
é+1* =1*, and can simplify to 01". O

3.4.4 The Idempotent Law

An operator is said to be zdempotent if the result of applying it to two of the
same values as arguments is that value. The common arithmetic operators are
not idempotent; «+z 4 z in general and z x z # x in general {although there
are some values of z for which the equality holds, such as 0+ 0 = 0). However,
union and intersection are common examples of idempotent operators. Thus,
for regular expressions, we may assert the following law:


--- Page 133 ---
3.4. ALGEBRAIC LAWS FOR REGULAR EXPRESSIONS 117

e L+L=L. This law, the idempotence law for union, states that Hf we
take the union of two identical expressions, we can replace them by one
copy of the expression.

3.4.5 Laws Involving Closures

There are a number of laws involving the closure operators and its UNIX-style
variants + and ?, We shall list them here, and give some explanation for why
they are true.

e (L")* = L*, This law says that closing an expression that is already
closed does not change the language. The language of (L*)” is all strings
created by concatenating strings in the language of Z*. But those strings
are themselves composed of strings from Z. Thus, the string in (L*)* is
also a concatenation of strings from ZL and is therefore in the language of
cD.

e §* =e. The closure of @ contains only the string €, as we discussed in
Example 3.6.

* «¢* =e. It is easy to check that the only string that can be formed by
concatenating any number of copies of the empty string is the empty
string itself.

eo Lt =LL* = L*L. Recall that £7 is defined to be D+ 254 2LL4+-:-.
Also, DY =e + h+LL4+ £55 +---. Thus,

LL* =Le+Lit+fLil+LLiLy+.--

When we remember that Le = L, we sec that the infinite expansions for
EL* and for £+ are the same. That proves Lt = LL*. The proof that
L* = L*L is similar.’

e L*=L+ +e. The proof is easy, since the expansion of Z* includes every
term in the expansion of L* except e. Note that if the language Z contains
the string ¢, then the additional “+e” term is not needed; that is, Lt = [*
in this special case.

e L?=e+8. This rule is really the definition of the ? operator.

3.4.6 Discovering Laws for Regular Expressions

Each of the laws above was proved, formally or informally. However, there is
an infinite variety of laws about regular expressions that might be proposed.
Is there a general methodology that will make our proofs of the correct laws

4Notice that, as a consequence, any language £ commutes (under concatenation) with its
own closure; iL" = £"8. That rule does not contradict the fact that, in general, concatena-
tion is not commutative.


--- Page 134 ---
118 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

easy? It turns out that the truth of a law reduces to a question of the equality
of two specific languages. Interestingly, the technique is closely tied to the
regular-expression operators, and cannot be extended to expressions involving
some other operators, such as intersection.

To see how this test works, let us consider a proposed law, such as

(L+ iM)" = (0*M")*

This law says that if we have any two languages £ and Af, and we close their
union, we get the same language as if we take the language £*M™, that is,
all strings composed of zero or more choices from £ followed by zero or more
choices from Mf, and close that language.

To prove this law, suppose first that string w is in the language of (£+ \4}*.5
Then we can write w= w)wes---w, for some &, where each w, is in either E or
AM. It follows that each w; is in the language of £*A¢*. To see why, if w is in
LE, pick one string, ;, from £; this string is also in L*. Pick no strings from
M; that is, pick « from A¢*. If w; is in M, the argument is similar. Once every
w; is seen to be in £*Mé*, it follows that w is in the closure of this language.

To complete the proof, we also have to prove the converse: that strings
in (L*Af*)* are also in (2 + AY)*. We omit this part of the proof, since our
objective is net to prove the law, but to notice the following important property
of regular expressions.

Any regular expression with variables can be thought of as a concrete regular
expression, one that has no variables, by thinking of each variable as if it were a
distinct symbol. For example, the expression (£+44)* can have variables L and
Mf replaced by symbols @ and 6, respectively, giving us the regular expression
{a+b)*.

The language of the concrete expression guides us regarding the form of
strings in any Janguage that is formed from the original expression when we
replace the variables by languages. Thus, in our analysis of (LE + AV)", we
observed that any string w composed of a sequence of choices from either £ or
M, would be in the language of (£ + Af)". We can arrive at that conclusion
by looking at the language of the concrete expression, L((a + b)"}, which is
evidently the set of all strings of a’s and 8's. We could substitute any string in
J, for any occurrence of @ in one of those strings, and we could substitute any
string in AJ for any occurrence of 6, with possibly different choices of strings for
different occurrences of a or b. Those substitutions, applied to all the strings
in (a+ b)*, gives us all strings formed by concatenating strings from £ and/or
AM, in any order.

The above statement may seem obvious, but as is pointed out in the box
on “Extensions of the Test Beyond Regular Expressions May Fail,” it is not
even truc when some other operators are added to the three regular-expression
operators. We prove the general principle for regular expressions in the next
theorem.

*For simplicity, we shall identify the regular expressions and their languages. and avoid
saying “the language of in front of every regular expression,


--- Page 135 ---
3.4. ALGEBRAIC LAWS FOR REGULAR EXPRESSIONS 119

Theorem 3.13: Let & be a regular expression with variables L,, Lo,...,Lm.
Form concrete regular expression C’ by replacing each occurrence of L; by the
symbol a;, for i = 1,2,...,m. Then for any languages £3, £2,...,Lm, every
string w in L(£) can be written w = w)wa-+-w,, where each w; is in one of
the languages, say L;,, and the string aj,aj,--+a@;, is in the language D(C).
Less formally, we can construct L(£) by starting with each string in L(C),
Say @;,a;,-°°a;,, and substituting for each of the aj,’s any string from the
corresponding language L,.

PROOF: The proof is a structural induction on the expression B.

BASIS: The basis cases are where E is e, §, or a variable Z. In the first two
cases, there is nothing to prove, since the concrete expression C is the same as
E. If E is a variable L, then L(E) = L. The concrete expression C is just a,
where a is the symbol corresponding to LE. Thus, L(C) = {a}. If we substitute
any string in ZL for the symbol a in this one string, we get the language L, which
is also L(E).

INDUCTION: There are three cases, depending on the final operator of E.
First, suppose that E = F'+ G; i.e., a union is the final operator. Let C and D
be the concrete expressions formed from F and G, respectively, by substituting
concrete symbols for the language-variables in these expressions. Note that the
same symbol must be substituted for all occurrences of the same variable, in
both F and G. Then the concrete expression that we get from EF is C+ D, and
E(C + D) = L{(C) + LD).

Suppose that w is a string in L(£), when the language variables of F are
replaced by specific languages. Then w is in either L(F) or L(G). By the
inductive hypothesis, w is obtained by starting with a concrete string m L(C) or
L(D), respectively, and substituting for the symbols strings in the corresponding
languages. Thus, in either case, the string w can be constructed by starting
with a concrete string in L(C + D), and making the same substitutions of strings
for syinbols.

We must also consider the cases where E is FG or F*, However, the ar-
guments are similar to the union case above, and we leave them for you to
complete. O

3.4.7 The Test for a Regular-Expression Algebraic Law

Now, we can state and prove the test for whether or not a law of regular
expressions is true. The test for whether £ = F is truce, where & and F are
two regular expressions with the same set of variables, is:

1. Convert E and F' to concrete regular expressions C and D, respectively,
by replacing each variable by a concrete symbol.

2, Test whether L(C) = L(D). If so, then B = F is a true law, and if not,
then the “law” is false. Note that we shall not see the test for whether two


--- Page 136 ---
120 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

regular expressions denote the same language until Section 4.4. However,
we Can use ad-hoc means to decide the equality of the pairs of languages
that we actually care about. Recall that if the languages are not the same,
than it is sufficient to provide one counterexample: a single string that is
in one language but not the other.

Theorem 3.14; The above test correctly identifies the true laws for regular
expressions.

PROOF: We shall show that L(£) = £(F) for any languages in place of the
variables of F and F if and only if £(C) = L(D).

(Only-if} Suppose L(E) = L(F) for all choices of languages for the variables.
In particular, choose for every variable £ the concrete symbol a that replaces £
in expressions C and 2. Then for this choice, £(C) = L(&), and L(D) = L(F).
Since E() = L(F) is given, it follows that £(C) = £(D).

(If) Suppose £(C) = LE(DP). By Theorem 3.13, L(£) and L(F) are each
constructed by replacing the concrete symbols of strings in L(C) and L(D),
respectively, by strings in the languages that correspond to those symbols. If
the strings of £(C’) and L(D) are the same, then the two languages constructed
in this manner will also be the same; that is, L(£) = £(F). O

Example 3.15: Consider the prospective law (2 + M)* = (L*M*)*. If we
replace variables L and M hy concrete symbols « and 0 respectively, we get the
regular expressions (a + b)* and (a*b*)*. It is easy to check that both these
expressions denote the language with all strings of a’s and b's. Thus, the two
concrete expressions denote the same language, and the law holds.

For another example of a law, consider £* = £°£*. The concrete languages
are a* aud aa", respectively, and each of these is the sct of all strings of a’s.
Again, the law is found to hold; that is, concatenation of a closed language with
itself yields that language.

Finally, consider the prospective law £4 AFL = (L+ ML. If we choose
symbols a and 6 for variables Z and Ad, respectively, we have the two concrete
regular expressions a + ba and (a+ bja. However, the languages of these
expressions are not the same. Fur example, the string aa is in the second, but
not the first. Thus, the prospective law is false. O

3.4.8 Exercises for Section 3.4
Exercise 3.4.1: Verify the following identities involving regular expressions.
*a) R+S=S+R.

b) (R+S5)4+7T=R+(547).

c) (R8)T = R(ST}.


--- Page 137 ---
3.4. ALGEBRAIC LAWS FOR REGULAR EXPRESSIONS 121

Extensions of the Test Beyond Regular Expressions
May Fail

Let us consider an extended regular-expression algebra that includes
the intersection operator. Interestingly, adding M to the three regular-
expression Operators does not increase the set of languages we can de-
scribe, as we shall see in Theorem 4.8. However, it duces make the test for
algebraic laws invalid.

Consider the “law? DO AON = £9 M; that is, the intersection of
any three languages is the same as the intersection of the first. two of these
languages. This “law” is patently false. For example, let L = Af = {a}
and N = @. But the test based on concretizing the variables would fail to
sec the difference. That is, if we replaced Z, AJ, and N by the symbols a,
b, aud e, respectively, we would test whether {a} 9 {8} 9 {c} = {a} 9 {bd}.
Since both sides are the empty set, the cquality of languages holds and
the Lest. would imply that the “law” is true.

d) R(S+T) = RS + RT.

e)
*f

(R4 S\T = RT + ST.
) (Rt) = Re.

g) (e+ R= Re.

h) (R°S*)* =(R+S)".

! Exercise 3.4.2: Prove or disprove cach of the following statements about
regular expressions.

*a) (4S) = B+ 8".
b) (AS+ RY R= RSR+ R)*.
*c) (RS + RRS = (RRS)*.
d) (R+S)*S = (R°S)*.
a} SIRS +S)*R= RRS(RR*S)".
Exercise 3.4.3: In Example 3.6. we developed the regular expression
(0+ 1)°1(0+ 1) + (0 +1)°1(0 + 1)(0+4 1)

Use the distributive laws to develop two different, simpler, equivalent expres-
sions.


--- Page 138 ---
122 CHAPTER 3. REGULAR EXPRESSIONS AND LANGUAGES

Exercise 3.4.4: At the beginning of Section 3.4.6, we gave part of a proof that
(L*M*)* = (L+M)*. Complete the proof by showing that strings in (L*M*)*
are also in (Z + A)".

1 Exercise 3.4.5: Complete the proof of Theorem 3.13 by handling the cases
where regular expression E is of the form FG or of the form F”*.

3.5 Summary of Chapter 3

+ Regular Expressions: This algebraic notation describes exactly the same
languages as finite automata: the regular languages. The regular-ex-
pression operators are union, concatenation (or “dot”), and closure (or
“star” ).

+ Regular Expressions in Practice: Systems such as UNIX and various of
its commands use an extended regular-expression language that provides
shorthands for many common expressions. Character classes allow the
easy expression of sets of symbols, while operators such as one-or-more-of
and at-most-onc-of augment the usual regular-expression operators.

+ Equivalence of Regular Expressions and Finite Automata: We can con-
vert a DFA to a regular expression by an inductive construction in which
expressions for the labels of paths allowed to pass through increasingly
larger sets of states are constructed. Alternatively, we can use a state-
elimination procedure to build the regular expression for a DFA. In the
other direction, we can construct recursively an e-NFA from regular ex-
pressions, and then convert the e-NFA to a DFA, if we wish.

+ The Algebra of Regular Expressions: Regular expressions obey many of
the algebraic laws of arithmetic, although there are differences. Union
and concatenation are associative, but only union is commutative. Con-
catenation distributes over union. Union is idempotent.

+ Testing Algebraic Identities: We can tell whether a regular-expression
equivalence involving variables as arguments is true by replacing the vari-
ables by distinct constants and testing whether the resulting languages
are the same.

3.6 References for Chapter 3

The idea of regular expressions and the proof of their equivalence to finite
automata is the work of S. C. Kleene [3]. However, the construction of an e-
NFA from a regular expression, as presented here, is the “McNaughton- Yamada
construction,” from [4]. The test for regular-expression identities by treating
variables as constants was written down by J. Gischer [2]. Although thought to


--- Page 139 ---
3.6. REFERENCES FOR CHAPTER 3 123

be folklore, this report demonstrated how adding several other operations such
as intersection or shuffle (See Exercise 7.3.4} makes the test fail, even though
they do not extend the class of languages representable.

Even before developing UNIX, K. Thompson was investigating the use of
regular expressions in commands such as grep, and his algorithm for processing
such commands appears in [5]. The early development of UNIX produced sev-
eral other commands that make heavy use of the extended regular-expression
notation, such as M. Lesk’s lex command. A description of this command and
other regular-expression techniques can be found in [1].

1. A.V. Aho, R. Sethi, and J.D. Ullman, Compilers: Principles, Techniques,
and Tools, Addison-Wesley, Reading MA, 1986.

2. J. L. Gischer, STAN-CS-TR-84-1033 (1984).

3. 8. C. Kleene, “Representation of events in nerve nets and finite automata,”
In C. E. Shannon and J. McCarthy, Automata Studies, Princeton Univ.
Press, 1956, pp. 3-42.

4. R. McNaughton and H. Yamada, “Regular expressions and state graphs
for automata,” IEEE Trans. Electronic Computers 9:1 (Jan., 1960), pp.
39-47.

5. K. Thompson, “Regular expression search algorithm,” Comm. ACM 11:6
(June, 1968), pp. 419-422.


--- Page 140 ---


--- Page 141 ---
Chapter 4

Properties of Regular
Languages

The chapter explores the propertics of regular languages. Our first tool for
this exploration is a way to prove that certain languages are not regular. This
theorem, called the “pumping lemma,” is introduced in Section 4.1.

One important kind of fact about the regular languages is called a “closure
property.” These properties let us build recognizers for languages that are
coustructed from other languages by certain operations. As an example, the
intersection of two regular languages is also regular. Thus, given automata
that recognize two different regular languages, we can construct mechanically
an automaton that recognizes exactly the intersection of these two languages.
Since the automaton for the intersection may have many more states than either
of the two given automata, this “closure property” can be a useful tool for
building complex automata. Section 2.1 used this construction in an essential
way.

Some other important facts about regular languages are called “decision
properties.” Our study of these properties gives us algorithms for answering
important questions about automata. A central example is an algorithm for
deciding whether two automata define the same Janguage. A consequence of
our ability to decide this question is that we can “minimize” automata, that
is, find an equivalent to a given automaton that has as few states as possible.
This problem has been important in the design of switching circuits for decades,
since the cost of the circuit (area of a chip that the circuit occupies) tends to
decrease as the nuruber of states of the automaton implemented by the circuit
decreases.


--- Page 142 ---
126 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

4.1 Proving Languages not to be Regular

We have established that the class of languages known as the regular languages
has at least four different descriptions. They are the languages accepted by
DFA’s, by NFA’s, and by e-NFA’s; they are also the languages defined by regular
expressions.

Not every language is a regular language. In this section, we shall introduce
a powerful technique, known as the “pumping lemma,” for showing certain
languages not to be regular. We then give several examples of nonregular
languages. In Section 4.2 we shall sec how the pumping lemma can be used in
tandem with closure propertics of the regular languages to prove other languages
not to be regular.

4.1.1 The Pumping Lemma for Regular Languages

Let us consider the langnage Lo, = {0°1” | n > 1}. This language contains
all strings 01, 0011, 000111, and so on, that consist of one cr more 0’s followed
by an equal number of 1's. We claim that Zp; is not a regular language. The
intuitive argument is that if Ly) were regular, then Lo) would be the language
of some DFA A. This automaton has some particular number of states, say k
states. Imagine this automaton receiving & 0's as input. It is in some state after
consuming each of the k +1 prefixes of the input: ¢,0,00,...,0*. Since there
are only & different states, the pigeanhole principle tells us that after reading
two different prefixes, say 0’ and 0’, A must be in the same state, say state g.

However, suppose instead that after reading i or j 0's, the automaton A
starts receiving 1’s as input. After receiving 7 1’s, it must accept if it previously
received 7 0’s, but not if it received 7 0’s. Since it was in state g when the 1's
started, if, cannot “remember” whether it received ¢ or 7 0’s, so we can “fool”
A and make it do the wrong thing -- accept if it should not, or fail to accept
when it should.

The above argument is informal, but can be made precise. However, the
same conclusion, that the language £y, is not regular, can be reached using a
general result, as follows.

Theorem 4.1: (The purnping lemma for regular languages) Let £ be a regular
language. Then there exists a constant n (which depends on £) such that for
every string w in £ such that || > m, we can break w into three strings,
w = xyz, such that:

Ll. ye.
2. |ay| <n.
3. For all & > 0, the string xy*z is also in L.

That is, we can always find a nonempty string y not too far from the beginning
of w that can be “pumped”; that is, repeating y any number of times, or deleting
it (the case k = 0), keeps the resulting string in the language F.


--- Page 143 ---
4.1. PROVING LANGUAGES NOT TO BE REGULAR 127

PROOF: Suppose £ is regular. Then L = £(A) for some DFA 4. Suppose -1 has
n states. Now, consider any string w of length a or more, say w= @)M2+--dm,
where m > n and cach @; is an input symbol. Por z= Q.1....,7 define state
p; to be (qo, a1a2---a;), where 6 is the transition function of 4, and go is the
start state of 4. That is, », is the state A is in after reading the first ¢ symbols
of w. Note that po = qi.

By the pigeonhole principle, it is nol possible for the n + 1 diferent p;’s for
i= 0,1....,2 to be distinct, since there are only n. different states. Thus, we
can find two different integers ¢ and j, with O< i <7 <n, such that p; = pj.
Now, we can break w = «yz as follows:

Ll. 2 = @,qy-++ ay.
2. Y= Qj-14s42 > ++ a5.
3. 2 = je Appa dy.

That is, # takes us to p; once; y takes us from p; back to p; (since p, is also pj),
and z is the balance of w. The relationships among the strings and states arc
suggested by Fig. 4.1. Note that 2 may be empty, in the case that ¢ = 0. Also,
z may be empty if j =n =m. However, y can not be empty, since i is strictly
less than j.

Figure 4.1: Every string longer than the number of stales must cause a state
to repeal

Now, consider what happens if the automaton .4 receives the input ay*z for
any k > 0. If k= 0, then the automaton goes from the start. state gg (which is
also yo) to p; on input x. Since p; is also p;, it must. be that A goes from p; to
the accepting state shown in Fig. 4.1 on input z. Thus, -1 accepts rz.

lf & > 0, then .4 goes from go to p; on input x, cireles from p; to p; & times
on input y*, and then gocs to the accepting state on input z. Thus, for any
k > 0, cy*z is alsa accepted by A; that is, ry*zisin gd. O

4.1.2 Applications of the Pumping Lemma

Let us ste some examples of how the pumping lemma is used. ln each case.
we shall propose a language and use the pumping lemma to prove that the
language is not regular.


--- Page 144 ---
128 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

The Pumping Lemma as an Adversarial Game

Recall our discussion from Section 1.2.3 where we pointed out that a theo-
rem whose statement involves several alternations of “for-all” and “there-
exists” quantifiers can be thought of as a game between two players. The
pumping lemma is an important example of this type of theorem, since it
in effect involves four different quantifiers: “for all regular languages L
there exists m such that for all w in L with |w| > n there exists xyz
equal to w such that --- .” We can see the application of the pumpmg
lemma, as a game, in which:

. Player 1 picks the language £ to be proved nonregular.

. Player 2 picks n, but doesn’t reveal to player 1 what 7 is; player 1
must devise a play for all possible n's.

. Player 1 picks w, which may depend on n and which must be of
length at least n.

. Player 2 divides w into x, y, and z, obeying the constraints that
are stipulated in the pumping lemma; y # € and [zy| < n. Again,
player 2 docs not have to tell player 1 what z, y, and z are, although
they must obey the constraints.

. Player 1 “wins” by picking &, which may be a function of n, z, y.
and z, such that zy*z is not in L.

Example 4.2: Let us show that the language Ly, consisting of all strings with
an equal number of 0’s and 1’s {not in any particular order) is not a regular
language. In terms of the “two-player game” described in the box on “The
Pumping Lemma as an Adversarial Game,” we shall be player 1 and we must
deal with whatever choices player 2 makes. Suppose 7 is the constant that must
oxist if Lg is regular, according to the pumping lemma; i.e., “player 2” picks
n. We shall pick w = 0"1", that is, 2 0’s followed by n 1’s, a string that surely
is In Log.

Now, “player 2” breaks our w up into syz. All we know is that y 4 ¢, and
|jry| <n. However, that information is very useful, and we “win” as follows.
Since |xy| <1, and xy comes at the front of w, we know that x and y consist
only of 0’s. The pumping lemma tells us that wz is in Ley, if Leg is regular.
This conclusion is the case k = 0 in the pumping lemma.! However, xz has nr
1’s, since all the 1’s of w are in 2. But xz also has fewer than nm 0’s, because we

‘Observe in what follows that we could have also succeeded by picking & = 2, or indeed
any value of & other than 1-


--- Page 145 ---
4.1. PROVING LANGUAGES NOT TO BE REGULAR 129

lost the 0’s of y. Since y # ¢ we know that there can be no more than n — 1 0's
among = and z. Thus, after assuming Leg is a regular language, we have proved
a fact. known to be false, that xz is in Lg. We have a proof by contradiction
of the fact that Leg is not regular. O

Example 4.3: Let us show that the language L,, consisting of all strings of
1’s whose length is a prime is not a regular language. Suppose it were. Then
there would be a constant » satisfying the conditions of the pumping lemma.
Consider some prime p > n + 2; there must be such a p, since there are an
infinity of primes. Let w = 1?.

By the pumping lemma, we can break w = xyz such that y # € and |zy| < rn.
Let |y| =m. Then [xz] = p—m. Now consider the string zy?" ™z, which must
be in Ly, by the pumping Jemma, if L,, really is regular. However,

jay?" 2| = |zz| + (p-—m)|y| = p—m+t(p-m)m = (m + 1)(p—m)

Tt looks like jzy?~™z| is not a prime, since it has two factors m+ 1 and
p-m. However, we must check that neither of these factors are 1, since then
(m+ 1}{p — m) might be a prime after all. But m+ 1 > 1, since y # € tells us
m > 1. Also, p—m > 1, since p > n + 2 was chosen, and m <n since

m= |y| < |xy| Sn

Thus, p-—m > 2.

Again we have started by assuming the language in question was regular,
and we derived a contradiction by showing that some string not in the language
was required by the pumping Jemma to be in the language. Thus, we conclude
that Z,, is not a regular language. O

4.1.3. Exercises for Section 4.1

Exercise 4.1.1: Prove that the following are not regular languages.

a) {071" | n > 1}. This language, consisting of a string of 0’s followed by an
equal-length string of 1’s, is the language Zo, we considered informally at
the beginning of the section. Here, you should apply the pumping lemma
in the proof.

b) The set of strings of balanced parentheses. These are the strings of char-
acters “(” and *)” that can appear in a well-formed arithmetic expression.

*c) {0"10" | n> 1}.
d) {07 172" | m and m are arbitrary integers}.
e} {0°17 |[n<m}.

f) {OP 12" | n> 1}.


--- Page 146 ---
130 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

! Exercise 4.1.2: Prove that the following are not regular languages.
*a) {0" | n is a perfect square}.
b) {0” | 2 is a perfect cube}.
c) {0" | 2 is a power of 2}.
d) The set of strings of 0’s and 1's whose length is a perfect square.

e) The set of strings of 0’s and 1’s that are of the form ww, that is, some
string repeated.

f) The set of strings of 0’s and 1's that are of the form ww”, that is, some
string followed by its reverse. (See Section 4.2.2 for a formal definition of
the reversal of a string.)

g) The set of strings of 0's and 1’s of the form wi, where @ is formed from
w by replacing all 0’s by 1’s, and vice-versa; e.g., O11 = 100, and 011100

is an example of a string in the language.

h) The set of strings of the form wi”, where w is a string of 0’s and 1’s of
length n.

! Exercise 4.1.3: Prove that. the following are not regular languages.

a} The set of strings of 0's and 1’s, beginning with a 1, such that when
interpreted as an integer, that integer is a prime.

b) The set of strings of the form 0°17 such that the greatest common divisor
of ¢ and 7 is 1.

' Exercise 4.1.4: When we try to apply the pumping lemma to a regular lan-
guage, the “adversary wins,” and we cannot complete the proof. Show what
goes wrong when we choose £ to be one of the following languages:

* a) The empty set.
*b) {00,11}.

*¢) (00+ 11)".

d) 01°0°2.


--- Page 147 ---
4.2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 131

4.2 Closure Properties of Regular Languages

In this section, we shall prove several theorems of the form “if certain languages
are regular, and a language L is formed from them by certain operations (e.g., L
is the union of two regular languages), then L is also regular.” These theorems
are often called closure properties of the regular languages, since they show that
the class of regular languages is closed under the operation mentioned. Closure
properties express the idea that when one (or several) languages are regular,
then certain related langnages arc also regular. They also serve as an imterest-
ing illustration of how the equivalent. representations of the regular languages
(automata and regular expressions} reinforce each other in our understanding
of the class of languages, since often one representation is far better than the
others in supporting a proof of a closure property. Here is a summary of the
principal closure properties for regular languages:

1. The union of two regular languages is regular.
. The intersection of two regular languages is regular.

. The complement of a regular language is regular.

me Cf bh

. The difference of two regular languages is regular.
. The reversal of a regular language is regular.
. The closure (star) of a regular language is regular.

. The concatenation of regular languages is regular.

oo - oo oo

_ A homomorphism (substitution of strings for symbols) of a regular lan-
guage is regular.

9. The inverse homomorphism of a regular Janguage is regular.

4.2.1 Closure of Regular Languages Under Boolean
Operations

Our first closure properties are the three boolean operations: union, intersec-
tion. and complementation:

1. Let L and M be languages over alphabet L. Then LU Af is the language
that contains all strings that are in either or both of LZ and M.

2. Let L and Af be languages over alphabet E. Then £M Af is the language
that contains all strings that are in both £ and Af.

3. Let £ be a language over alphabet ©. Then L, the complement of L, is
the set of strings in ©* that are not in LZ.

It turns out that the regular languages are closed under all three of the
boolean operations. The proofs take rather different approaches though, as we
shall see.


--- Page 148 ---
132 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

What if Languages Have Different Alphabets?

When we take the union or intersection of two languages L and MW, they
might have different alphabets. For example, it is possible that £, € {a, 6}
while Lo C {b.c.d}. However, if a language J consists of strings with
syiubols in ©. then we can also think of Z as a language over any finite
alphabet that is a superset of ©. Thus, for example. we can think of both
L, and Lz above as being languages over alphabet {a,b,c,d}. The fact
that none of L,’s strings contain symbols ¢ or d is irrelevant, as is the fact
that Lo's strings will not contain a.

Likewise, when taking the complement of a language £ that is a subset
of ©} for some alphabet £), we may choose to take the complement with
respect to some alphabet Y2 that is a superset of £,. If so, then the
complement of £ will be ©} — £; that. is, the complement of E with respect
ta Sy includes (among other strings) all those strings in £3 that have at
least one symbol that isin Sy but not in 2). Had we taken the complement
of L with respect to E,, then no string with symbols in 22-21 would be in
L. Thus, to be strict, we should always state the alphabet with respect to
which a complement. is taken. However, often it is obvious which alphabet
is meant; ¢.g., if L is defined by an automaton, then the specification of
thal automaton includes the alphabet. Thus, we shall often speak of the
“complement” without specifying the alphabet.

Closure Under Union
Theorem 4.4: If Z and M are regular languages, thon so is LU M.

PROOF: This proof is simple. Since & and M are regular, they have regular
expressions; say L = L(R) and M = £(S). Then £U Af = L(+ S) by the
definition of the + operator for regular expressions. 0

Closure Under Complementation

The theorem for union was made very easy by the use of the regular-expression
representation for the languages. However, let us next consider complemen-
tation. Do you see how to take a regular expression and change it into one
that defines the complement language? Well neither do we, However, it can he
done. because as we shall sec in Theorem 4.5, it is easy to start with a DFA and
construct a DFA that accepts the complement. Thus, starting with a regular
expression, we could find a regular expression for its complement as follows:

1. Convert the regular expression to an «-NFA.

9. Convert that «-NFA to a DFA by the subset construction.


--- Page 149 ---
4.2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 133

Closure Under Regular Operations

The proof that regular languages are closed under union was exceptionally
easy because union is one of the three operations that define the regular
expressions. The same idea as Theorem 4.4 applies to concatenation and
closure as well. That is:

e If Land Af are regular languages, then so is LAL.

e If £ is a regular language, then so is D*.

3. Complement the accepting states of that DFA.

4. Turn the complement DFA back into a regular expression using the con-
struction of Sections 3.2.1 or 3.2.2.

Theorem 4.5: If £ is a regular language over alphabet D, then L = U* — L is
also a regular language.

PROOF: Let £ = L(A) for some DFA A = (Q,¥,4,¢0,F). Then £ = £(B),
where B is the DFA (Q.5.6,g0.Q — F). That is, B is exactly like A, but the
accepting states of A have become nonaccepting states of B, and vice versa.
Then w is in L(B) if and only if 6(go, w) is in Q — F, which occurs if and only
if wis notin 204). OD

Notice that it is important for the above proof that (go, w) is always some
state; ie., there are no missing transitions in A. If there were, then certain
strings might lead neither to an accepting nor nonaccepting state of A, and
those strings would be missing from both £(A) and L(B). Fortunately, we
have defined a DFA to have a transition on every symbol of © from every state,
so each string leads either to a state in F or a state in @ — F.

Example 4.6: Let 4 be the automaton of Fig. 2.14. Recall that DFA A ac-
cepts all and only the strings of 0’s and 1's that end in 01; in reguiar-expression
terms, L(A} = (0+ 1)*01. The complement of L(A) is therefore all strings
of 0’s and 1’s that do not end in O1. Figure 4.2 shows the automaton for
{0,1}* — L(A). It is the same as Fig. 2.14 but with the accepting state made
nonaccepting and the two nonaccepting states made accepting. O

Example 4.7: In this cxample, we shall apply Theorem 4.5 to show a certain
language not to be regular. In Example 4.2 we showed that the language Ly
consisting of strings with an equal number of U's and 1’s and is not regular. This
proof was a straightforward application of the pumping lemma. Now consider


--- Page 150 ---
134 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

Figure 4.2: DFA accepting the complement of the language (0 + 1)*01

the language JZ consisting of those strings of 0’s and 1’s that have an unequal
number of 0’s and 1’s.

it would be hard to use the pumping lemma to show M is not regular.
Intuitively, if we start with some string w in Mf, break it into w = xyz, and
“pump” y, we might find that y itself was a string like 01 that had an equal
number of 0's and 1’s. If so, then for no & will sy*z have an equal number of 0’s
and 1’s, since zyz has an unequal number of 0’s and 1's, and the numbers of 0’s
and 1’s change equally as we “pump” y. Thus, we can never use the pumping
lemma to contradict the assumption that Mf is regular.

However, M is still not regular. The reason is that Mf = E. Since the
complement of the complement is the set we started with, it also follows that
L= M. If Af is regular, then by Theorem 4.5, L is regular. But we know F is
not regular, so we have a proof by contradiction that Mf is not regular. O

Closure Under Intersection

Now, jet us consider the intersection of two regular languages. We actually
have little to do, since the three boolean operations are not independent. Once
we have ways of performing complementation and union, we can obtain the
intersection of languages L and Af by the identity

LOM=LEUM (4.1)

Tn general, the intersection of two scts is the set of elements that are not in
the complement of either set. That observation, which is what Eqnation (4.1)
says, 1s one of DeMorgan’s laws. The other law is the same with union and
intersection interchanged; that is, LUM =EnM.

However, we can also perform a direct construction of a DFA for the in-
tersection of two regular languages. This construction, which essentially runs
two DFA's in parallel, is useful in its own right. For instance, we used it to
construct the automaton in Fig. 2.3 that represented the “product” of what
two participants — the bank and the store — were doing. We shall make the
product construction formal in the next theorern.


--- Page 151 ---
4.2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 135

Theorem 4.8: If L and M are regular languages, then so is L 1 M.

PROOF: Let I and M be the languages of automata Az = (Q1,¥,41,¢2; FL)
and Ay = (Qar,U,6ar,¢u, Fag). Notice that we are assuming that the alpha-
bets of both automata are the same; that is, © is the union of the alphabets
of L and M, if those alphabets are different. The product construction actu-
ally works for NFA’s as well as DFA’s, but to make the argument as simple as
possible, we assume that Az, and Ag, are DFA’s.

For LM M we shall construct an automaton A that simulates both A; and
Am. The states of A are pairs of states, the first from Az and the second from
Am. To design the transitions of A, suppose A is in state (p,q), where p is the
state of Az and q is the state of Ajy. If a is the input symbol, we see what A,
does on input a; say it goes to state s. We also see what Ayy does on input
a; say it makes a transition to state ¢. Then the next state of A will be (s, ¢).
In that manner, A has simulated the effect of both A, and Ay. The idea is
sketched in Fig. 4.3.

Input a

Start Accept

Figure 4.3: An automaton simulating two other automata and accepting if and
only if both accept

The remaining details are simple. The start state of A is the pair of start
states of Ay and Ags. Since we want to accept if and only if both automata
accept, we select as the accepting states of A all those pairs (p,q) such that p
is an accepting state of A, and q is an accepting state of Aay. Formally, we
define:

A= (Qr x Qar, &, 6, (gz; ar), FL x Frys)


--- Page 152 ---
136 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

To see why L(A) = E(AL) L(Ans), first observe that an easy induction
on |w| proves that 6((qz,¢),#) = (67.(ar.,w), du (qu, w)). But A accepts w if
and only if d((ar, ar); w) is a pair of accepting states. That is, by (gz, 2%) must
be in Fy, and dy; (qar,w) must be in Fy,. Put another way, w is accepted by A

if and only if both Az and Ags accept w. Thus, A accepts the intersection of
Land Af. O

Example 4.9: In Fig. 4.4 we see two DFA’s. The automaton in Fig. 4.4{a}
accepts all those strings that have a 0, while the automaton in Fig. 4.4(b)
accepts all those strings that have a 1. We show in Fig. 4.4(c} the product of
these two automata. Its states are labeled by the pairs of states of the automata
in (a) and {b}.

1
Start" io 0 @)) 0,1
(a)
0
sunt l a
(b)
Ounme
O
(c)

Figure 4.4: The product construction

It is casy to argue that this automaton accepts the intersection of the first
two languages: those strings that have both a 0 and a 1. State pr represents
only the initial condition, in which we have seen neither 0 nor 1. State gr means
that we have seen only 0’s, while state ps represents the condition that we have


--- Page 153 ---
4.2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 137

seen only 1’s. The accepting state gs represents the condition where we have
seen both 0's and l’s. O

Closure Under Difference

There is a fourth operation that is often applied to scts and is related to the
boolean operations: set difference. In terms of languages, L — M, the difference
of L and AM, is the set of strings that are in language £ but not in language
M. The regular languages are also closed under this operation, and the proof
follows easily from the theorems just proven.

Theorem 4.10; If Z and M are regular languages, then so is E ~ M.

PROOF: Observe that L— M = LM. By Theorem 4.5, M is regular, and
by Theorem 4.8 £9 M is regular. Therefore £ — M is regular. O

4.2.2 Reversal

The reversal of a string a)a2--:a@, is the string written backwards, that is,
Qnty —1-*-a,. We use w® for the reversal of string w. Thus, 0010” is 0100, and
e%& — ¢,

The reversal of a language L, written L*, is the language consisting of the
reversals of all its strings. For instance, if L = {001,10,111}, then L* =
{100, 01, 141}.

Reversal is another operation that preserves regular languages; that is, if
L is a regular language, so is L*. There are two simple proofs, one based on
automata and one based on regular expressions. We shall give the automaton-
based proof informally, and let you fill in the details if you like. We then prove
the theorem formally using regular expressions.

Given a language I that is L(A) for some finite automaton, perhaps with
nondeterminism and e-transitions, we may construet an automaton for L® py:

1. Reverse all the arcs in the transition diagram for A.

2. Make the start state of A be the only accepting state for the new automa-
ton.

3. Create a new start state pg with transitions on € to all the accepting states
of A.

The result is an automaton that simulates A “in reverse.” and therefore accepts
a string w if and only if 4 accepts w*. Now, we prove the reversal theorem

formally.

Theorem 4.11: If Z is a regular language, so is L*.


--- Page 154 ---
138 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

PROOF: Assume L is defined by regular expression E. The proof is a structural
induction on the size of E. We show that there is another regular expression
E* such that L(E*) = (L(E))*; that is, the language of E* is the reversal of
the language of F. )

BASIS: If E is e, 6, or a, for some symbol a, then EB” is the same as E. That
is, we know {c}” = {ce}, 0% = 9, and {a}* = {a}.

INDUCTION: There are three cases, depending on the form of E.

1. B= FE, +E. Then B® = EF + EF. The justification is that the reversal
of the union of two languages is obtained by computing the reversals of
the two languages and taking the union of those languages.

2. E = E,E,. Then B® — ERER. Note that we reverse the order of
the two languages, as well as reversing the languages themselves. For
instance, if L(E,) = {01,111} and L(E2) = {00,10}, then L(B,E)) =
{0100, 0110, 11100, 11110}. The reversal of the latter language is

{0010, 0110, 00111, 01111}
If we concatenate the reversals of L(H2) and L(#,) in that order, we get

{00,01}{10, 111} = {0010, 00111, 0110, 01111}

which is the same language as (L(Ey F)}*. In general, if a word w in
E(E) is the concatenation of w, from L£(F,) and w2 from E(£), then
wt = wRy®
2 Wy.
3. E = EY. Then B® = (EF)*. The justification is that any string w in
L(E) can be written as wi we ---w,, where each w; is in E(£). But

we = wy k | .. wi
Each wf is in L(E), so w* is in L((EP)*). Conversely, any string in
L((BP)*} is of the form w,we---wr, where each w, is the reversal of a
string in L(E,). The reversal of this string, w*w?_, --.w, is therefore
a string in L(Ef), which is L(E£). We have thus shown that a string is in
L{) if and only if its reversal is in L((£)*).

o

Example 4.12: Let L be defined by the regular expression (0 + 1)0*. Then
LF is the language of (0*)*(0+1)*, by the rule for concatenation. If we apply
the rules for closure and union to the two parts, and then apply the basis rule
that says the reversals of 0 and 1 are unchanged, we find that L* has regular
expression 0*(0+1). CO


--- Page 155 ---
4.2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 139

4.2.3. Homomorphisms

A string homomorphism is a function on strings that works by substituting a
particular string for each symbol.

Example 4.13: The function / defined by A(0) = ab and A(1) = is a homo-
morphism. Given any string of 0’s and 1's, it replaces all 0’s by the string ab
and replaces all 1’s by the empty string. For example, 4 applied to the string
0011 is aia. O

Formally, if A is a homomorphism on alphabet £, and w = aja2--- ay
is a string of symbols in ©, then h{w) = h(a,)A(ae)---h{aa). That is, we
apply A to each symbol of w and concatenate the results, in order. For in-
stance, if A is the homomorphism in Example 4.13, and w = 0011, then
A(w) = ACO)ACO)A(1)A(1) = (ab)(ad)(e)(e) = abab, as we claimed in that ex-
ample.

Further, we can apply a homomorphism to a language by applying it to
each of the strings in the language. That is, if Z is a language over alphabet
y, and h is a homomorphism on E, then A(L) = {h(w) | w is in LE}. For
instance, if L is the language of regular expression 10*1, i.c., any number of
0’s surrounded by single 1’s, then A(Z)} is the language (ab)". The reason is
that A of Example 4.13 effectively drops the 1's, since they are replaced by ¢,
and turns each 0 into ab. The same idea, applying the homomorphism directly
to the regular expression, can be used to prove that the regular languages are
closed under homomorphisms.

Theorem 4.14: If Z is a regular language over alphabet ©, and h is a homo-
morphism on 5S, then A(£) is also regular.

PROOF: Let L = L(R) for some regular expression R. In general, if E is a
regular expression with symbols in E, let A{£) be the expression we obtain by
replacing each symbol a of 5 in E by h(a). We claim that h(R) defines the
language A(Z).

The proof is an easy structural induction that says whenever we take a
subexpression E of R and apply hf to it to get A(E), the language of h(E)
is the same language we get if we apply A to the language L(£). Formally,
L(h{B)) = (LCE).

BASIS: If E is € or 4, then A({E) is the same as FE, since A docs uot affect the
string ¢ or the language §. Thus, L(h(£)) = L(&). However, if £ is @ or ¢, then
L(E) contains either no strings or a string with no symbols, respectively. Thus
h(L(E)) = LB) im either case. We conclude L(A(E)) = L(E) = A(L{B)).

The only other basis case is if E = a for some symbol a in X. In this case,
L(E) = {a}, so h(L(E)) = {h(a)}. Also, k(E) is the regular expression that
is the string of symbols h(a). Thus, L({h(B)) is also {A(a)}, and we conclude
L(h(E)) = h(L()).


--- Page 156 ---
140 CHAPTER 4, PROPERTIES OF REGULAR LANGUAGES

INDUCTION: There are three cases, each of them simple. We shall prove only
the union case, where & = F'+G. The way we apply homomorphisms to regular
expressions assures us that A(E) = A(F + G) = ACF) + h(G). We also know
that £(2) = L(F) U L(G) and

L(A(E)) = L(ACF) + R(G)) = L(A(F)) UL(A(G)) (4.2)

by the definition of what “4+” means in regular expressions. Finally,

h(L(E)) = h(L(F) U L(G)) = A(L(F)) U h(L(G)) (4.3)

because A is applied to a language by application to each of its strings individ-
ually. Now we may invoke the inductive hypothesis to assert that L(A(F =
A(L(F)) and L{k(G)) = A(L(G)). Thus, the final expressions in (4.2) and
(4.3) are equivalent, and therefore so are their respective first terms; that is,
L(h(B)) = R(E(E)).

We shall not prove the cases where expression # is a concatenation or clo-
sure: the ideas are similar to the above in both cases. The conclusion is that
L(A(R)) is indeed h(L(R)); ie., applying the homomorphism h to the regu-
lar expression for language £ results in a regular expression that defines the
language A(L). O

4.2.4 Inverse Homomorphisms

Homomorphisms may also be applicd “backwards,” and in this mode they also
preserve regular languages. That is, suppose A is a homomorphism from some
alphabet 5 to strings in another (possibly the same) alphabet T.? Let L be
a language over alphabet T. Then #71(£), read “A inverse of ZL,” is the sct
of strings w in £* such that A{w) is in L. Figure 4.5 suggests the effect of
a homomorphism on a language L in part (a), and the effect of an inverse
homomorphism in part (b).

Example 4.15: Let £ be the language of regular expression (00 + 1)*. That
is, £ consists of all strings of 0’s and 1’s such that all the Q’s occur in adjacent
pairs. Thus, 0010011 and 10000111 are in £, but 000 and 10100 are not.

Let # be the homomorphism defined by A{a) = 01 and h(b) = 10. We claim

that h7'(Z) is the language of regular expression (ba)*, that is, all strings of
repeating ba pairs. We shall prove that A(w) is in ZL if and only if w is of the
form baba --- ba.
{If} Suppose w is n repetitions of ba for some n > 0. Note that A(ba) = 1001,
so A(w) is n repetitions of 1001. Since 1001 is composed of two 1’s and a pair of
Q’s, we know that 1001 is in £. Therefore any repetition of 1001 is also formed
from 1 and 00 segments and is in Z. Thus, A(w) is in LZ.

2That “T” should be thought of as a Greek capital tau, the letter following sigma.


--- Page 157 ---
4.2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 141

(a)

Ete

(b)

Figure 4.5: A homomorphism applied in the forward and inverse direction

(Only-if) Now, we must assume that h(w) is in LE and show that w is of the
form baba---ba. There are four conditions under which a string is not of that
form, and we shall show that if any of them hold then A{w) is not in L. That
is, we prove the contrapositive of the statement we set out to prove.

1. If w begins with a, then A(w) begins with 01. It therefore has an isolated
0, and is not in L.

2. If w ends in 4, then A(w) ends in 10, and again there is an isolated 0 in
A(w).

3. If w has two consecutive a’s, then h(w) has a substring 0101. Here too,
there is an isolated 0 in w.

4. Likewise, if w has two consecutive 6°s, then A(w) has substring 1010 and
has an isolated 0.

Thus, whenever one of the above cases hold, A(u’) is not in £. However, unless
at least one of items (1) through (4) hold, then w is of the form baba ---ba.


--- Page 158 ---
142 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

To see why, assume none of (1) through (4) held. Then (1) tells us w must
begin with 6, and (2) tells us w ends with a. Statements (3) and (4) tell us that
a’s and #’s must alternate in w. Thus, the logical “OR” of (1) through (4) is
equivalent to the statement “w is not of the form baba---ba.” We have proved
that the “OR” of (1) through (4} implies A(w) is not in £. That statement is
the contrapositive of the statement we wanted: “if A(w) is in L, then w is of
the form baba--:ba.” O

We shall next prove that the inverse homomorphism of a regular language
is also regular, and then show how the theorem can be used.

Theorem 4.16: If 4 is a homomorphism from alphabet 5 to alphabet T, and
L is a regular language over T, then 4~1(Z) is also a regular language.

PROOF: The proof starts with a DFA A for L. We construct from A and ha
DFA for 2-1(Z) using the plan suggested by Fig. 4.6. This DFA uses the states
of A but translates the input symbol according to A before deciding on the next
state.

Input a

|

Input
Start hfa} to A

—_—_
| Accept/reject
A

Figure 4.6: The DFA for 4~!(L) applies # to its input, and then simulates the
DFA for £

Formally, let Z be L(A), where DFA A = (Q,7, 6,49, F). Define a DFA

where transition function + is constructed by the rule +(g,a) = 8(q, h{a)). That.
is, the transition B makes on input a is the result of the sequence of transitions
that A makes on the string of symbols A(a). Remember that A{a) could be e,
it could be one symbol, or it could be many symbols, but 6 is properly defined
to take care of all these cases.


--- Page 159 ---
4.2, CLOSURE PROPERTIES OF REGULAR LANGUAGES 143

It is an easy induction on |w| to show that 4(go.w) = 4(go,h(w)). Since the
accepting states of A and B are the same, B accepts w if and only if A accepts
h(w). Put another way, B accepts exactly those strings w that are in h71(L).
CJ

Example 4.17: In this example we shall use inverse homomorphism and sev-
eral other closure properties of regular sets to prove an odd fact about finite
automata. Suppose we required that a DFA visit every state at least. once when
accepting its input. More precisely, suppose A = (Q, &,4,qu, F) is a DFA, and
we are interested in the language L£ of all strings w in L£* such that 8(40, w)
is in F, and also for every state g in Q there is some prefix z, of w such that
&(qo, tq) = q. Is L regular? We can show it is, but the construction is complex.

First, start with the language M that is L(A), i.e., the set of strings that
A accepts in the usual way, without regard to what states it visits during the
processing of its input. Note that ZL C M, since the definition of L puts an
additional condition on the strings of L(A). Our proof that £ is regular begins
by using an inverse homomorphism to, in effect, place the states of A into the
input symbols. More precisely, let us define a new alphabet 7 consisting of
symbols that we may think of as triples [pag], where:

1. pand g are states in Q,
2. ais a symbol in ©, and
3. d(p,a) = 4.

That is, we may think of the symbols in T as representing transitions of the
automaton A. It is important to see that the notation [pag] is our way of
expressing a single symbol, not the concatenation of three symbols. We could
have given it a single letter as a name, but then its relationship to p, g, and a
would be hard to describe.

Now, define the homomorphism h([pag]} = a for all p, a, and g. That is, A
removes the state components from each of the symbols of 7 and leaves only
the symbol from ©. Our first step in showing LZ is regular is to construct. the
language DZ) = A71(M). Since M is regular, so is L; by Theorem 4.16. The
strings of L, are just the strings of Mf with a pair of states, representing a
transition, attached to each symbol.

As a very simple illustration, consider the two-state automaton of Fig.
4.Afa). The alphabet EF is {0,1}, and the alphabet T consists of the four syim-
bols [pg]. [gq], [pip], and [g1g]. For instance, there is a transition from state
p to g on input 0, so [p0g] is one of the symbols of J. Since 101 is a string ac-
cepted by the automaton, 47! applied to this string will give us 24 = 8 strings,
of which [plp][p0q][q1q] and [¢lgq][q0q][plp] are two examples.

We shall now construct Z from L, by using a series of further operations
that preserve regular languages. Our first goal is to eliminate all those strings
of £, that deal incorrectly with states. That is, we can think of a symbol like


--- Page 160 ---
144 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

[pag] as saying the automaton was in state p, read input @, and thus entered
state q. The sequence of symbols must satisfy three conditions if it is to be
deemed an accepting computation of A:

1. The first state in the first. symbol must be go, the start state of A.

2. Each transition must pick up where the previous one left off. That is,
the first state in one symbol must equal the second state of the previous
symbol.

3. The second state of the last symbol must be in F. This condition in fact
will be guaranteed once we enforce (1) and (2), since we know that every
string in £y came from a string accepted by A.

The plan of the construction of Z is shown in Fig. 4.7.

The language of automaton A

Inverse homomorphism

Strings of M with state transitions embedded
Intersection with a regular language

Add condition that first state is the start state
Difference with a regular language

Add condition that adjacent states are equal
Difference with regular languages

Add condition that all states appear on the path
Homomorphism

Delete state components, leaving the symbols

Figure 4.7: Constructing language £ from language Af by applying operations
that preserve regularity of languages

We enforce (1) by intersecting £, with the set of strings that begm with a
symbol of the form [gag] for some symbol a and state g. That is, let £, be the


--- Page 161 ---
4,2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 145

expression [goei¢i] + [¢o@292] + ---, where the pairs aq; range over all pairs in
Sx Q such that d(g9,4@;) = q;. Then let Ly = L, 1 L(E\T*). Since BE, T* iz
a regular expression denoting all strings in T* that begin with the start state
(treat T in the regular expression as the sum of its symbols), Lz is all strings
that are formed by applying h7! to language M and that have the start state
as the first component of its frst symbol; i.c., it meets condition (1).

To enforce condition (2), it is easier to subtract from Lz (using the set-
difference operation) all those strings that violate it. Let Hz be the regular
expression consisting of the sum (union) of the concatenation of all pairs of
symbols that fail to match; that is, pairs of the form [pag|[rbs] where g # r.
Then T*£27* is a regular expression denoting all strings that fail to meet
condition (2).

We may now define £3 = Lo — L(T* E,T*). The strings of £3 satisfy condi-
tion (1) because strings in L, must begin with the start symbol. They satisfy
condition (2) because the subtraction of L(T*E)T*) removes any string that
violates that condition. Finally, they satisfy condition (3), that the last. state
is accepting, because we started with only strings in AZ, all of which lead to
acceptance by A. The effect is that Ly consists of the strings in M with the
states of the accepting computation of that string embedded as part of each
symbol. Note that Ly is regular because it is the result of starting with the
regular language A/, and applying operations — inverse homomorphism, inter-
section, and set difference — that yield regular sets when applied to regular
sets.

Recall that our goal was to accept only those strings in Af that visited
every state in their accepting computation. We may enforce this condition by
additional applications of the sct-difference operator. That is, for each state q,
let E, be the regular expression that is the sum of all the symbols in J such
that g appears in neither its first or last position. If we subtract L(E}) froin
Lz we have those strings that are an accepting computation of 4 and that visit
state g at least once. If we subtract from £3 all the languages L(£}) for g in
Q, then we have the accepting computations of A that visit all the states. Call
this language Ly. By Theorem 4.10 we know Ly is also regular.

Our final step is to construct ZL from £4 by getting rid of the state com-
ponents. That is, £ = A(£4). Now, LZ is the set of strings in &* that are
accepted by <1 and that visit each state of A at least once during their accep-
tance. Since regular languages are closed under homomorphisms, we conclude
that Zis regular. O

4.2.5 Exercises for Section 4.2

Exercise 4.2.1; Suppose ht is the homomorphism from the alphabet {0,1,2}
to the alphabet {a,b} defined by: 2(0) = a; A(1) = ab, and A(2) = ba.

* a) What is h(0120)?
b) What is R(21120)?


--- Page 162 ---
146 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

*c) If £ is the language £(01*2), what is A(L)?
a) If £ is the language L(0 + 12}, what is A(L)?

*e) Suppose L is the language {ababa}, that is, the language consisting of
only the one string baba. What is h~'(L)?

!f) If L is the language L(a(ba)*), what is A71(L)?

*! Exercise 4.2.2: If L is a language, and a is a symbol, then L/a, the quotient
of LE and a, is the set of strings w such that wa is in L. For example, if
L = {a,aab, baa}, then L/a = {e,ba}. Prove that if L is regular, so is L/a.
Hint: Start with a DFA for £ and consider the set of accepting states.

! Exercise 4.2.3: Lf Z is a language, and a is a symbol, then a\Z is the sct
of strings w such that aw is in £. For example, if L = {a,aab,baa}, then
a\L = {e,ab}. Prove that if Z is regular, so is a\L. Hint: Remember that the
regular languages are closed under reversal and under the quotient operation of
Exercise 4.2.2,

! Exercise 4.2.4: Which of the following identities are true?

a) (L/aja = L (the left side represents the concatenation of the languages

L/a and {a}).
b) afa\L) = ZL (again, concatenation with {a}, this time on the left. is
intended).
c) (La)/a= L.
d) @\(aL) = L.
Exercise 4.2.5: The operation of Exercise 4.2.3 is sometimes viewed as a “der-
ivative,” and a\Z is written ab These derivatives apply to regular expressions

in 4 manner similar to the way ordinary derivatives apply to arithmetic expres-
sions. Thus, if R is a regular expression, we shall use aR to mean the same as
ab; _

Ga if £ = DR).

‘ : d{R+5) _ @R , dS
a) Show that S7-* = 984 &.

*! b) Give the rule for the “derivative” of RS. Hint: You need to consider two
cases: if L(A) does or does not contain e. This rule is not quite the same
as the “product rule” for ordinary derivatives, but is similar.

'c) Give the rule for the “derivative” of a closure, i-e., se)

d) Use the rules from (a)- (c) to find the “derivatives” of regular expression
(0 + 1}*011 with respect to 0 and 1.

¥
z

‘ swhich @ —
Characterize those languages L for which $2 = @.


--- Page 163 ---
*!

4.2. CLOSURE PROPERTIES OF REGULAR LANGUAGES 147

: -hi dip __
*I f) Characterize those languages L for which 4 = Z£.

Exercise 4.2.6: Show that the regular languages are closed under the follow-
ing operations:

a) min(L) = {w | w is in L, but no proper prefix of w is in L}.
b) max(L) = {w | w is in Z and for no & other than ¢ is wa in L}.
c) init(L) = {w | for some z, we is in DL}.

Hint: Like Exercise 4.2.2, it is easiest to start with a DFA for Z and perform a
construction to get the desired language.

Exercise 4.2.7: If w = aiag---a@, and x = 6,b9---6,, are strings of the
same length, define alf(w,2) to be the string in which the symbols of w and z
alternate, starting with w, that is, a, b,a2b2---a,b,. If L and M are languages,
define alé(L, Mf) to be the set of strings of the form alf(w,xz), where w is any
string in £ and a is any string in AY of the same length. Prove that if Z and
M are regular, so is ali(Z,M).

Exercise 4.2.8: Let L be a language. Define halif(L) to be the set of first
halves of strings in L, that is, {w | for some # such that |x| = |w|, we have wa
in L}. For example, if L = {e,0010,011,010110} then Aaif(Z) = {e, 00,010}.
Notice that odd-length strings do not contribute to half(L). Prove that if L is
a regular language, so is haif(ZL).

Exercise 4.2.9: We can generalize Exercise 4.2.8 to a number of functions that
determine how much of the string we take. If f is a function of integers, define
f(L) to be {w | for some «, with |z| = f(|w]), we have wa in L}. For instance,
the operation half corresponds to f being the identity function f(m) = n, since
half(L) is defined by having |x| = [w|. Show that if Z is a regular language,
then so is f({Z), if f is one of the following functions:

a) f(n) = 2n {ie., take the first thirds of strings).

b) f(n) = n? (i.e., the amount we take has length equal to the square root
of what we do not take.

c) f(m) = 2" (ie, what we take has length equal to the logarithm of what
we leave).

Exercise 4.2.10: Suppose that ZL is any language, not necessarily regular,
whose alphabet is {6}; i-e., the strings of L consist of 0’s only. Prove that D* is
regular. Hint: At first, this theorem sounds preposterous. However, an example
will help you see why it is true. Consider the language L = {0* | i is prime},
which we know is not regular by Example 4.3. Strings 00 and 000 are in £,
since 2 and 3 are both primes. Thus, if 7 > 2, we can show 0? is in L*. If j is
even, use j/2 copies of 00, and if j is odd, use one copy of 000 and (j — 3)/2
copies of 00. Thus, £* = 000".


--- Page 164 ---
nN

148 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

Exercise 4.2.11: Show that the regular languages are closed under the fol-
lowing operation: cycle(L) = {w | we can write w as w = zy, such that yz is
in LE}. For example, if 2 = {01,011}, then cycle(Z) = (01, 10,011, 110, 101}.
Hint: Start with a DFA for £ and construct an e-NFA for eyele(Z).

1 Exercise 4.2.12: Let w, = an@q@,;, and w; = wy_jwj_1a; for all i > 1.

For instance, w3 = d9@9@)A9@9@1@2090901 49a9a14a2a3. The shortest regular
expression for the language Ly, = {w,}, i-c., the language consisting of the one
string wp, is the string w, itself, and the length of this expression is 2"+1 — 1.
However, if we allow the intersection operator, we cam write an expression for
EL, whose length is O(n"). Find such an expression. Hint: Find n languages,
each with regular expressions of length O(2), whose intersection is £,,.

Exercise 4.2.13: We can use closure properties to help prove certain lan-
guages are not regular. Start with the fact that the language

Lonin = {0"L” | ad 0}

is not a regular set. Prove the following languages not to be regular by trans-
forming them, using operations known to preserve regularity, to Lonin:

*a) {O'l |i ¢ J}.
b} {or ywQr—™ | n > m > O}.

Exercise 4.2.14: In Theorem 4.8, we described the “product construction”
that took two DFA’s and constructed one DFA whose language is the intersec-
tion of the languages of the first two.

Show how to perform the product construction on NFA’s (without e-
transitions}.

a

—

!b) Show how to perform the product construction on e-NFA’s.

* ©) Show how to modify the product construction so the resulting DFA ac-
cepts the difference of the languages of the two given DFA‘s.

d) Show how to modify the product construction so the resulting DFA ac-

cepts the union of the languages of the two given DFA’s.

—

Exercise 4.2.15: In the proof of Theorem 4.8 we claimed that it could be
proved by induction on the length of w that

8((ar.¢ar),w) = (6r(97, 0). 4ar(au.))
Give this inductive proof.

Exercise 4.2.16: Complete the proof of Theorem 4.14 by considering the cases
where expression £ is a concatenation of two subexpressions and where & is
the closure of an expression.

Exercise 4.2.17: In Theorem 4.16, we omitted a proof by induction on the
length of w that +(g9.w) = 6(go,2(w)). Prove this statement.


--- Page 165 ---
4.3. DECISION PROPERTIES OF REGULAR LANGUAGES 149

4.3 Decision Properties of Regular Languages

In this section we consider how one answers important questions about regular
languages. First, we must consider what it means to ask a question about a
language. The typical language is infinite, so you cannot present the strings of
the language to someone and ask a question that requires them to inspect the
infinite set of strings. Rather, we present a language by giving one of the finite
representations for it that we have developed: a DFA, an NFA, an e-NFA, or a
regular expression.

Of course the language so described will be regular, and in fact there is no
way at all to represent completely arbitrary languages. In later chapters we
shall see finite ways to represent more than the regular languages, so we can
consider questions about languages in these more general classes. However, for
many of the questions we ask, algorithms exist only for the class of regular
languages. The same questions become “undecidable” {no algorithm to answer
them exists) when posed using more “expressive” notations {ie., notations that
can be used to express a larger set. of languages) than the representations we
have developed for the regular languages.

We begin our study of algorithms for questions about regular languages by
reviewing the ways we can convert one representation into another for the same
language. In particular, we want to observe the time complexity of the algo-
rithms that perform the conversions. We then consider some of the fundamental
questions about languages:

1. Is the language described empty?
2. Is a particular string w in the described language?

3. Do two descriptions of a language actually describe the same language?
This question is often called “equivalence” of languages.

4.3.1 Converting Among Representations

We know that we can convert any of the four representations for regular lan-
guages to any of the other three representations. Figure 3.1 gave paths from
any representation to any of the others. While there are algorithms for any
of the conversions, sometimes we are interested not only in the possibility of
making a conversion, but in the amount of time it takes. In particular, it is
important to distinguish between algorithms that take exponential time (as a
function of the size of their input}, and therefore can be performed only for
relatively small instances, from those that take time that is a linear, quadratic,
or some small-degree polynomial of their input size. The latter algorithms are
“realistic,” in the sense that we expect them to be executable for large instances
of the problem. We shall consider the time complexity of each of the conversions
we discussed.


--- Page 166 ---
150 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

Converting NFA’s to DFA’s

When we start with either an NFA or and «NFA and convert it to a DFA, the
time can be exponential in the number of states of the NFA. First, computing
the e-clusure of n states takes O(n?) time. We must search from each of the n
states along all ares labeled ¢. If there are n states, there can be no more than
n® arcs. Judicious bookkeeping and well-designed data structures will make
sure that we can explore from each state in O(n”) time. In fact, a transitive
closure algorithm such as Warshall’s algorithm can be used to compute the
entire e-closure at once.®

Once the e-closure is computed, we can compute the equivalent DFA by the
subset. construction. The dominant cost is, in principle, the number of states
of the DFA, which can be 2”. For each state, we can compute the transitions
in O(n?) time by consulting the e-closure information and the NFA’s trausition
table for each of the input symbols. That is, suppose we want to compute
5({@1,G2,.-.,4%},a@) for the DFA. There may be as many as 7 states reachable
from each g; along e-labeled paths, and each of those states may have up to 2
arcs labeled a. By creating an array indexed by states, we can compute the
union of up to n sets of up to vz states in time proportional to nr’.

In this way, we can compute, for each g;, the set of states reachable from
q, along a path labeled a (possibly including e's). Since * < 1, there are at
inost 72 states to deal with. We compute the reachable states for each in O(n”)
time. Thus, the total time spent computing reachable states is O(n*). The
union of the sets of reachable states requires only O(n?) additional time, and
we couclude that the computation of one DFA transition takes O(n°)} time.

Note that the number of input symbols is assumed constant, and does not
depend on nm. Thus, in this and other estimates of running time, we do not
consider the number of input symbols as a factor. The size of the input alpha-
bet influences the constant factor that is hidden in the “big-oh” notation, but
nothing more.

Our conclusion is that the running time of NFA-to-DFA conversion, in¢lid-
ing the case where the NFA has e-transitions, is O(n°2"). Of course in practice
it is common that the number of states created is much less than 2", often only
n states. We could state the bound on the runing time as O(n3s), where s is
the number of states the DFA actually has.

DFA-to-NFA Conversion

This conversion is simple, and takes O(n} time on an n-state DFA. All that we
need to do is modify the transition table for the DFA by putting set-brackets
around states and, if the output is an e-NFA, adding a column for ¢. Since we
treat the number of input symbols (i.c., the width of the transition table) as a
constant, copying and processing the table takes O(n) time.

"lor a discussion of transilive closure algorithms, sce A. V. Aho, J. E. Hopcroft, and J.
D. Ullman, Data Siractures and Algorithms, Addison-Wealey, 1984.


--- Page 167 ---
4.3. DECISION PROPERTIES OF REGULAR LANGUAGES 151

Automaton-to-Regular-Expression Conversion

If we examine the construction of Section 3.2.1 we observe that at each of n
rounds (where 7 is the number of states of the DFA) we can quadruple the size
of the regular expressions constructed, since each is built from four expressions
of the previous round. Thus, simply writing down the n* expressions can take
time O{n34"). The improved construction of Section 3.2.2 reduces the constant
factor, but does not affect the worst-case exponentiality of the problem.

The same construction works in the same running time if the input is an
NFA, or even an e-~NFA, although we did not prove those facts. It is important
to use those constructions for NFA’s, however. If we first convert an NFA to
a DFA and then convert the DFA to a regular expression, it could take time
O(n34"°2"), which is doubly exponential.

Regular-Expression-to-Automaton Conversion

Conversion of a regular expression to an ¢-NFA takes linear tinic. We need to
parse the expression efficiently, using a technique that takes only O(n) time on
a regular expression of length 7.1 The result is an expression tree with one
node for each symbol of the regular expression (although parentheses do not
have to appear in the tree; they just guide the parsing of the expression).

Once we have an expression tree for the regular expression, we can work
up the tree, building the e-NFA for each node. The construction rules for the
conversion of a regular expression that we saw in Section 3.2.3 never add more
than two states and four arcs for any node of the expression tree. Thus, the
numbers of states and arcs of the resulting e NFA are both O(n). Moreover,
the work at each node of the parse tree in creating these elements is constant,
provided the function that processes each subtree returns pointers to the start
and accepting states of its automaton.

We conclude that construction of an e~-NFA from a regular expression takes
time that is linear in the size of the expression. We can eliminate ¢-transitions
from an n-state «NFA, to make an ordinary NFA, in O(n*) time, without
increasing the number of states. However, proceeding to a DFA can take expo-
nential time.

4.3.2 Testing Emptiness of Regular Languages

At first glance the auswer to the question “is regular language £ empty?” is
obvious: @ is empty, and all other regular languages are not. However, as we
discussed at the beginning of Section 4.3, the problem is not stated with an
explicit list of the strings in Z. Rather, we are given some representation for L
and need to decide whether that representation denotes the language @.

tParsing methods capable of doing this task in O(n} time are discussed in A. ¥. Aho,
Ri. Sethi, and J.D. Ullman. Compiler Design: Principles, Toots, and Techniques, Addison-
Wesley, 1986,


--- Page 168 ---
152 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

If our representation is any kind of finite automaton, the emptiness question
is whether there is any path whatsoever from the start state to some accepting
state. If so, the language is nonempty, while if the accepting states are all
separated from the start state, then the language is empty. Deciding whether
we can reach an accepting state from the start state is a simple instance of
graph-reachability, similar in spirit to the calculation of the e-closure that we
discussed in Section 2.5.3. The algorithm can be summarized by this recursive
process.

BASIS: The start state is surely reachable from the start state.

INDUCTION: If state q is reachable from the start state, and there is an arc
from g to p with any label (an input symbol, or ¢ if the automaton is an e-NFA},
then p is reachable.

In that manner we can compute the set of reachable states. If any accepting
state is among them, we answer “no” (the language of the automaton is not
empty}, and otherwise we answer “yes.” Note that the reachability calculation
takes no more time that O(n”) if the autornaton has nm states, and in fact it is
no worse than proportional to the number of ares in the automaton’s transition
diagram, which could be less than n® and cannot be more than O(n”),

If we are given a regular expression representing the language Z, rather
than an automaton, we could convert the expression to an e-NFA and proceed
as above. Since the automaton that results from a regular expression of length
nm has at most O(n) states and transitions, the algorithm takes O(n) time.

However, we can also inspect the regular expression to decide whether it
is empty. Notice first that if the expression has no occurrence of @, then its
language is surely not empty. If there are @’s, the language may or may not be
empty. The following recursive rules tell whether a regular expression denotes
the empty language.

BASIS: denotes the empty language; € and a for any input symbol a do not.

INDUCTION: Suppose £# is a regular expression. There are four cases to con-
sider, corresponding to the ways that A could be constructed.

1. R= R, + Re. Then L(R) is empty if and only if both L(A) and L(Re)
are empty.

2. R= RRs. Then L(R) is empty if and only if either L(A) or L( Re) is
empty-

3. R= Ri. Then L{R) is not empty; it always includes at least e.

4. R=(R,). Then L(A) is empty if and only if L(R,) is empty, since they
are the same language.


--- Page 169 ---
4.3. DECISION PROPERTIES OF REGULAR LANGUAGES 153

4.3.3 Testing Membership in a Regular Language

The next question of importance is, given a string w and a regular language LD,
is win L. While w is represented explicitly, £ is represented by an automaton
or regular expression.

If L is represented by a DFA, the algoritiun is simple. Simnulate the DFA
processing the string of input symbols w, beginning in the start state. If the
DFA ends in an accepting state, the answer is “yes”; otherwise the answer is
“no.” This algorithm is extremely fast. If |w| =n, and the DFA is represented
by a suitable data structure, such as a two-dimensional array that is the transi-
tion table, then each transition requires constant time, and the entire test takes
O(n) time.

If L has any other representation besides a DFA, we could convert to a DFA
and run the test above. That approach could take time that is exponential
in the size of the representation, although it is linear in |w|. However, if the
representation is an NFA or «-NFA, it is simpler and more efficient to simulate
the NFA directly. That is, we process symbols of w one at a time, maintaining
the set of states the NFA can be in after following any path labeled with that
prefix of w. The idea was presented in Fig. 2.10.

If w is of length n, and the NFA has s states, then the running time of this
algorithm is O(rs?). Each input symbol can be processed by taking the previous
set of states, which numbers at most s states, and looking at the successors of
each of these states. We take the union of at most s sets of at most s states
each, which requires O(s?) time.

If the NFA has e-transitions, then we must compute the e-closure before
starting the simulation. Then the processing of each input symbol a has two
stages, each of which requires O{s") time. First, we take the previous set of
states and find their successors on input symbol @. Next, we compute the e-
closure of this set of states. The initia] sct of states for the simulation is the
e-closure of the initial state of the NFA.

Lastly, if the representation of L is a regular expression of size s, we can
convert to an «NFA with at most 2s states, in O(s) time. We then perform
the simulation above, taking O(ns*) time on an input w of length z.

4.3.4 Exercises for Section 4.3

Exercise 4.3.1: Give an algorithm to tell whether a regular language Eis
infinite. Hint: Use the pumping lemma to show that if the language contains
any string whose length is above a certain lower limit, then the language must
be infinite.

Exercise 4.3.2: Give an algorithm to tell whether a regular language Z con-
tains at least. 100 strings.

Exercise 4.3.3: Suppose L is a regular language with alphabet =. Give an
algorithm to tell whether L = b*, i-e., all strings over its alphabet.


--- Page 170 ---
154 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

Exercise 4.3.4: Give an algorithm to tell whether two regular languages Ly
and ZL» have at least one string in common.

Exercise 4.3.5: Give an algorithm to tell, for two regular languages £, and
Ly over the same alphabet ©, whether there is any string in ©* that is in neither
Ly nor Lo.

4.4 Equivalence and Minimization of Automata

In contrast to the previous questions — emptiness and membership — whose
algorithins were rather simple, the question of whether two descriptions of two
regular languages actually define the same language involves considerable intel-
lectual mechanics. In this section we discuss how to test whether two deseriptors
for regular languages are equivalent, in the sense that they define the same lan-
guage. An important consequence of this test is that there is a way to minimize
a DFA, That is, we can take any DFA and find an equivalent DFA that has
the minimum number of states. In fact, this DFA is essentially unique: given
any two minimum-state DFA’s that are equivalent, we can always find a way
to rename the states so that the two DFA’s become the same.

4.4.1 Testing Equivalence of States

We shall begin by asking a question about the states of a single DFA. Our goal
ig to understand when two distinct states p and q can be replaced by a single
state that behaves like both p and g. We say that states p and g are equivalent
ifs

e For all input strings w, 4p, w) is an accepting state if and only if 4(q, w)
is an accepting state.

Less formally, it is impossible to tell the difference between equivalent states
p and g merely by starting in one of the states and asking whether or not a
given input string leads to acceptance when the automaton is started in this
(unknown) state. Note we do not require that 4{p,w) and 4(q, w) are the same
state, only that either both are accepting or both are nonaccepting.

If two states are not equivalent, then we say they are distinguishable. That
is, state p is distinguishable from state q if there is at least one string w such
that one of A(p, w) and A(g,w) is accepting, and the other is not accepting.

Example 4.18: Consider the DFA of Fig. 4.8, whose transition function we
shall refer to as 6 in this example. Certain pairs of states are obviously not
equivalent. For example, C and G are not equivalent because one is accepting
and the other is not. That is, the empty string distinguishes these two states,
because 4(C, €) is accepting and 6(G,e) is not.

Consider states A and G. String € doesn’t distinguish them, because they are
both nonaccepting states. String 0 doesn’t distinguish them because they go to


--- Page 171 ---
4.4. EQUIVALENCE AND MINIMIZATION OF AUTOMATA 15

©

G1

Figure 4.8: An automaton with equivalent states

states B and G, respectively on input 0, and both these states are nonaccepting.
Likewise, string 1 doesn’t distinguish A from G,, because they go to F and £,
respectively, and both are nonaccepting. However, 01 distinguishes .4 from G,
because 6{4,01) = C, d(G,01) = E, C is accepting, and # is not. Any input
string that takes A and G to states only one of which is accepting is sufficient
to prove that A and G are not equivalent.

In contrast, consider states A and E. Neither is accepting, so « does not
distinguish them. On input 1, they both go to state F. Thus, no input string
that begins with 1 can distinguish A from #, since for any string x, 6(A, 1x) =
8(B, 1x).

Now consider the behavior of states A and £ on inputs that begin with 0.
They go to states B and H, respectively. Since neither is accepting, string 0
by itself does not distinguish A from E. However, B and H are uo help. On
input 1 they both go to C, and on input 0 they both go to G. Thus, all inputs
that begin with 0 will fail to distinguish A from E. We conclude that no input
string whatsoever will distinguish A from &; i.e., they are equivalent states. O

To find states that are equivalent, we make our best efforts to find pairs
of states that are distinguishable. It is perhaps surprising, but true, that if
we try our best, according to the algorithm to be described below, then any
pair of states that we do not find distinguishable are equivalent. The algo-
rithm, which we refer to as the table-filling algorithm, is a recursive discovery
of distinguishable pairs in a DFA A = (Q, 2.4, qo; F)-

BASIS: If p is an accepting state and g is nonaccepting, then the pair {p,q} is
distinguishable.

INDUCTION: Let p and q be states such that for some input symbol a, r =
6(p,a) and s = 6(q,a) are a pair of states known to be distinguishable. Then


--- Page 172 ---
156 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

{p,q} is a pair of distinguishable states. The reason this rule makes sense is
that there must be some string w that distinguishes r from s; that is, exactly
one of b(r, w) and d(s, ,w) is accepting. Then string aw must distinguish p from
q, since 6(p,aw) and 5(q,aw) is the same pair of states as A(r,w) and 4(s, w).

Example 4.19: Let us cxccute the table-filling algorithm on the DFA of
Fig 4.8. The final table is shown in Fig. 4.9, where an x indicates pairs of
distinguishable states, and the blank squares indicate those pairs that have
been found equivalent. Initially, there are no z’s in the table.

Ta 7h BA Bw

Figure 4.9: Table of state inequivalences

For the basis, since C' is the only accepting state, we put 2’s in each pair
that involves C. Now that we know some distinguishable pairs, we can discover
others. For instance, since {C,H} is distinguishable, and states E and F go to
Hf and C, respectively, on input 0, we know that {F£, F} is also a distinguishable
pair. In fact, all the x’s in Fig. 4.9 with the exception of the pair (4,G} can be
discovered simply by looking at the transitions from the pair of states on cither
0 or on 1, and observing that (for one of those inputs) one state goes to C’ and
the other does not. We can show {A,G} is distinguishable on the next round,
since on input 1 they go to F and &, respectively, and we already established
that the pair {&, F} is distinguishable.

However, then we can discover no more distinguishable pairs. The three
remaining pairs, which are therefore equivalent pairs, are {A, £}, {B,}, and
{D,F}. For example, consider why we can uot infer that {4, E} is a distin-
guishable pair. On input 0, 4 and £ go to B and H, respectively, and {B,H}
has not yet been shown distinguishable. On input 1, A and E both go to F, so
there is no hope of distinguishing them that way. The other two pairs, {B, 1}
and {D, F} will never be distinguished because they each have identical tran-
sitions on 0 and identical transitions on 1. Thus, the table-filling algorithm
stops with the table as shown in Fig. 4.9, which is the correct determination of
equivalent and distinguishable states. ©


--- Page 173 ---
4.4. EQUIVALENCE AND MINIMIZATION OF AUTOMATA 157

Theorem 4.20: If two states are not distinguished by the table-filling algo-
rithm, then the states are equivalent.

PROOF: Let us again assume we are talking of the DFA A = (Q,%,3, go, F).
Suppose the theorem is false; that is, there is at least one pair of states {p,q}
such that

1. States p and q are distinguishable, in the sense that there is some string
w such that exactly one of d(p, w) and 4(q, w) is accepting, and yet

2. The table-filling algorithm does not find p and g to be distinguished.

Call such a pair of states a bad pair.

If there are bad pairs, then there must be some that are distinguished by the
shortest strings among all those strings that distinguish bad pairs. Let {p,q}
be one such bad pair, and let w = @,a2+--¢@, be a string as short as any that
distinguishes p from g. Then exactly one of 6(p, uw) and 6(q¢,w) is accepting.

Observe first that w cannot be «, since if « distinguishes a pair of states,
then that pair is marked by the basis part of the table-filling algorithm. Thus,
n> 1.

Consider the states r = d(p,a,) and s = 6(g,a1). States r and ¢ are distin-
guished by the string a2ay---dn, Since this string takes r and s to the states
d(p,w) and 5(q, w). However, the string distinguishing r from ¢ is shorter than
any string that distinguishes a bad pair. Thus, {r,s} cannot be a bad pair.
Rather, the table-filling algorithm must have discovered that they are distin-
guishable.

But the inductive part of the table-filling algorithm will not stop until it has
also inferred that p and g are distinguishable, since it finds that é(p, a1) = r is
distinguishable from 6(g,@;) = s. We have contradicted our assumption that
bad pairs exist. If there are no bad pairs, then every pair of distinguishable
states is distinguished by the table-filling algorithm, and the theorem is true.
ol

4.4.2 Testing Equivalence of Regular Languages

The table-filling algorithm gives us an easy way to test if two regular languages
are the same. Suppose languages £ and Af are each represented in some way,
e.g., one by a regular expression and one by an NFA. Convert each represent-
ation to a DFA. Now, imagine one DFA whose states are the union of the
states of the DFA’s for E and Af. Technically, this DFA has two start states,
but actually the start state is irrelevant as far as testing state equivalence is
concerned, so make any state the lone start state.

Now, test if the start states of the two original DFA’s are equivalent, using
the table-filling algorithm. If they are equivalent, then L = M, and if not, then
LEM.


--- Page 174 ---
158 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

0 I

l

Figure 4.10: Two equivalent DFA’s

Example 4.21: Consider the two DFA’s in Fig. 4.10. Bach DFA accepts
the empty string and all strings that end in @; that is the language of regular
expression ¢ + (0 + 2)*0. We can imagine that Fig. 4.10 represents a single
DFA, with five states A through EB. If we apply the table-filling algorithm to
that automaton, the result is as shown in Fig. 4.11.

moe 4 &

A B C D

Figure 4.11: The table of distinguishabilities for Fig. 4.10

To see how the table is filled out, we start by placing «’s in all pairs of
states where exactly one of the states is accepting. It turns out that there is
no more to do, The four remaining pairs, {A,C}, {A,D}, {C, D}, and {B, E}
are all equivalent pairs. You should check that no more distinguishable pairs
are discovered in the inductive part of the table-filling algorithm. For instance,
with the table as in Fig. 4.11, we cannot distinguish the pair {A, D} because
on 0 they go to themselves, and on 1 they go to the pair {B, £}, which has


--- Page 175 ---
4.4. EQUIVALENCE AND MINIMIZATION OF AUTOMATA 159

not yet been distinguished. Since A and C' are found equivalent by this test,
and those states were the start states of the two original automata, we conclude
that thesc DFA’s do accept the same language. ©

The time to fill out the table, and thus to decide whether two states are
equivalent is polynomial in the number of states. If there are n states, then
there are (3). or n(n — 1)/2 pairs of states. _In one round, we consider all pairs
of states, to sce if one of their successor pairs has been found distinguishable,
so a round surely takes no more than O(n?) time. Moreover, if on some round,
no additional x's are placed in the table, then the algorithm ends. Thus, there
can be no more than O(n) rounds, and O(n*) is surely an upper bound on the
running time of the table-filling algorithm.

However, a more careful algorithm can fill the table in O(n’) time. The
idea is to initialize, for each pair of states {r,s}, a list of those pairs {p,q} that
“depend on” {r,s}. That. is, if {r,s} is found distinguishable, then {p.q} is
distinguishable. We create the lists initially by examining each pair of states
{p,q}, and for each of the fixed number of input symbols a. we put {p,q} on
the list for the pair of states {6(p, a), 6(g, @)}, which are the successor states for
pand g on input a.

If we ever find {r,s} to be distinguishable, then we go down the list for
{r,s}. For each pair on that list that is not already distinguishable, we make
that pair distinguishable, and we put the pair on a queue of pairs whose lists
we must. check similarly.

The total work of this algorithm is proportional to the sum of the lengths
of the lists, since we are at all times either adding something to the lists (ini-
tialization) or examining a member of the list for the first and Jast time (when
we go down the list for a pair that has been found distinguishable). Since the
size of the input alphabet is considered a constant, each pair of states is put on
O(1) lists. As there are O(n?) pairs, the total work is O(n*).

4.4.3. Minimization of DFA’s

Another important consequence of the test for equivalence of states is that we
can “minimize” DFA’s. That is, for each DFA we can find an equivalent DFA
that has as few states as any DFA accepting the sutne language. Moreover,
except for our ability to call the states by whatever names we choose, this
minimum-state DFA is unique for the language. The algorithm is as follows:

1. First, eliminate any state that cannot be reached from the start state.

2. Then, partition the remaining states into blocks, so that. all states in the
same block are equivalent, and no pair of states from different blocks are
equivalent, Theorem 4.24, below. shows that we can always make such a
partition.

Example 4.22: Consider the table of Fig. 4.9, where we determined the state
equivalences and distinguishabilities for the states of Fig. 4.8. The partition


--- Page 176 ---
160 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

of the states into equivalent blocks is ({A, EZ}. {B, HZ}, {C}, {D, F}, {G}).
Notice that the three pairs of states that are equivalent are each placed in a
block together, while the states that are ‘distinguishable from all the other states
are each in a block alone.

For the automaton of Fig. 4.10, the partition is ({4,C,D}, {B, E}). This
example shows that we can have more than two states in a block. It may
appear fortuitous that A, C, and D can all live together in a block, because
every pair of them is equivalent, and none of them is equivalent to any other
state. However, as we shall see in the next theorem to be proved, this situation
is guaranteed by our definition of “equivalence” for states. O

Theorem 4.23: The equivalence of states is transitive. That is, if in some
DFA A = (Q,5,4,q0,F) we find that states p and g are equivalent, and we also
find that q and r are equivalent, then it must be that p and r arc equivalent.

PROOF: Note that transitivity is a property we expect. of any relationship called
“equivalence.” However, simply calling something “equivalence” doesn’t make
it transitive; we must prove that the name is justified.

Suppose that the pairs {p,q} and {g,r} are equivalent, but pair {p,7r} is
distinguishable. Then there is some input string w such that exactly one of
b(p, w) and é(r,w) is an accepting state. Suppose, by symmetry, that é(p, w)
is the accepting state. .

Now consider whether d(g, w) is accepting or not. If it is accepting, then
{q,r} is distinguishable, since 8(q,w) is accepting, and d(r,w) is not. If 6(q, w)
is nonaccepting, then {p,¢@} is distinguishable for a similar reason. We conclude
by contradiction that {p,r} was not distinguishable, and therefore this pair is
equivalent. O

We can use Theorem 4.23 to justify the obvious algorithm for partitioning
states. For each state g, construct a block that consists of g and all the states
that are equivalent to g. We must show that the resulting blocks are a partition:
that is, no state is in two distinct blocks.

First, observe that all states in any block are mutually equivalent. That is,
if p and r are two states in the block of states equivalent to g, then p and r are
equivalent, to each other, by Theorem 4.23.

Suppose that there are two overlapping, but not identical blocks. That
is, there is a block & that includes states » and g, and another block C that
includes p but not g. Since p and g are in a block together, they are equivalent.
Consider how the block C was formed. If it was the block generated by p, then
q would be in C, because those states are equivalent. Thus, it must be that
there is some third state s that generated block C; i-e., C is the set of states
equivalent to s.

We know that p is equivalent to s, because pis in block C. We also know that
p is equivalent to g because they are together in block B. By the transitivity of
Theorem 4.23, g is equivalent to s. But then g belongs in block C’, a contradic-
tion. We conclude that equivalence of states partitions the states; that is, two


--- Page 177 ---
4.4. EQUIVALENCE AND MINIMIZATION OF AUTOMATA 161

states either have the same set of equivalent states (including themselves), or
their equivalent states are disjoint. To conclude the above analysis:

Theorem 4.24: If we create for each state g of a DFA a block consisting of
q and ali the states equivalent to g, then the different blocks of states form a
partition of the set of states.2 That is, each state is in exactly one block. All
members of a block are equivalent, and no pair of states chosen from different
blocks are equivalent. O

We are now able to state succinctly the algorithm for minimizing a DFA

1. Use the table-filling algorithm to find all the pairs of equivalent states.

9. Partition the set of states Q into blocks of mutually equivalent. states by
the method described above.

3. Construct the minimum-state equivalent DFA B by using the blocks as
its states. Let 7 be the transition function of B. Suppose S is a sct of
equivalent states of A, and a is an input symbol. Then there must exist one
block T of states such that for all states ¢ in S, 6(q, 2) is a member of block
T. For if not, then input symbol a takes two states p and q of 5 to states
in different blocks, and those states are distinguishable by Theorem 4.24.
That fact lets us conclude that p and g are not equivalent, and they did
not both belong in $. As a consequence, we can let 7(5, a) = T. In
addition:

(a) The start state of B is the block containing the start state of A.

(b) The set of accepting states of B is the set of blocks containing ac-
cepting states of A. Note that if one state of a block is accepting,
then all the states of that block must be accepting. The reason ix
that any accepting state is distinguishable from any nonaccepting
state, so you can’t have both accepting and nonaccepting states in
one block of equivalent states.

Example 4.25: Let us minimize the DFA from Fig. 4.8. We established the
blocks of the state partition in Example 4.22. Figure 4.12 shows the minimum-
state automaton, Tts Ave states correspond to the five blocks of equivalent states
for the automaton of Fig. 4.8.

The start state is {A,B}, since A was the start state of Fig. 4.8. The ouly
accepting state is {C}, since C is the only accepting state of Fig. 4.8. Notice
that the transitions of Fig. 4.12 properly reflect the transitions of Fig. 4.8. For
instance, Fig. 4.12 has a transition on input 0 from {A, E\ to {8,H}. That

SYou should remember that the same block may be formed several times, starting from
different states. [loweyer, the partition consists of the different blocks. so this block appears
only once in (he partition.


--- Page 178 ---
162 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

Figure 4.12: Minimum-state DFA equivalent to Fig. 4.8

makes sense, because in Fig. 4.8, A goes to B on input 0, and F goes to H.
Likewise, on input 1, {A,£} goes to {D, F}. If we examine Fig. 4.8, we find
that both A and # go to F on input 1, so the selection of the successor of
{A, E} on input 1 is also correct. Note that the fact neither 4 nor E goes to
DP on input 1 is not important. You may check that all of the other transitions
are also proper. O

4.4.4 Why the Minimized DFA Can’t Be Beaten

Suppose we have a DFA A, and we minimize it to construct a DFA M, using the
partitioning method of Theorem 4.24. That theorem shows that we can’t group
the states of A into fewer groups and still have an equivalent DFA. However,
could there be another DFA N, unrelated to A, that accepts the same language
as A and M, yet has fewer states than Mf? We can prove by contradiction that
N does not exist.

First, run the state-distinguishability process of Section 4.4.1 on the states of
M and N together, as if they were one DFA. We may assume that the states of
iM and N have no names in common, so the transition function of the combined
automaton is the union of the transition rules of M7 and N, with no interaction.
States are accepting in the combined DFA if and only if they are accepting in
the DFA from which they come.

The start states of M and N are indistinguishable because L(M) = L(N).
Further, if {p,q} are indistinguishable, then their successors on any one input


--- Page 179 ---
4.4. EQUIVALENCE AND MINIMIZATION OF AUTOMATA 163

Minimizing the States of an NFA

You might imagine that the same state-partition technique that minimizes
the states of a DFA could also be used to find a minimum-state NFA
equivalent to a given NFA or DFA. While we can, by a process of exhaustive
enumeration, find an NFA with as few states as possible accepting a given
regular language, we cannot simply group the states of some given NFA
for the language.

An example is in Fig. 4.13. None of the three states are equivalent.
Surely accepting state B is distinguishable from nonaccepting states A and
C. However, A and C are distinguishable by input. 0. The successors of C’
are A alone, which does not inclide an accepting state, while the successors
of A are {A,B}, which does include an accepting state. Thus, grouping
equivalent states does not reduce the number of states of Fig. 4.13.

However, we can find a smaller NFA for the same language if we
simply remove state C. Note that A and £ alone accept all strings ending
in 0, while adding state C’ does not allow us to accept any other strings.

symbol are also indistinguishable. The reason is that if we could distinguish
the successors, then we could distinguish p from q.

Neither Af nor N could have an inaccessible state, or else we could eliminate
that state and have an even smaller DFA for the same language. Thus, every
state of M is indistinguishable from at least one state of N. To sec why, suppose
pis a state of M. Then there is some string @,a2---@, that takes the start
state of Af to state p. This string also takes the start state of Vv to some state
q. Since we know the start states are indistinguishable, we also know that their
successors under input symbol a, are indistinguishable. Then, the successors
of those states on input a are indistinguishable, and so on. until we conclude

0,1

0

Start

Figure 4.13: An NFA that cannot be minimized by state equivalence


--- Page 180 ---
164 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

that p and q are indistinguishable,

Since has fewer states than M, there are two states of Af that are in-
distinguishable from the same state of VV’, and therefore indistinguishable from
each other. But AY was designed so that all its states are distinguishable from
each other. We have a contradiction, so the assumption that N exists is wrong,
and Af in fact has as fow states as any equivalent DFA for .4. Formally, we
have proved:

Theorem 4,26: If Ais a DFA, and M the DFA constructed from A by the
algorithm described in the statement of Theorem 4.24, then i has as few states
as any DFA equivalent to 4. O

In fact we can say something even stronger than Theorem 4.26. There must
be a one-to-one correspondence between the states of any other minimum-state
N and the DFA M. The reason is that we argued above how each state of M
must be equivalent to one state of N, and no state of M can be equivalent to
two states of N. We can similarly argue that no state of N can be equivalent
to two states of Af, although each state of N must be equivalent to one of Af’s
states. Thus, the minimum-state DFA equivalent to A is unique except for a
possible renaming of the states.

| 0 | 1
3SAIBIA
BAC
CidDIB
tD | DSA
BEY, DIF
FYUG|E
G|PF |G
AWG |D

Figure 4.14: A DFA to be minimized

4.4.5 Exercises for Section 4.4
* Exercise 4.4.1: In Fig. 4.14 is the transition table of a DFA.
a) Draw the table of distinguishabilities for this automaton.
b) Construct the minimum-state equivalent DFA.
Exercise 4.4.2: Repeat Exercise 4.4.1 for the DFA of Fig 4.15.
!! Exercise 4.4.3: Suppose that p are g are distinguishable states of a given DFA

A with n states. As a function of n, what is the tightest upper bound on how
long the shortest string that distinguishes p from q can be?


--- Page 181 ---
4.5. SUMMARY OF CHAPTER 4 165

| 0 1
~AI[B]E
BIC |LF
aC || D | &
DE |#
E| F |g
ar | GIB
G| H|B
Als? | ¢
al || A | &

Figure 4.15: Another DFA to minimize

4.5 Summary of Chapter 4

+ The Pumping Lemma: If a language is regular, then every sufficiently long
string in the language has a nonempty substring that can be “pumped,”
that is, repeated any number of times while the resulting strings are also
in the language. This fact can be used to prove that many different
languages are not regular.

+ Operations That Preserve the Property of Being a Regular Language:
There are many operations that, when applied to regular languages, yield
a regular language as a result. Among these are union, concatenation, clo-
sure, intersection, complementation, difference, reversal, homomorphism
(replacement of each symbol by an associated string), and inverse homo-
morphism.

Testing Emptiness of Regular Languages: There is an algorithm that,
given a representation of a regular language, such as an automaton or
regular expression, tells whether or not the represented language is the
empty set.

+ Testing Membership in a Regular Language: There is an algorithm that,
given a string and a representation of a regular language, tells whether or
not the string is in the language.

+ Testing Distinguishability of States: Two states of a DFA are distinguish-
able if there is an input string that takes exactly one of the two states to
an accepting state. By starting with only the fact that pairs consisting of
one accepting and one nonaccepting state are distinguishable, and trying
to discover addition pairs of distinguishable states by finding pairs whose
successors on one input symbol are distinguishable, we can discover all
pairs of distinguishable states.


--- Page 182 ---
166 CHAPTER 4. PROPERTIES OF REGULAR LANGUAGES

+ Minimizing Deterministic Finite Automata: We can partition the states
of any DFA into groups of mutually indistinguishable states. Members of
two different groups are always distinguishable. If we replace each group
by a single state, we get an equivalent DFA that has as few states as any
DFA for the same language.

4.6 References for Chapter 4

Except for the obvious closure properties of regular expressions — union, con-
catenation, and star — that were shown by Kleene [6], almost all results about
closure properties of the regular languages mimic similar results about context-
free languages (the class of languages we study in the next chapters). Thus,
the pumping lemma for regular languages is a simplification of a correspond-
ing result for context-free languages by Bar-Hillel, Perles, and Shamir [1]. The
same paper indirectly gives us several of the other closure properties shown
here. However, the closure under inverse homomorphism is from [2].

The quotient operation introduced in Exercise 4.2.2 is from [8]. In fact, that
paper talks about a more general operation where in place of a single symbol a
is any regular language. The series of operations of the “partial removal” type,
starting with Exercise 4.2.8 on the first halves of strings in a regular language,
began with [8]. Seiferas and McNaughton [9] worked out the general case of
whou a removal operation preserves regular languages.

The original decision algorithms, such as emptiness, finiteness, and meniber-
ship for regular languages, are from [7]. Algorithms for minimizing the states
of a DPA appear there and in [5]. The most efficient algorithm for finding the
ininimurm-state DFA is in [4].

1. ¥. Bar-Hillel, M. Perles, and E. Shamir, “On formal properties of simple
phrase-structure grammars,” 4. Phonetik. Sprachwiss. Kominunikations-
forseh. 14 (1961), pp. 143 172.

2, &. Ginsburg and G. Rose, “Operations which preserve definability in lan-
guages.” J. ACM 10:2 (1963), pp. 175-195.

3. 5. Ginsburg and E. H. Spanier, “Quotients of context-free languages,” JJ.
ACM 10:4 (1963). pp. 487-492.

4. J. E. Moperoft, “Au nlogz algorithm for minimizing the states in a finite
automaton,” in Z. Kohavi (ed.) The Theory of Machines and Computa-
tions, Academic Press, New York, pp. 189-196,

. D. A. Huffman, “The synthesis of sequential switching circuits,” J. Frank-
lin Inst. 257:3-4 (1954), pp. 161 190 and 275-303.

on

6. S.C. Kleene, “Representation of events in nerve nets and finite automata.”
in C. B. Shannon and J. McCarthy, Automate Studies, Princeton Univ.
Press, 1956, pp. 3-42.


--- Page 183 ---
4.6. REFERENCES FOR CHAPTER 4 167

7. E. F. Moore, “Gedanken experiments on sequential machines,” in C. E.

Shannon and J. McCarthy, Automata Studies, Princeton Univ. Press,
1956, pp. 129-153.

8. R. E. Stearns and J. Hartmanis, “Regularity-preserving modifications of
regular expressions,” Information and Control 6:1 (1963), pp. 55-69.

9. J. 1 Seiferas and R. McNaughton, “Regularity-preserving modifications,”
Theoretical Computer Science 2:2 (1976), pp. 147-154.


--- Page 184 ---


--- Page 185 ---
Chapter 5

Context-Free Grammars
and Languages

We now turn our attention away from the regular languages to a larger class of
languages, called the “context-free languages.” These languages have a natu-
ral, recursive notation, called “context-free grammars.” Context-tree grammars
have played a central role in compiler technology since the 1960’s; they turned
the implementation of parsers (functions that discover the structure of a pro-
gram) from a time-consuming, ad-hoc implementation task into a routine job
that can be done in an afternoon. More recently, the context-free grammar has
been used to describe document formats, via the so-called document-type defi-
nition (DTD) that is used in the XMI. (extensible markup language) community
for information exchange on the Web.

In this chapter, we introduce the context-free grammar notation, and show
how grammars define languages. We discuss the “parse tree,” a picture of the
structure that a grammar places on the strings of its language. The parse tree
is the product of a parser for a programming language and is the way that the
structure of prograins is normally captured.

There is an automaton-like notation, called the “pushdown automaton,”
that also describes all and only the coutext-free languages; we introduce the
pushdown automaton in Chapter 6. While less inyportant than finite automata,
we shall find the pushdown automaton, especially its equivalence to context-free
grammars as a language-defining mechanism, to be quite useful when we explore
the closure and decision properties of the context-free languages in Chapter 7.

5.1 Context-Free Grammars
We shall begin by introducing the context-free grammar uotation informally.
After seeing some of the important capabilities of these grammars, we offer

formal definitions. We show how to define a grammar formally, and introduce

169


--- Page 186 ---
170 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

the process of “derivation,” whereby it is determined which strings are in the
language of the grammar.

5.1.1 An Informal Example

Let us consider the language of palindromes. A palindrome is a string that reads
the same forward and backward, such as otto or madamimadam (“Madam, I’m
Adam,” allegedly the first thing Eve heard in the Garden of Eden). Put another
way, string w is a palindrome if and only if w = w, To make things simple,
we shall consider describing only the palindromes with alphabet {0,1}. This
language includes strings like 0110, 11011, and ¢, but not 011 or 0101.

It is easy to verify that the language Lpq: of palindromes of 0’s and 1’s is
not a regular language. To do so, we use the pumping lemma. If Lpq; is a
regular language, let 2 be the associated constant, and consider the palindrome
w = 0°10". If Lye is regular, then we can break w into w = zyz, such that
y consists of one or more 0’s from the first group. Thus, xz, which would also
have to be in La if Epa: were regular, would have fewer 0’s to the left of the
lone 1 than there are to the right of the 1. Therefore xz cannot be a palindrome.
We have now contradicted the assumption that Lp, is a regular language.

There is a natural, recursive definition of when a string of 0’s and 1’s is in
Epa. It starts with a basis saying that a few obvious strings are in pet, and
then exploits the idea that if a string is a palindrome, it must begin and end
with the same symbol. Further, when the first. and last symbols are removed,
the resulting string must also be a palindrome. That is:

BASIS: ¢«, 0, and 1 are palindromes.

INDUCTION: If w is a palindrome, so are 0wO and lw1. No string is a palin-
drome of 0’s and 1's, unless it follows from this basis and induction rule.

A context-free grammar is a formal notation for expressing such recursive
definitions of languages. A grammar consists of one or more variables that
represent classes of strings, ic., languages. In this example we have need for
only one variable P, which represents the set of palindromes; that is the class of
strings forming the language Lpq. There are rules that say how the strings in
each class are constructed. The construction can use symbols of the alphabet,
strings that are alrcady known to be in one of the classes, or both.

Example 5.1: The rules that define the palindromes, expressed in the context-
free grammar notation, are shown in Fig. 5.1. We shall see in Section 5.1.2 what
the rules mean.

The first three rules form the basis. They tell us that the class of palindromes
includes the strings e, 0, and 1. None of the right sides of these rules (the
portions following the arrows) contains a variable, which is why they form a
basis for the definition.

The last two rules form the inductive part of the definition. For mstance,
rule 4 says that if we take any string w from the class P, then Qw0 is also in
class P. Rule 5 likewise tells us that lwl is alsoin P, O


--- Page 187 ---
5.1. CONTEXT-FREE GRAMMARS 171

1. PO «€
2 P 73 0
3B OP A= 1
4. P —+ OPO
5. P - 1P!1

Figure 5.1: A context-free grammar for palindromes

5.1.2 Definition of Context-Free Grammars

There are four important components in a grammatical description of a lan-
guage:

1. There is a finite set of symbols that form the strings of the language being
defined. This set was {0,1} in the palindrome example we just saw. We
call this alphabet the terminals, or terminal symbols.

2. There is a finite set of variables, also called sometimes nonterminals or
syntactic categories. Each variable represents a language; i.e., a set of
strings. In our example above, there was only one variable, P, which we
used to represent the class of palindromes over alphabet {0, 1}.

3. One of the variables represents the language being defined; it is called the
start symbol. Other variables represent auxiliary classes of strings that
are used to help define the language of the start symbol. In our example,
P, the only variable, is the start symbol.

4, There is a finite set of productions or rules that represent the recursive
definition of a language. Each production consists of:

(a) A variable that is being (partially} defined by the production. This
variable is often called the head of the production.

(b} The production symbol >.

(c) A string of zero or more terminals and variables. This string, called
the body of the production, represents one way to form strings in the
language of the variable of the head. In so doing, we icave terminals
unchanged and substitute for each variable of the body any string
that is known to be in the language of that variable.

We saw an example of productions in Fig. 5.1.

Che four components just described form a context-free grammar, or just gram-
mar, or CFG. We shall represent a CFG G by its four components, that 1s,
G = (V,T, P,S), where V is the set of variables, T the terminals, P the set of
productions, and S the start symbol.


--- Page 188 ---
172 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

Example 5.2: The grammar Gp, for the palindromes is represented by
Goat = ({P}, {0, 1}. A, P)
where A represents the set of five productions that we saw in Fig. 4.1. 0

Example 5.3: Let us explore a more complex CFG that represents (a simplific-
ation of) expressions in a typical programming language. First, we shall limit
ourselves to the operators + and *, representing addition and multiplication.
We shall allow arguments to be identifiers, but instead of allowing the full set of
typical identifiers (letters followed by zero or more Icttors and digits), we shall
allow only the letters @ and b and the digits 0 and 1. Every identifier must
begin with @ or b, which may be followed by any string in {a, 4,0, 1}*.

We need two variables in this grammar. One, which we call F, represents
expressions. It is the start symbol and represents the language of expressions
we are defining. The other variable, J, represents identifiers. Its languaye is
actually regular; it is the language of the regular expression

(a+b)(a+b+0+1)*

However, we shall not use regular expressions directly in grarmmars. Rather,
we use a set of productions that say essentially the same thing as this regular
expression.

1 Eva Tf

2 E+ EB+E
3 BE +> Exké
4 BE 7 (£)

5. Foo 4

6. if 3 8b

7. i - fa

8 ff + Ib

9 F + F0

10. fF + fi

Figure 5.2: A context-irce grammar for simple expressions

The grammar for expressions is stated formally as G = ({£,7},7,P, £),
where 7 is the sect of symbols {+, », (,), @, 4,0, 1} and P is the set of productions
shown in Fig. §.2. We interpret the productions as follows.

Rule (1) is the basis rule for expressions. It says that an expression can
be a single identifier. Rules (2) through (4) describe the inductive case for
expressions. Rule (2) says that an expression can be two expressions connected
by a plus sign: rule (3) says the same with a multiplication sign. Rule (4) says


--- Page 189 ---
6.1. CONTEXT-FREE GRAMMARS 173

Compact Notation for Productions

It is convenient to think of a production as “belonging” to the variable
of its head. We shall often use remarks like “the productions for A” or
“ 4-prodnctions” to refer to the productions whose head is variable A. We
may write the productions for a grammar by listing each variable once, and
then listing all the bodies of the productions for that variable, separated by
vertical bars. That is, the productions 44 a), A a2,...,4 4 a» can
be replaced by the notation 4 -+ a) |a2|---|a,. For instance, the grammar
for palindromes from Fig. 3.1 can be written as P > €|0]1)0P0|1P1.

that if we take any expression and put matching parentheses around it, the
result is also an expression,

Rules (5) through (10) describe identifiers {. The basis is rules (5) and (6);
they say that a and 4 are identifiers. The remaining four rules are the inductive
case. They say that if we have any identifier, we can follow it by a, 6, 0, or 1,
and the result. will be another identifier. O

5.1.3 Derivations Using a Grammar

We apply the productions of a CFG to infer that certain strings are in the
language of a certain variable. There are two approaches to this inference. The
more conventional approach is to use the rules from hody to head. That is, we
take strings known to be in the language of each of the variables of the body,
concatenate them, in the proper order, with any terminals appearing in the
body, and infer that the resulting string is in the language of the variable in
the head. We shall refer to this procedure as recursive inference.

There is another approach to defining the language of a grammar, in which
we use the productions from head to body. We expand the start symbol using
one of its productions (i.e., using a production whose head is the start symbol).
We further expand the resulting string by replacing one of the variables by the
body of one of its productions, and so on, until we derive a string consisting
entirely of terminals. The language of the grammar is all strings of terminals
that we can obtain in this way. This use of grammars is called derivation.

We shall begin with an example of the first approach — recursive inference.
However, it is often more natural to think of grammars as used in derivations,
and we shall next develop the notation for describing these derivations.

Example 5.4: Let us consider some of the inferences we can make using the
grammar for expressions in Fig. 5.2. Figure 5.3 summarizes these inferences.
For example, line (¢) says that we can infer string @ is in the language for
I by using production 5. Lines (ii) through (év) say we can infer that 600


--- Page 190 ---
174 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

is an identifier by using production 6 once (to get the 8) and then applying
production 9 twice (to attach the two 0’s).

String For lang- | Production | String(s)
Inferred uage of used used

(i) | a i 5
(#2) | B i 6
(z#4) | 80 f 9
(tv) | 600 i 9
(v) | a EB 1
{wi} | &00 E 1
(wid) | a+ b00 £ 2
{utii) | (a + 600) E 4
(iz) | ax (a +600) | EF 3

Figure 3.3: Inferring strings using the grammar of Fig. 3.2

Lines (v) and (vi) exploit production 1 to infer that, since any identifier is
an expression, the strings a and 600, which we inferred in lines (¢) and (iv) to
be identifiers, are also in the language of variable &. Line (vii) uses produc-
tion 2 to infer that the sum of these identifiers is an expression; line (wifi) uses
production 4 to infer that the same string with parentheses around it is also an
expression, and line (iz) uses production 3 to multiply the identifier a by the
expression we had discovered in line (vidi). O

The process of deriving strings by applying productions from head to body
requires the definition of a new relation symbol >. Suppose G = (V,T, P, S) is
a CFG, Let aAg be a string of terminals and variables, with 4 a variable. That
is, a and @ are strings in (V UT)*, and Aisin V. Let 4 > y be a production
of G. Then we say aAS => ay, If G is understood, we just say aAd > ay.
Notice that one derivation step replaces any variable anywhere in the string by
the body of one of its productions.

We may extend the => relationship to represent zero, one, or many derivation
steps, much as the transition function 6 of a finite automaton was extended to
6. For derivations, we use a * to denote “zero or more steps,” as follows:

BASIS: For any string @ of terminals and variables, we say a = a. That is,
G

any string derives itself. ,

INDUCTION: If a = 8 and 8 me ¥, then a 4 +. That is, if a can become ?

by zero or more steps, and one more step takes 3 to 7, then a can become -y.
* * .
Put another way, a = 33 means that there is a sequence of strings 7, ¥2,-.-, Ya;

for some n > 1, such that

las THs


--- Page 191 ---
5.1. CONTEXT-FREE GRAMMARS 175

2. 3 =7,, and

3. For i=1,2,...,2—1, we have % => 41-
If grammar G is understood, then we use +. in place of > .

Example 5.5: The inference that a* (a+ 00) is in the language of variable £
can be reflected in a derivation of that string, starting with the string &. Here
is one such derivation:

B>BseEofleboar k=
a+(E)>a*(B+E)>as(l+£) Sax(at+b)=>
«(at iI}>a*(at+ 10) > a* (a+ 100) > ax (a + 600)

At the first step, E is replaced by the body of production 3 (from Fig. 5.2).
At the second step, production 1 is used to replace the first & by 7, and so
on. Notice that we have systematically adopted the policy of always replacing
the leftmost variable in the string. However, at each step we may choose which
variable to replace, and we can use any of the productions for that variable.
For instance, at the second step, we could have replaced the second E by (&),
using production 4. In that case, we would say Bx E > E*(). We could also
have chosen to make a replacement that would fail to lead to the same string
of terminals. A simple example would be if we used production 2 at the first
step, and said E > E+E. No replacements for the two £’s could ever turn
E+ E inte ax (a + 500).

We can use the > relationship to condense the derivation. We know E > E
by the basis. Repeated use of the inductive part gives us & = ExE,E=> IE,
and so on, until finally FS a (a+ 000).

The two viewpoints — recursive inference and derivation - are equivalent.
That is, a string of terminals w is inferred to be in the language of some variable
A if and only if A > w. However, the proof of this fact requires some work,
and we leave it to Section 6.2, O

5.1.4 Leftmost and Rightmost Derivations

In order to restrict the number of choices we have in deriving a string, it is
often useful to require that at each step we replace the leftmost variable hy one
of its production bodies. Such a derivation is called a leftmost der ivation, and
we indicate that a derivation is leftmost by using the relations > and =. for

one or many steps, respectively. If the grammar G that is being used is not
obvious, we can place the name G below the arrow in either of these symbols.

Similarly, it is possible to require that. at cach step the rightmost variable
is replaced by one of its bodies. If so, we call the derivation rightmost and use


--- Page 192 ---
176 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

Notation for CFG Derivations

There are a number of conventions in common use that help us remember
the role of the symbols we use when discussing CFG’s. Here are the
conventions we shall use:

1. Lower-case letters near the beginning of the alphabet, a, 4, and so
on, are terminal symbols. We shall also assume that digits and other
characters such as + or parentheses are terminals.

. Upper-case letters near the beginning of the alphabet, A, DB, and so
on, are variables.

. Lower-case letters near the end of the alphabet, such as w or z, are
strings of terminals. This convention reminds us that the terminals
are analogous to the input symbols of an automaton.

. Upper-case letters near the end of the alphabet, such as X or ¥, are
either terminals or variables.

. Lower-case Greek letters, such as @ and 3, are strings consisting of
terminals and/or variables.

There is no special notation for strings that consist of variables only, since
this concept plays no important role. However, a string named a or an-
other Greek letter might happen to have only variables.

the symbols = and *. to indicate one or many rightmost derivation steps,

rm vith
respectively. Again, the name of the grammar may appear below these symbols
if it is not clear which grammar ts beng used.

Example 5.6: The derivation of Example 5.5 was actually a leftmost deriva-
tion. Thus, we can describe the same derivation by:

B> Bebo fe E> axek=

br ft fate im

a* (EB) > ax(E+E)= ax(i+E)=> ax (a+ E) >

tyr

ar(a+iI> ax (at 10) = ax (a+ 100) > ax (a + 600)
Tf mH

fart

. : . * bi
We can also summarize the leftmost derivation by saying FE > a (a+ 00), or
im

express several steps of the derivation by expressions such as E + & = ax«(E).
im


--- Page 193 ---
5.1. CONTEXT-FREE GRAMMARS 177

There is a rightmost derivation that uses the same replacements for each
variable, although it makes the replacements in different order. This rightmost
derivation is:

E= ExB=> Bs(B)> E*(E+E)>

Prt Tt rm

(2+ D5 Ex(E+10)> Bx(E +100) > E* (2+ 000) =
Tit Pvt rm rn

Ex(1+b00) > Ex (a+b00) > I* (a+ 000) > a+ (a+ 600)
mm

rit rin

This derivation allows us to conclude E > a*(a+600). O

Any derivation has an equivalent leftmost and an equivalent rightmost der-
ivation. That is, if w is a terminal string, and A a variable, then A > w if and
only if A> w, and A “> w if and only if A= w. We shall also prove these

Tit

tim
claims in Section 5.2.

5.1.5 The Language of a Grammar

If G(V,T, P,S) is a CFG, the language of G, denoted L{G), is the set of terminal
strings that have derivations from the start symbol. That is,

G) = {win T* | 85 wt

If a language L is the language of sone context-free grammar, then L is said to
be a contezt-free language, or CFL. For instance, we asserted that the grammar
of Fig. 5.1 defined the language of palindromes over alphabet {0,1}. Thus, the
set. of palindromes is a context-free language. We can prove that statement, as
follows.

Theorem 5.7: £(Gpa:), where Gyq: is the grammar of Example 5.1, is the set
of palindromes over {0,1}.

PROOF: We shall prove that a string w in {0,1}* is in Z(Gyq;) if and only if it
is a palindrome; i-e., w = ur.

(If) Suppose w is a palindrome. We show by induction on |w| that w is in
L(Gpai)-

BASIS: We use lengths 0 and 1 as the basis. If |w| = 0 or || = 1, then w is ¢,

0, or 1. Since there are productions P > ¢, P > 0, and P 3 1, we conclude
a . :

that P => win any of these basis cases.

INDUCTION: Suppose |w| > 2. Since w = uw", w must begin and end with the
same symbol That is, w = 0x0 or w = Iai, Moreover, x must be a palindrome;
that is, 2 = 2”. Note that we need the fact that |w| > 2 to infer that there are
two distinct 0’s or 1’s, at cither end of w.


--- Page 194 ---
178 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

If w = Ox0, then we invoke the inductive hypothesis to claim that P => =.
Then there is a derivation of w from P, namely P > 0PO > 020 = w. If
w = Izl, the argument is the same, but we use the production P > 1P1 at
the first step. In either case, we conclude that w is in Z(Gpgr) and complete
the proof.

(Only-if} Now, we assume that w is in L(Gpar); that is, P + w. We must
conclude that w is a palindrome, The proof is an induction on the number of
steps in a derivation of w from P.

BASIS: If the derivation is one step, then it must use one of the three produe-
tions that do not have P in the body. That is, the derivation is P > «, P => 0,
or P => 1. Since €, 0, and 1 are all palindromes, the basis is proven.

INDUCTION: Now, suppose that the derivation takes n+ 1 steps, where n > 1,
and the statement is true for all derivations of n steps. That is, if P > z inn
steps, then z is a palindrome.

Consider an (7 + 1)-step derivation of w, which must be of the form

P>0P0> 0c0=wh

or P > IP1 > rl = w, sincen +1 steps is at least two steps, and the
productions P + 0P0and P > 1P1 are the only productions whose use allows
additional steps of a derivation. Note that in cither case, P => x in n steps.

By the inductive hypothesis, we know that z is a palindrome; that is, ¢ = 2".
But if so, then Oz0 and 1z1 are also palindromes. For instance, (020) =
Or"Q = 0x0. We conclude that w is a palindrome, which completes the proof.
G

5.1.6 Sentential Forms

Derivatious from the start symbol produce strings that have a special role. We

call these “sentential forms.” That is, if G = (V.T, P,S) is a CFG, then any

string a in (1 U T)* such that S$ 5 a is a sentential form. If S 3 a, then
fim

a is a feft-sentential form, and if S > a, then a is a right-sentential ferm.

rm
Note that the language L(G) is those sentential forms that are in T*; i.e., they
consist solely of terminals.

Example 5.8: Consider the grammar for expressions from Fig. 5.2. For ex-
ample, & + (1 + £) is a sentential form, since there is a derivation

E> ExE=>Ex«(E)>E*(E+ 5) > Ex(I+B)

However this derivation is neither leftmost aor rightmost, since at the last step,
the middle £ is replaced.

As an example of a left-sentential form, consider a * E, with the leftmost
derivation


--- Page 195 ---
5.1, CONTEXT-FREE GRAMMARS 179

The Form of Proofs About Grammars

Theorem 5.7 is typical of proofs that show a grammar defines a particular,
informally defined language. We first develop an inductive hypothesis that
states what properties the strings derived from each variable have. In this
example, there was only one variable, P, so we had only to claim that its
strings were palindromes.

We prove the “if” part: that if a string w satisfies the informal state-
ment about the strings of one of the variables A, then A=> w. In our
example, since P is the start symbol, we stated “P => w” by saying that
w is in the language of the grammar. Typically, we prove the “if” part by
induction on the length of w. If there are & variables, then the inductive
statement to be proved has & parts, which must be proved as a mutual
induction.

We must also prove the “only-if” part, that if A > w, then w sat-
isfies the informal statement about the strings derived from variable A.
Again, in our example, since we had to deal only with the start symbol
P, we assumed that w was in the language of G,qi as an equivalent to
PS w. The proof of this part is typically by induction on the number of
steps in the derivation. If the grammar has productions that allow two or
inore variables to appear in derived strings, then we shall have to break
a derivation of n steps into several parts, one derivation from each of the
variables. These derivations may have fewer than n steps, so we have to
perform an induction assuming the statement for all values n or less, as
discussed in Section 1.4.2.

E> ExE> le B> atk
im tm im
Additionally, the derivation

E> ExE> Ex(E)> Ex*(E+ £2)
rr

rim

shows that E « (£ + £) is a right-sentential form. ©

5.1.7 Exercises for Section 5.1
Exercise 5.1.1: Design context-free grammars for the following languages:

* a) The set {0"1" | n > 1}, that is, the set of all strings of one or more 0's
followed by an equal number of 1’s

*! b) The set {a*bict |i ¢ j or j # &}, that is, the set of strings of a’s followed
by 8’s followed by c’s, such that there are either a different number of a’s
and b's or a different number of 8’s and c’s, or both.


--- Page 196 ---
*

—

—

on

180 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

'c) The set of all strings of a’s and t's that are noé of the form ww, that is,
not equal to any string repeated.

!!d) The set of all strings with twice as many 0°s as I's.

Exercise 5.1.2: The following grammar generates the language of regular
expression 0°1(0 + 1)":

S + AIB
A > OAle
B > (0B|1Bl\e

Give leftrnost and rightmost derivations of the following strings:
* a) OO101.

b) 1001.

¢} 00011.

Exercise 5.1.3; Show that every regular language is a context-free language.
Hint: Construct a CFG by induction on the number of operators in the regular
expression.

Exercise 5.1.4: A CFG is said to be right-linear if each production body
has at most one variable, and that variable is at the right end. That is, all
productions of a right-linear grammar are of the form A — wB oor A > w,
where A and B are variables and w some string of zero or more terminals.

a) Show that every right-linear grammar generates a regular language. Hint:
Construct an «-NFA that simulates leftmost derivations, using its state to
represent the lone variable in the current left-sentential form.

b) Show that every regular language has a right-linear graminar. Hint: Start
with a DFA and let the variables of the grammar represent states.

Exercise 5.1.5: Let FT = {0,1,(,),+,+#,@,e}. We may think of T as the set of
symbols used by regular expressions over alphabet {0.1}: the only difference is
that we use e for symbol ¢, to avoid potential confusion in what follows. Your
task is to design a CFG with set of terminals 7 that generates exactly the
regular expressions with alphabet {0,1}.

Exercise 5.1.6: We defined the relation > with a basis “a > a” and an
induction that says “a + Band §=> + imply @ > +. There are several other
ways to define > that also have the effect of saying that «3, is zero or more
=> steps.” Prove that the following are true:

* * . 4 *
a) a> #@ if and only if there ts a sequence of one or more strings
Ves Voss Yn

such that a = 41, 3 = Yq, and fori =1,2,...,7 —1 we have yj > 4.1.


--- Page 197 ---
5.2. PARSE TREES 181

b) Tf a a 3,and 8> y, then a = y. Hint: use induction on the number
of steps in the derivation @ ay,

! Exercise 5.1.7; Consider the CFG G defined by productions:
S3aS|Sbla]é

a) Prove by induction on the string length that no string in L(G) has ba as
a substring.

b) Describe L(G) informally. Justify your answer using part (a).
!! Exercise 5.1.8: Consider the CFG G defined by productions:
S + aSbS | bSaS | €

Prove that £(G) is the set of all strings with an equal number of a@’s and 6’s.

5.2 Parse Trees

There is a tree representation for derivations that has proved extremely useful.
This tree shows us clearly how the symbols of a terminal string are grouped
into substrings, each of which belongs to the language of one of the variables of
the grammar. But perhaps more importantly, the tree, known as a “parse tree”
when used in a compiler, is the data structure of choice to represent the source
program. In a compiler, the tree structure of the source program facilitates
the translation of the source program into executable code by allowing natural,
recursive furictions to perform this translation process.

In this section, we introduce the parse tree and show that the existence of
parse trees is tied closely to the existence of derivations and recursive inferences.
We shall later study the matter of ambiguity in grammars and languages, which
is an important application of parse trees. Certain grammars allow a terminal
string to have more than one parse tree. That situation makes the grammar
unsuitable for a programming language, since the compiler could not tell the
structure of certain source programs, and therefore could not with certainty
deduce what the proper executable code for the program was.

5.2.1 Constructing Parse Trees

Let us fix on a grammar G = (V,T,P,S). The parse trees for G are trees with
the following conditions:

1. Each interior node is labeled by a variable in V.

2. Each leaf is labeled by either a variable, a terminal, or «. However, if the
leaf is labeled c, then it must be the only child of its parent.


--- Page 198 ---
182 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

Review of Tree Terminology

We assume you have been introduced to the idea of a tree and are familiar
with the commonly used definitions for trees. However, the following will
serve as a review.

e Trees are collections of nodes, with a parent-child relationship. A
node has at most one parent, drawn above the node, and zero or
more children, drawn below. Lines connect parents to their children.
Figures 5.4, 5.5, and 5.6 are examples of trees.

There is one node, the reot, that has no parent; this node appears at
the top of the tree. Nodes with no children are called leaves. Nodes
that are not leaves are interior nodes.

A child of a child of a --: node is a descendant of that node. A parent
of a parent of a ---is an ancestor. Trivially, nodes are ancestors and
descendants of themselves.

The children of a node are ordered “from the left,” and drawn so. If
node V is to the left of node AY, then all the descendants of NV are
considered to be to the left of all the descendants of AY.

3. If an interior node is labeled A, anct its children arc labeled
X1,X». nae Xk

respectively. from the left, then 4 — X)X»9---X, is a production in P.
Note that the only time one of the Xs can be ¢ is if that is the label of
the only child, and A — € is a production of G.

Example 5.9: Figure 5.4 shows a parse tree that uses the expression grammar
of Fig. 5.2. The root is labeled with the variable &. We sce that the production
used at the root is HE EH + E, since the three children of the root have labels
E, +, and E, respectively, from the left. At the leftmost child of the root, the
production & > J is used, since there is one child of that node, labeled J. O

Example 5.10: Figure 5.5 shows a parse tree for the palindrome grammar of
Fig. 5.1. The production used at. the root is P + OPO, and at the middle child
of the root it is P 4 1P1, Note that at the bottom is a use of the production
Pe. That use, where the node labeled by the head has one child, labeled ¢,
is the only time that a node labeled € can appear in a parse tree. O


--- Page 199 ---
5.2. PARSE TREES 183

Pan

Figure 5.4: A parse tree showing the derivation of J+ £ from &

Zan
yan
|

é

Figure 5.5: A parse tree showing the derivation P = 0110

5.2.2 The Yield of a Parse Tree

If we look at the leaves of any parse tree and concatenate them from the left, we
get a string, called the yield of the tree, which is always a string that is derived
from the root variable. The fact that the yield is derived from the root will be
proved shortly. Of special importance are those parse trees such that.

1. The yield is a terminal string. That is, all leaves are labeled either with
a terminal or with €.

2. The root is labeled by the start symbol.

These are the parse trees whose yields are strings in the language of the under-
lying grammar. We shall also prove shortly that another way to describe the
language of a grammar is as the set of yields of those parse trees having the
start symbol at the root and a terminal string as yield.

Example 5.11: Figure 5.6 is an example of a tree with a terminal string as
yield and the start symbol at the root; it is based on the grammar for expressions
that we introduced in Fig. 9 2. This tree's yield is the string a * (a+ 400) that
was derived in Example 5.5. In fact, as we shall see, this particular parse tree
is a representation of that det ivation.


--- Page 200 ---
184 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

aN
PAIN
PAIN

i
1 SN
/\
|

b

Figure 5.6: Parse tree showing a+ (a+ 600) is in the language of our expression
grammar

5.2.3 Inference, Derivations, and Parse Trees

Each of the ideas that we have introduced so far for describing how a grammar
works gives us essentially the same facts about strings. That is, given a grammar
G =(V,T, P,S), we shall show that the following are equivalent:

1. The recursive inference procedure determines that terminal string w is in
the language of variable A.

2 A> w.
3. A> w.
im

4 AS w.

Trt

5. There is a parse tree with root A and yield w.

In fact, except for the use of recursive inference, which we only defined for
terminal strings, all the other conditions —- the existence of derivations, leftmost
or rightmost derivations, and parse trees — are also cquivalent if w is a string
that has some variables.


--- Page 201 ---
5.2. PARSE TREES 185

We need to prove these equivalences, and we do so using the plan of Fig. 5.7.
That is, each arc in that diagram indicates that we prove a theorem that says
if w meets the condition at the tail, then it meets the condition at the head of
the arc. For instance, we shall show in Theorem 5.12 that if w is inferred to be
in the language of A by recursive inference, then there is a parse tree with root
A and yield w.

derivation vA \
a Rightmost

Derivation ““———_ derivation Recursive

ee inisrenee

Figure 5.7: Proving the equivalence of certain statements about grammars

Note that two of the ares are very simple and will not be proved formally. If
w has a leftmost derivation from A, then it surely has a derivation from A, since
a leftmost derivation is a derivation. Likewise, if w has a rightmost derivation,
then it surely has a derivation. We now procced to prove the harder steps of
this equivalence.

5.2.4 From Inferences to Trees

Theorem 5.12: Let G = (V,T,P,S) be a CFG. If the recursive inference
procedure tells us that terminal string w is in the language of variable A, then
there is a parse tree with root. A and yield w.

PROOF: The proof is an induction on the number of steps used to infer that w
is in the language of A.

BASIS: One step. Then only the basis of the inference procedure must have
been used. Thus, there must be a production A — w. The tree of Fig. 5.8,
where there is one leaf for each position of w, mects the conditions to be a parse
tree for grammar G, and it evidently has yield w and root A. In the special
case that w = ¢, the tree has a single leaf labeled € and is a legal parse tree
with root A and yield w.

INDUCTION: Suppose that the fact w is in the language of A is inferred after
n+1 inference steps, and that the statement of the theorem holds for all strings
a and variables B such that the membership of z in the language of B was
inferred using n or fewer inference steps. Consider the last step of the inference
that w is in the language of A. This inference uses some production for A, say
AW X,X3---X,, where each X; is either a variable or a terminal.


--- Page 202 ---
186 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

Figure 5.8: Tree constructed in the basis case of Theorem 5.12

We can break w up as wy we +++ w,, where:

1. If X; is aterminal, then w; = Xj; i-e., w; consists of only this one terminal
from the production.

2. If X; is a variable, then w; is a string that was previously inferred to be in
the language of X;. That is, this inference about w, took at most n of the
n-+ 1 steps of the inference that w is in the language of A. It cannot take
allm+1 steps, because the final step, using production 4 4 X|%q---Xz,
is surely not part of the inference about w,;. Consequently, we may apply
the inductive hypothesis to wj and Xj, and conclude that there is a parse
tree with yield w; and root Aj.

AA

Figure 5.9: Tree used in the inductive part of the proof of Theorem 5.12

We then construct a tree with root 4 and yield w, as suggested in Fig. 5.9.
There is a root labeled A, whose children arc X,,Xy,...,X,. This choice is
valid, since A + X/Xy---X, is a production of G.

The node for each X; is made the root of a subtree with yield w;. In case (1),
where X; is a terminal, this subtree is a trivial tree with a single node labeled
X;. That is, the subtree consists of only this child of the root. Since w; = X;
in case (1), we meet the condition that the yield of the subtree is wy.

In case (2), X; is a variable. Then, we invoke the inductive hypothesis to
claim that there is some tree with root X; and yield w;. This tree is attached
to the node for X; in Fig. 5.9.

The tree so constructed has root A. Its yield is the yields of the subtrees,
concatenated from left to right. That string is w)we---w,, whichis w. O


--- Page 203 ---
5.2. PARSE TREES 187

5.2.5 From Trees to Derivations

We shall now show how to construct a leftmost derivation from a parse tree.
The method for constructing a rightmost derivation uses the same ideas, and
we shall not explore the rightmost-derivation case. In order to understand how
derivations may be constructed, we need first to see how one derivation of a
string from a variable can be embedded within another derivation. An example
should illustrate the point.

Example 5.13: Let us again consider the expression grammar of Fig. 5.2. It
is easy to check that there is a derivation

E=>i=>Ib> ab
As a result, for any strings a and J, it is also true that
akbB > al 8 => albB > nabs

The justification is that we can make the same replacements of production
bodies for heads in the context of a and # as we can in isolation.’

For instance, if we have a derivation that begins E> E+E => E + (2),
we could apply the derivation of a) from the second E by treating “E + (” as
a and “)” as 3. This derivation would then continue

E4+(E) > E4+(1) > E+ (ib) > E+ (ab)

We are now able to prove a theorem that lets us convert a parse tree to a
leftrnost derivation. The proof is an induction on the height of the tree, which is
the maximum length of a path that starts at the root, and proceeds downward
through descendants, to a leaf. For instance, the height of the tree in Fig. 5.6 is
7. The longest root-to-leaf path in this tree goes to the leaf labeled b. Note that
path lengths conventionally count the edges, not the nodes, so a path consisting
of a single node is of length 0.

Theorem 5.14: Let G = (V,T, P,S) be a CFG, and suppose there is a parse
tree with root labeled by variable A and with yield w, where w is in T*. Then
there is a leftrnost. derivation A > win grammar G.

PROOF: We perform an induction on the height of the tree.

1Tn fact, it is this property of being able to make a string-for-variable substitution regard-
less of context that gave rise originally to the term “context-free.” ‘here is a more powerful
classes of grammars, called “context-sensitive,” where replacements are permitted only if cer-
tain strings appear to the left and/or right. Context-sensitive grammars do not play a major
role in practice today.


--- Page 204 ---
188 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

BASIS: The basis is height 1, the least that a parse tree with a yield of terminals

can be. In this case, the tree must look like Fig. 5.8. with a root labeled A and

children that read w, left-to-right. Since this tree is a parse tree, A > w must

be a preduction, Thus, A = w is a one-step, leftmost derivation of w from A.
ard

INDUCTION: If the height of the tree is n, where n > 1, it must look like
Fig 5.9. That is, there is a root labeled A, with children labeled X), Xo,...,.X¢
from the left. The X’s may be either terminals or variables.

1. If X; is a terminal, define w; to be the string consisting of X; alone.

2. If A; is a variable, then it must be the root of some subtree with a yield
of terminals, which we shall call «#,;. Note that in this case, the subtree is
of height less than 7, so the inductive hypothesis applies to it. That is,
there is a leftmost derivation A; = Uy.

TEL
Note that w = uywe---imy.
We construct a leftmost. derivation of w as follows. We begin with the step
A= X,X2---X,_. Then, for each i = 1,2,...,%, in order, we show that

in
Ps
A > ww: wy Me Aigo Xp
m7

This proof is actually another induction, this time on i. For the basis, ¢ = 0,
we already know that A => Ay X,---X,. For the induction, assume that
TH

*
A> WW] Ws - wy WE AG ne AR

fon

a) If .X; is a terminal, do nothing. However, we shall subsequently think of
; as the terminal string w;. Thus, we already have

* =
A > Wy We wiA Vega ++ XE
m
b) IfX; is a variable, continue with a derivation of w,; from X,, in the context
of the derivation being constructed. That is, if this derivation is
X;> Oy > Oy SS Wy
fore im fin

we proceed with

UW] Wig Wa ANG Xy >

tr
Wy We Wor Ay XE zm
rH
Wy Uy >> Wy aX yyy + Xp in
TT

Wy we WEN G1 Mayo Ly


--- Page 205 ---
6.2. PARSE TREES 189

The result is a derivation 4 > Wy wes: WA Xk.
1
When i = k, the result is a leftmost derivation of w from 4. 0

Example 5.15: Let us construct the leftmost derivation for the tree of Fig. 5.6.
We shall show only the final step, where we construct the derivation from the
entire tree from derivations that correspond to the subtrees of the root. That is,
we shall assume that by recursive application of the technique in Theorem 5.14,
we have deduced that the subtree rooted at the first child of the root has
leftinost derivation £ => J = a, while the subtree rooted at the third child of

tna

the root has leftmost derivation
E> (f)=> (F+E)=> G+) => (a+ b=
fm. im im im tm

(a+1) > (a+ 10) > (a+ 100) > (a +400)
brik m™m

fone

To build a leftmost derivation for the entire tree, we start with the step at
the root: A => E»* E. Then, we replace the first & according to its deriva-

TH
tion, following each step by *«# to account for the larger context in which that
derivation is used. The leftmost derivation so far is thus
E> ExE=> le Fo atk

im im im

The * in the production used at the root requires no derivation, so the
above leftmost derivation also accounts for the first two children of the root.
We complete the leftmost derivation by using the derivation of E => (a + b00},

in a context where it is preceded by ax and followed by the empty ‘string. This
derivation actually appeared in Example 5.6; it is:
E> Bato Ie E> uxES

im. im fm

* (E) = ar(E+E)= ar(i+B)=> at(a+ FE) =>

int im fen
e(atl)=> ax (a+ 10) > a» (a+ 100) = a * (a + 600)

art)

Oo

A similar theorem lets us convert a tree to a rightmost derivation. The
construction of a rightmost derivation from a tree is almost the same as the
construction of a leftmost derivation. However, after starting with the step
A => X,N>---Xz, we expand Ny first, using a rightmost derivation, then

rim
expand X,-, and so on, down to X1. Thus, we shall state without further

proof:

Theorem 5.16: Let G = (V,T, P,S) be a CFG, and suppose there is a parse
tree with root labeled by variable A and with yield w, where w is in T*. Then
there is a rightmost derivation A > win grammar G. O

rrr


--- Page 206 ---
190 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

5.2.6 From Derrvations to Recursive Inferences

We now complete the loop suggested by Fig. 5.7 by showing that whenever
there is a derivation A > w for some CFG, then the fact that w is in the
language of A is discovered in the recursive inference procedure. Before giving
the theorem and proof, let us observe something important about derivations.

Suppose that. we have a derivation A = X Xo--+X, “+s w. Then we can
break w into pieces w = ww,---w, such that X; + wy. Note that if XY; is
a terminal, then w, = X;, and the derivation is zero steps. The proof of this
observation is not hard. You can show by induction on the number of steps
of the derivation, that if X,X4-+--N; => ¢, then all the positions of a that
come from expansion of X; are to the left of all the positions that come from
expansion of Xj, if i <j.

If X; is a variable, we cau obtain the derivation of X; 5 w; by starting
with the derivation 4 > w, and stripping away:

a) All the positions of the sentential forms that are either to the left or right
of the positions that are derived from X;, and

b) All the steps that are not relevant to the derivation of w; from X;.
An example should make this process clear.

Example 5.17: Using the expression grammar of Fig. 5.2, consider the deriva-
tion
ESEsE>Ex E+E S>Il+E+ERalxI+ b=

Fael+Joaxl4+2}] 5>a%x6+ > atbta

Consider the third sentential form, EF +* +E, and the middle # in this form.?

Starting from & x E+ EB, we inay follow the steps of the above derivation,
but strip away whatever positions are derived from the Ex to the left of the
central E or derived from the + to its right. The steps of the derivation then
become FE, #,1,1,1,6,b. That is, the next step does not change the central E,
the step after that changes it to J, the next two steps leave it as J, the next
changes it to b, and the final step does not change what is derived from the
central F.

If we take only the steps that change what comes from the central Z, the
sequence of strings &,&,f,f,2,b,b becomes the derivation EF + J => 6. That
derivation correctly deseribes how the central E evolves during the complete
derivation. O

Theorem 5.18: Let G = (V,T, P,S) be a CFG, and suppose there is a deriva-
tion A = w, where wis in T*, Then the recursive inference procedure applied

G
to G determines that w is in the language of variable 4.

2Our discussion of finding subderivations from larger derivations assumed we were con-
cerned with a variable in the second sentential form of some derivation. However, the idea
applies 16 a variable in any step of a derivation.


--- Page 207 ---
—

5.3. APPLICATIONS OF CONTEXT-FREE GRAMMARS 191

PROOF: The proof is an induction on the length of the derivation A = w.

BASIS: If the derivation is one-step, then A 4 w must be a production. Since
w consists of terminals only, the fact that w is in the language of A will be
discovered in the basis part of the recursive inference procedure.

INDUCTION: Suppose the derivation takes n + 1 steps, and assume that for
any derivation of n or fewer steps, the statement holds. Write the derivation
as A => XyXe--: Xp 3 w. Then, as discussed prior to the theorem, we can
break was w= Wy We:--w,, where:

a) If X; is a terminal, then w; = X;.

b) If X; is a variable, then X; => w,. Since the first step of the derivation
A> w is surely not part of the derivation X; = w;, we know that this
derivation is of n or fewer steps. Thus, the inductive hypothesis applics
to it, and we know that w, is inferred to be in the language of Xj.

Now, we have a production 4 4 X)X2---X», with w; either equal to X; or
known to be in the language of X;. In the next round of the recursive inference
procedure, we shall discover that ww ++: wy is m the language of A. Since
wis +++ w, = w, we have shown that w is inferred to be in the language of A.
oO

5.2.7 Exercises for Section 5.2

Exercise 5.2.1: For the grammar and each of the strings in Exercise 3.1.2,
give parse trees.

Exercise 5.2.2: Suppose that G is a CFG without any productions that have
eas the right side. If w is in L(G), the length of w is n, and w has a derivation
of m steps, show that w has a parse tree with n +m nodes.

Exercise 5.2.3: Suppose all is as in Exercise 5.2.2, but G may have some
productions with € as the right side. Show that a parse tree for a string w other
than e may have as many as 7 + 2m — 1 nodes, but no more.

Exercise 5.2.4: In Section 5.2.6 we mentioned that if X,X2---Xx = a, then
all the positions of a that come from expansion of .X; are to the left. of all the
positions that come from expansion of Xj, if i < j. Prove this fact. Hint:
Perform an induction on the number of steps in the derivation.

5.3 Applications of Context-Free Grammars

Context-free grammars were originally conceived by N. Chomsky as a way to
describe natural languages. That promise has not been fulfilled. However, as
uses for recursively defined concepts in Computer Science have multiplied, so
has the need for CFG’s as a way to describe instances of these concepts. We
shall sketch two of these uses, one old and one new.


--- Page 208 ---
192 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

1. Grammars are used to describe programming languages. More impor-
tantly, there is a mechanical way of turning the language description as
a CFG into a parser, the component of the compiler that discovers the
structure of the source program and represents that structure by a parse
tree. This application is one of the earliest uses of CFG’s; in fact it is
one of the first ways in which theoretical ideas in Computer Science found
their way into practice.

2. The development of XML (Extensible Markup Language) is widely pre-
dicted to facilitate electronic commerce by allowing participants to share
conventions regarding the format of orders, product descriptions, and
many other kinds of documents. An essential part of XML is the Docu-
ment Type Definition (DTD), which is essentially a context-free grammar
that describes the allowable tags and the ways in which these tags may
be nested. Tags are the familiar keywords with triangular brackets that
you may know from HTML, e¢.g., <EM> and </EM> to surround text that
necds to be emphasized. However, XML tags deal not with the formatting
of text, but with the meaning of text. For instance, one could surround
a sequence of characters that. was intended to be interpreted as a phone
number by <PHONE> and </PHONE>.

5.3.1 Parsers

Many aspects of a programming language have a structure that may be de-
scribed by regular expressions. For instance, we discussed in Example 3.9 how
identifiers could be represented by regular expressions. However, there are also
some very important aspects of typical programming languages that cannot be
represented by regular expressions alone. The following are two examples.

Example 5.19: Typical languages use parentheses and/or brackets in a nested
and balanced fashion. That is, we must be able to match some left. parenthesis
against a right parenthesis that appears immediately to its right, remove both
of them, and repeat. If we eventually eliminate all the parentheses, then the
string was balanced, and if we cannot match parentheses in this way, then it is
unbalanced. Examples of strings of balanced parentheses are (()), OQ, (00),
and ¢, while )( and (() are not.

A grammar Goa = ({B},{(,)},2,8) generates ali and only the strings of
balanced parentheses, where P consists of the productions:

B— BB | (B)\«

The first production, B ~ BB, says that the concatenation of two strings of
balanced parentheses is balanced. That assertion makes sense, because we can
match the parentheses in the two strings independently. The second production,
B— (B), says that. if we place a pair of parentheses around a balanced string,
then the result is balanced. Again, this rule makes sense, because if we match


--- Page 209 ---
5.3. APPLICATIONS OF CONTEXT-FREE GRAMMARS 193

the parentheses in the inner string, then they are all eliminated and we are then
allowed to match the first and last parentheses, which have become adjacent.
The third production, B — € is the basis; it says that the empty string is
balanced.

The above informal arguments should convince us that Gag generates all
strings of balanced parentheses. We need a proof of the converse — that every
string of balanced parentheses is generated by this grammar. However, a proof
by induction on the length of the balanced string is not hard and is left as an
exercise.

We mentioned that the set of strings of balanced parentheses is not a regular
language, and we shall now prove that fact. If L(Gyat) were regular, then there
would be a constant n for this language from the pumping lemma for regular
languages. Consider the balanced string w = (")", that is, n left. parentheses
followed by n matching right parentheses. If we break w = cyz according to
the pumping lemma, then y consists of only left parentheses, and therefore rz
has more right parentheses than left. This string is not balanced, contradicting
the assumption that the language of balanced parentheses is regular. O

Programming languages consist of more than parentheses, of course, but
parentheses are an essential part of arithmetic or conditional expressions. The
grammar of Fig. 5.2 is more typical of the structure of arithmetic expressions,
although we used only two operators, plus and times, and we included the de-
tailed structure of identifiers, which would more likely be handled by the lexical-
analyzer portion of the compiler, as we mentioned in Section 3.3.2. However,
the language described in Fig. 5.2 is not regular either. For instance, according
to this gramunar, ("a)” is a legal expression. We can use the pumping lemma
to show that if the language were regular, then a string with some of the left
parentheses removed and the a and all right parentheses intact would also be a
legal expression, which it is not.

There are numerous aspects of a typical programming language that behave
like balanced parentheses. There will usually be parentheses themselves, in
expressions of all types. Beginnings and endings of code blocks, such as begin
and end in Pascal, or the curly braces {...} of C, are examples. That is,
whatever curly braces appear in a C program must form a balanced sequence,
with { in place of the left parenthesis and } in place of the right parenthesis.

There is a related pattern that appears occasionally, where “parentheses”
can be balanced with the exception that there can be unbalanced left parenthe-
ses. An example is the treatment of if and else in C. An if-clause can appear
unbalanced by any else-clause, or it may be balanced by a matching else-clause.
A grammar that generates the possible sequences of if and else (represented
by z and e, respectively) is:

Se|SS|iS |iSeS
For instance, ieie, die, and tei are possible sequences of if's and else’s, and

each of these strings is generated by the above grammar. Some examples of
illegal sequences, not generated by the grammar, are €7 and ieeii.


--- Page 210 ---
194 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

A simple test (whose correctness we leave as an exercise), for whether a
sequence of z’s and e’s is generated by the grammar is to consider each e, in
turn from the left. Look for the first ¢ to the left of the e being considered. If
there is none, the string fails the test and is not in the language. If there is such
an 7, delete this i and the e being considered. Then, if there are no more e’s
the string passes the test and is in the language. If there are more e's, proceed
to consider the next one.

Example 5.20: Consider the string iee. The first e is rnatched with the 7 to
its left. They are removed, leaving the string e. Since there are more e’s we
consider the next. However, there is no i to its left, so the test fails: tee is not
in the language. Note that this conclusion is valid, since you cannot have more
else’s than if’s in a C program.

For another example, consider tieie. Matching the first. e with the i to its
left leaves fe. Matching the remaining e with the 7 to its left leaves 7. Now
there are no more e’s, so the test. succeeds. This conclusion also makes sense,
because the sequence iieie corresponds to a C program whose structure is like
that of Fig. 5.10. In fact, the matching algorithm also tells us (and the C
compiler) which if matches any given else. That knowledge is essential if the
compiler is to create the control-flow logic intended by the programmer. O

if (Condition) {

if (Condition) Statement;
else Statement;

if (Condition) Statement;
else Statement;

Figure 5.10: An if-else structure; the two else’s match their previous if's, and
the first if is unmatched

5.3.2 The YACC Parser-Generator

The generation of a parser (function that creates parse trees from source pro-
grams) has been institutionalized in the YACC command that appears in all
UNIX systems. The input to YACC is a CFG, in a notation that differs only
in details from the one we have used here. Associated with cach production is
an action, which is a fragment of C code that is performed whenever a node of
the parse tree that (with its children) corresponds to this production is created.
Typically, the action is code to construct that node, although in some YACC


--- Page 211 ---
§.3. APPLICATIONS OF CONTEXT-FREE GRAMMARS 195

applications the tree is not actually constructed, and the action does something
else, such as emit a piece of object code.

Example 5.21: In Fig. 5.11 is a sample of a CFG in the YACC notation.
The grammar is the same as that of Fig. 5.2. We have elided the actions, just
showing their (required) curly braces and their position in the YACC input.

Exp : Id {...}
| Exp 7+? Exp {...}
| Exp ?*? Exp {...}
| 7(? Exp 2)’ {...}

Id : 7a? {...}
| ?b? {...}
| Id ’a? {...}
| Id ?b? {..-}
| Id 70? {...}
{ Id ?1° {...}

Figure 5.11: An example of a grammar in the YACC notation

Notice the following correspondences between the YACC notation for gram-
mars and ours:

The colon is used as the production symbol, our +.

All the productions with a given head are grouped together, and their
bodies are separated by the vertical bar. We also allow this convention,
as an option.

The list of bodies for a given head ends with a semicolon. We have not
used a terminating symbol.

Terminals are quoted with single quotes. Several characters can appear
within a single pair of quotes. Although we have not shown it, YACC al-
lows its user to define symbolic terminals as well. The occurrence of these
terminals in the source program are detected by the lexical analyzer and
signaled, through the returu-value of the lexical analyzer, to the parscr.

Unquoted strings of letters and digits are variable names. We have taken
advantage of this capability to give our two variables more descriptive
names — Exp and Id — although E and I could have been used.


--- Page 212 ---
196 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

5.3.3 Markup Languages

We shall next consider a family of “languages” called markup languages. The
“strings” in these languages are documents with certain marks (called ¢ags) in
them. Tags tell us something about the semantics of various strings within the
document.

The markup language with which you are probably most familiar is HTML
(HyperText. Markup Language). This language has two tuajor functions: cre-
ating links between documents and describing the format (“look”) of the a
document. We shall offer only a simplified view of the structure of HTML,
but the following examples should suggest both its structure and how a CFG
could be used both to describe the legal HTML documents and to guide the
processing (i.¢., the display on a monitor or printer) of a document.

Example 5.22: Figure 5.12(a) shows a piece of text, comprising a list of items,
and Fig. 5.12(b) shows its expression in HTML. Notice from Fig. 5.12(b) that
HTML consists of ordinary text interspersed with tags. Matching tags are of
the form <> and </x> for some string z.? For instance, we see the matching
tags <EM> and </EM>, which indicate that the text between them should be
emphasized, that is, put in italics or another appropriate font. We also see the
matching tags <OL> and </QL>, indicating an ordered list, i.e., an cuumeration
of list items.

The things I hate:
1. Moldy bread.
2. People who drive too slow in the fast lane.
(a) The text as viewed

<P>The things I <EM>hate</EM>:
<OL>

<LI>Moldy bread.

<LI>People who drive too slow
in the fast lane.

</OL>

(b) The HTML source
Figure 5.12: An HTML document and its printed version

We also see two examples of unmatched tags: <P> and <LI>, which introduce
paragraphs and list items, respectively. HTML allows, indeed encourages, that

Ssometimes the Introducing tag <z> has more injormation int Ynan just the ame & fot
the tag. However. we shall not consider that possibility in examples.


--- Page 213 ---
5.3. APPLICATIONS OF CONTEXT-FREE GRAMMARS 197

these tags be matched by </P> and </LI> at the ends of paragraphs and list
items, but it does not require the matching. We have therefore left the matching
tags off, to provide some complexity to the sample HTML grammar we shall
develop. O

There are a number of classes of strings that are associated with an HTML
document. We shall not try to list them all, but here are the ones essential to
the understanding of text like that of Example 5.22. For each class, we shall
introduce a variable with a descriptive name.

1. Test is any string of characters that can be literally interpreted; ie., it
has no tags. An example of a Text element in Fig 5.12(a) is “Moldy
bread.”

2. Char is any string consisting of a single character that is legal in HTML
text. Note that blanks are included as characters.

3. Doe represents documents, which are sequences of “elements.” We define
elements next, and that definition is mutually recursive with the definition
of a Doc.

4. Element is either a Text string, or a pair of matching tags and the doc-
ument between them, or an unmatched tag followed by a document.

5. ListItem is the <LI> tag followed by a document, which is a single list
item.

6. List is a sequence of zero or more list items.

1. Char > a|lAl--
2. Text 4 e| Char Text
3. Doe + | Element Doc

4, Element + Test |
<EM> Doc </EM> |
<P> Doc |
<OL> List </OL> | ---
5. Listitem —- <LI> Doc

6. List + ¢| Listitem List

Figure 5.13: Part of an HTML grammar


--- Page 214 ---
198 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

Figure 5.13 is a CFG that describes as much of the structure of the HTML
language as we have covered. In line (1) it is suggested that a character can
be “a” or “A” or many other possible characters that are part of the HTML
character set. Line (2) says, using two productions, that Text can be either the
empty string. or any legal character followed by more text. Put another way,
Teat is zero or more characters. Note that < and > are not legal characters,
although they can be represented by the sequences &1t; and &gt;, respectively.
Thus, we cannot accidentally get a tag into Tezt.

Line (3) says that a document is a sequence of zero or more “elements.” An
clement in turn, we learn at line (4), is either text, an emphasized dacument, a
paragraph-beginning followed by a document, or a list. We have also suggested
that there are other productions for Element, corresponding to the other kinds
of tags that appear in HTML. Then, iu line (5) we find that a list item is the
<LI> tag followed by any document, and line (6) tells us that a list is a sequence
of zero or more list elements.

Some aspects of HTML do not require the power of context-free grammars;
regular expressions are adequate. For example, lines (1) and (2) of Fig. 5.13
simply say that Teat represents the same language as does the regular expres-
sion (a + A+ ---)*. However, some aspects of HTML do require the power
of CFG's. For instance, each pair of tags that are a corresponding beginning
and ending pair, e.g., <EM> and </EM>, are like balanced parentheses, which we
already know are not regular.

5.3.4 XML and Document-Type Definitions

The fact that HTML is described by a grammar is not in itself remarkable.
Essentially all programming languages can be described by their own CFG’s,
so it would be more surprising if we could not so describe HTML. However,
when we look at another important markup language, XML (eXtensible Markup
Language), we find that the CFG’s play a more vital role, as part of the process
of using that language.

The purpose of XML is not to describe the formatting of the document;
that is the jab for HTML. Rather, XML tries to describe the “semantics” of
the text. For example, text like “12 Maple St.” looks like an address, but is
it? In XML, tags would surround a phrase that represented an address; for
example:

<ADDR>12 Maple St.</ADDR>

However, it is not. immediately obvious that <ADDR> means the address of a
building. For instance, if the document were about memory allocation, we might
expect that the <ADDR> tag would refer to a memory address. To make clear
what the different kinds of tags are, and what structures may appear between
matching pairs of these tags, people with a common interest are expected to
develop standards in the form of a DTD (Document-Type Definition).


--- Page 215 ---
5.3. APPLICATIONS OF CONTEXT-FREE GRAMMARS 199

A DTD is essentially a context-free grammar, with its own notation for
describing the variables and productions. In the next example, we shall show
@ simple DTD and introduce some of the language used for describing DTD’s,
The DTD language itself has a context-free grammar, but it is not that grammar
we are interested in describing. Rather, the language for describing DTD’s is
essentially a CFG notation, and we want to see how CFG’s are expressed in
this language.

The form of a DTD is

<!DOCTYPE name-of-DTD [
list of element definitions

]>
An element definition, in turn, has the form
<!ELEMENT element-name (description of the element}>

Element descriptions are essentially regular expressious. The basis of these
expressions are:

1. Other element names, representing the fact that elements of one type can
appear within elements of another type, just as in HTML we might find
emphasized text. within a list.

2. The special term #PCDATA, standing for any text that does not involve
XML tags. This term plays the role of variable Text in Example 5.22.

The allowed operators are:

1. | standing for union, as in the UNIX regular-expression notation discussed
in Section 3.3.1.

2. A comma, denoting concatenation.

3. Three variants of the closure operator, as in Section 3.3.1. These are *,
the usual operator meaning “zero or more occurrences of,” +, meaning
“one or more occurrences of,” and ?, meaning “zero or one occurrence
of.”

Parentheses may group operators to their arguments; otherwise, the usual prece-
dence of regular-expression operators applies.

Example 5.23: Let us imagine that computer vendors get together to create
a standard DTD that they can use to publish, on the Web, descriptions of the
various PC’s that they currently sell. Each description of a PC will have a
model number, and details about the features of the model, e.g., the amount of
RAM, number and size of disks, and so on. Figure 5.14 shows a hypothetical,
yery simple, DTD for personal computers.


--- Page 216 ---
200 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

<!DOCTYPE PeSpecs [
<!ELEMENT PCS (PC#)>
<!ELEMENT PC (MODEL, PRICE, PROCESSOR, RAM, DISK+)>
<!ELEMENT MODEL (\#PCDATA)>
<!ELEMENT PRICE (\#PCDATA)>
<!ELEMENT PROCESSOR (MANF, MODEL, SPEED)>
<!ELEMENT MANF (\#PCDATA)>
<!ELEMENT MODEL (\#PCDATA)}>
<!ELEMENT SPEED (\#PCDATA)>
<!ELEMENT RAM C\#PCDATA)>
<!ELEMENT DISK (HARDDISK | CD | DVD)>
<!ELEMENT HARDDISK (MANF, MODEL, SIZE)
<!ELEMENT SIZE (\#PCDATA)>
<!ELEMENT CD (SPEED)>
<!ELEMENT DVD (SPEED)>
]>

Figure 5.14: A DTD for personal computers

The name of the DTD is PcSpecs. The first element, which is like the start
symbol of a CFG, is PCS (list of PC specifications). Its definition, PC*, says
that a PCS is zero or more PC entries.

We then see the definition of a PC element. It consists of the concatenation
of five things. The first. four are other clements, corresponding to the model,
price, processor type, and RAM of the PC. Each of these must appear once,
in that order, since the comma represents concatenation. The last constituent,
DISK-+, tells us that there will be one or more disk entries for a PC.

Many of the constituents are simply text; MODEL, PRICE, and RAM are of this
type. However, PROCESSOR has more structure. We see from its definition that
it consists of a manufacturer, model, and speed, in that order; each of these
elements is simple text.

A DISK entry is the most complex. First, a disk is either a hard disk, CD, or
DVD, as indicated by the rule for element DISK, which is the OR of three other
élements. Hard disks, in turu, have a structure in which the manufacturer,
model, and size are specified, while CD’s and DVD’s are represented only by
their speed.

Figure 5.15 is an example of an XML document that conforms to the DTD
of Fig. 5.14. Notice that each element is represented in the document by a tag
with the name of that element and a matching tag at the end, with an extra
slash, just as in HTML. Thus, in Fig. 5.15 we see at the outermost level the tag
<PCS>...</PCS>. Inside these tags appears a list of entries, one for each PC
sold by this manufacturer; we have only shown one such entry explicitly.

Within the illustrated <PC> entry, we can easily see that the model number


--- Page 217 ---
5.3.

APPLICATIONS OF CONTEXT-FREE GRAMMARS

<PCS>
<PC>
<MODEL>4560< /MODEL>
<PRICE>$2295</PRICE>
<PROCESSOR>
<MANF>Intel</MANF>
<KODEL>Pentium</MODEL>
<SPEED>800HHz</SPEED>
</PROCESSOR>
<RAM>256</RAM>
<DISK><HARDDISK>
<MANF>Maxtor</MANF>
<MODEL>Diamond</MODEL>
<SIZE>30 . 5Gb</SIZE>
</HARDDISK></DISK>
<DISK><CD>
<SPEED>32x</SPEED>
</CD></DISK>
</PC>
<PC>

</PC>
</PCS>

201

Figure 5.15: Part of a document obeying the structure of the DTD in Fig. 5.14

is 4560, the price is $2295, and it has an 800MH~a Intel Pentium processor. It
has 256Mb of RAM, a 30.5Gb Maxtor Diamond hard disk, and a 32x CD-ROM
reader. What is important is not that we can read these facts, but that a
program could read the document, and guided by the grammar in the DTD
of Fig. 5.14 that it has also read, could interpret the numbers and names in
Fig. 5.15 properly. O

You may have noticed that the rules for the elements in DTD’s like Fig. 5.14

are not quite like productions of context-free grammars. Many of the rules are
of the correct form. For instance,

<!ELEMENT PROCESSOR (MANF, MODEL, SPEED)>

Is analogous to the production

Processor -> Manf Model Speed

However, the rule


--- Page 218 ---
202 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

<!ELEMENT DISK (HARDDISK | CD | DVD)>

does not have a definition for DISK that is like a production body. In this case,
the extension is simple: we may interpret this rule as three productions, with
the vertical bar playing the same role as it does in our shorthand for productions
having a common head. Thus, this rule is equivalent to the three productions

Disk > HardDisk | Cd | Dud
The most. difficult. case is
<!ELEMENT PC (MODEL, PRICE, PROCESSOR, RAM, DISK+)>

where the “body” has a closure operator within it. The solution is to replace
DISK+ by a new variable, say Disks, that generates, via a pair of productions,
one or more instances of the variable Disk. The equivalent productions are
thus:

Pe > Model Price Processor Ram Disks
Disks 3 Disk | Disk Disks

There is a general technique for converting a CFG with regular expressions
as production bodies to an ordinary CFG. We shall give the idea informally;
you may wish to formalize both the meaning of CFG’s with regular-expression
productions and a proof that the extension yields no new languages beyond
the CFL’s. We show, inductively, how to convert a production with a regular-
expression bady to a collection of equivalent ordinary productions. The induc-
tion is on the size of the expression in the body.

BASIS: If the body is the concatenation of elements, then the production is
already in the legal form for CFG’s, so we do nothing.

INDUCTION: Otherwise, there are five cases, depending on the final operator
used,

1. The production is of the form A — E), £2, where £, and EF» are expres-
sions permitted in the DTD language. This is the concatenation case.
Introduce two new variables, B and C, that appear nowhere else in the
grammar. Replace A + &,, & by the productions

A> BC
Bok,
C3 Bs

The first production, A ~ BC, is legal for CFG’s. The last two may or
may not be legal. However, their bodies are shorter than the body of the
original production, so we may inductively convert them to CFG form.


--- Page 219 ---
5.38. APPLICATIONS OF CONTEXT-FREE GRAMMARS 203

2. The production is of the form A —> &, | By. For this union operator,
replace this production by the pair of productions:

Aw Fy

Again, these productions may or may not be legal CFG productions, but
their bodies are shorter than the body of the original. We may therefore
apply the rules recursively and eventually convert these new productions
to CFG form.

3. The production is of the form A > (F,)*. Introduce a new variable B
that appears nowhere else, and replace this production by:

AOBA
Ate
Bo

4, The production is of the form A > (£,)*. Introduce a new variable B
that appears nowhere else, and replace this production by:

A~ BA
AoB
Bok

5. The production is of the form A > (£,)?. Replace this production by:

Ave
Av BE,

Example 5.24: Let us consider how to convert the DTD rule
<!ELEMENT PC (MODEL, PRICE, PROCESSOR, RAM, DISK+)}>

to legal CFG productions. First, we can view the body of this rule as the con-
catenation of two expressions, the first of which is MODEL, PRICE, PROCESSOR,
RAM and the second of which is DISK+. If we create variables for these two
subexpressions, say A and B, respectively, then we can use the productions:

Pe AB
A -+ Model Price Processor Ram
Bo Diskt

Only the last of these is not in legal form. We introduce another variable C’
and the productions:


--- Page 220 ---
—_

204 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

BoCB|C
C3 Disk

In this special case, because the expression that A derives is just a concate-
nation of variables, and Disk is a single variable, we actually have no need for
the variables A or C’. We could use the following productions instead:

Pe Model Price Processor Ram B
B- Disk B | Disk

Oo

5.3.5 Exercises for Section 5.3

Exercise 5.3.1: Prove that if a string of parentheses is balanced, in the sense
given in Example 5.19, then it is generated by the grammar B > BB | (B) |e.
Aint: Perform an induction on the length of the string.

Exercise 5.3.2: Consider the set. of all strings of balanced parentheses of two
types, round and square. An example of where these strings come from is as
follows. If we take expressions in C, which use round parentheses for grouping
aul for arguineuts of function calls, and use square brackets for array indexes,
and drop out everything but the parentheses, we get ail strings of balanced
parentheses of these two types. For example,

f(alil*@Li) [jl],<ig()]), ali)

becomes the balanced-parenthesis string (17 (C7 (O]) 01>. Design a gram-
inar for all and only the strings of round and square parentheses that are bal-
anced.

Exercise 5.3.3: In Section 5.3.1, we considered the grammar
Sre|S5|i5 | iSes

and claimed that we could test for membership in its language L by repeatedly
doing the following, starting with a string w. The string w changes during
repetitions.

1. If the current string begins with e, fail; wis not in LZ.
2. Lf the string currently has uo e’s (it may have 7's), succeed; w is in L.

3. Otherwise, delete the first e and the i immediately to its left. Then repeat
these three steps on the new string.

Prove that this process correctly identifies the strings in LZ.

exercise 5.3.4: Add the following forms to the HTML grammar of Fig. 5.13:


--- Page 221 ---
5.4. AMBIGUITY IN GRAMMARS AND LANGUAGES 205

* a) A list item must be ended by a closing tag </LI>.

b) An element can be an unordered list, as well as an ordered list. Unordered
lists are surrounded by the tag <UL> and its closing </UL>.

1c) An element can be a table. Tables are surrounded by <TABLE> and its
closer </TABLE>. Inside these tags are one or more rows, each of which
is surrounded by <TR> and </TR>. The first row is the header, with one
or more fields, each introduced by the <TH> tag (we'll assume these are
not closed, although they should be). Subsequent rows have their flelds
introduced by the <TD> tag.

<!IDOCTYPE CourseSpecs [
<!ELEMENT COURSES (COURSE+) >
<!ELEMENT COURSE (CNAME, PROF, STUDENT*, TA?)>
<!ELEMENT CNAME (#PCDATA)>
<!ELEMENT PROF (#PCDATA}>
<!ELEMENT STUDENT (#PCDATA) >
<!ELEMENT TA (#PCDATA)> ]>

Figure 5.16: A DTD for courses
Exercise 5.3.5: Convert the DTD of Fig. 5.16 to a context-free graminar.

5.4 Ambiguity in Grammars and Languages

As we have seen, applications of CFG’s often rely on the gramunar to provide
the structure of files. For instance, we saw in Section 5.3 how grammars can be
used to put structure on programs and documents. The tacit assumption was
that a grammar uniquely determines a structure for each string in its language.
However, we shall see that not every grammar does provide unique structures.

When a grammar fails to provide unique structures, it is sometimes possible
to redesign the grammar to make the structure unique for each string in the
language. Unfortunately, sometimes we cannot do so. That is, there are some
CFL’s that are “inherently ambiguous”; every grammar for the language puts
more than one structure on some strings in the language.

5.4.1 Ambiguous Grammars

Let us return to our running example: the expression grammar of Fig. 5.2. This
grammar lets us generate expressions with any sequence of * and + operators,
and the productions & + E+E | £* £ allow us to generate these expressions
in any order we choose,


--- Page 222 ---
206 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

Example 5.25: For instance, consider the sentential form # + E* &. It has
two derivations from &:

LBP E+EsS B+E«eE
2Bbs>Bht+tho RL Exe

Notice that in derivation (1}, the second E is replaced by E « E, while in
derivation (2), the first E is replaced by H+ EH. Figure 5.17 shows the two
parse trees, which we should note are distinct trees.

aN aN
AN AN

E * E

(a) (b)
Figure 5.17: Two parse trees with the same yield

The difference between these two derivations is significant. As far as the
structure of the expressions is concerned, derivation (1) says that the second and
third expressions are multiplied, and the result is added to the first expression,
while derivation (2) adds the first two expressions and multiplies the result. by
the third. In more concrete terms, the first derivation suggests that 1+ 2 +3
should be grouped 1 + (2 * 3) = 7, while the second derivation suggests the
same expression should be grouped (1 + 2} *3 = 9. Obviously, the first of
these, and not the second, matches our notion of correct grouping of arithmetic
expressions.

Since the grammar of Fig. 5.2 gives two different structures to any string
of terminals that is derived by replacing the three expressions in E + E + E by
identifiers, we see that this grammar is not a good one for providing unique
structure. In particular, while it can give strings the correct grouping as arith-
metic expressions, it also gives them incorrect groupings. To use this expression
grammar in a compiler, we would have to modify it to provide only the correct
groupings. O

On the other hand, the mere existence of different derivations for a string
(as opposed to different parse trees) does not imply a defect in the grammar.
The following is an example.

Example 5.26: Using the same expression grammar, we find that the string
a+ 6 has many different derivations. Two examples are:


--- Page 223 ---
5.4, AMBIGUITY IN GRAMMARS AND LANGUAGES 207

1ESE+EeI+EomatEsaatisaatsb
9 FoE+ESOE+I oI +i si+ba>ath

However, there is no real difference between the structures provided by these
derivations; they each say that a and 8 are identifiers, and that their values are
to be added. In fact, both of these derivations produce the same parse tree if
the construction of Theorems 5.18 and 5.12 are applied. 0

The two examples above suggest that it is not a multiplicity of derivations
that cause ambiguity, but rather the existence of two or more parse trees. Thus,
we say a CFG G = (V,T,P,S) is ambiguous if there 18 at least one string w
in T* for which we can find two different parse trees, each with root labeled $
and yield w. If each string has at most one parse tree in the grammar, then the
grammar is unambiguous.

For instance, Example 5.25 almost demonstrated the ambiguity of the gram-
mar of Fig. 5.2. We have only to show that the trees of Fig. 5.17 can be com-
pleted to have terminal yields. Figure 5.18 is an example of that, completion.

aN va

PAIN aN
| i | ||

| | | |

a a a a

(a) (b)

Figure 5.18: Trees with yield a +a *a, demonstrating the ainbignity of our
expression granumar

5.4.2 Removing Ambiguity From Grammars

Tn an ideal world, we would be able to give you an algorithm to remove ambi-
guity from CFG’s, much as we were able to show an algorithm in Section 4.4 to
remove unnecessary states of a finite antomaton. However, the surprising fact
is, as we shall show in Section 9.5.2, that there is no algorithm whatsoever that
can even tell us whether a CFG is ambiguous in the first place. Moreover, we


--- Page 224 ---
208 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

Ambiguity Resolution in YACC

If the expression gramunar we have been using is ambignous. we might
wonder whether the sample YACC program of Fig. 5.11 is realistic. True,
the underlying grammar is ambiguous, but much of the power of the YACC
parser-generator comes from providing the user with simple mechanisms
for resolying most of the common causes of ambiguity. For the expression
grammar, it is sufficient to insist that:

a) * takes precedence over +. That is, *'’s must be grouped before
adjacent +’s on either side. This rule tells us to use derivation (1)
in Example 5.25, rather than derivation (2).

b) Both « and + are left-associative. That is, group sequences of ex-
pressions, all of which are connected by *, from the left, and do the
same for sequences connected by +.

YACC allows us to state the precedence of operators by listing them
in order, from lowest. to highest precedence. Technically, the precedence
of an operator applies to the use of any production of which that operator
is the rightmost terminal in the body. We can also declare operators to
be left- or right-associative with the keywords {left and %right. For
instance, to declare that + and * were both ieft associative, with * taking
precedente over +, we would put ahead of the grammar of Fig. 5.11 the
statements:

‘left 3+?
“%left ?*?

shall see in Section 5.4.4 that there are context-free languages that have nothing
but ambiguous CFG’s; for these languages, removal of ambiguity is impossible.

Fortunately, the situation in practice is not. so grim. For the sorts of con-
structs that appear in common programming languages, there are well-known
techniques for eliminating ambiguity. The problem with the expression gram-
mar of Fig. 5.2 is typical, and we shall explore the elimination of its ambiguity
as an important illustration.

First, let us note that there are two causes of ambiguity in the grammar of
Fig. 5.2:

1. The precedence of operators is not respected. While Fig. 5.17(a) properly
groups the * before the + operator, Fig 5.17(b} is also a valid parse tree
and groups the + ahead of the *. We need to force only the structure of
Fig. 5.17(a} to be legal in an unambiguous grammar.


--- Page 225 ---
5.4. AMBIGUITY IN GRAMMARS AND LANGUAGES 209

2. A sequence of identical operators can group either from the left. or from the
right. For example, if the *’s in Fig. 5.17 were replaced by +’s, we would
see two different parse trees for the string B+ B+ £&. Since addition
and multiplication are associative, it doesn’t matter whether we group
from the left or the right, but to eliminate ambiguity, we must. pick one.
The conventional approach is to insist on grouping from the left, so the
structure of Fig. 5.17(b) is the only correct. grouping of two +-signs.

The solution to the problem of enforcing precedence is to introduce several
different variables, each of which represents those expressions that share a level
of “binding strength.” Specifically:

1. A factor is an expression that cannot be broken apart by any adjacent
operator, either a * or a +. The only factors in our expression language
are:

(a) Identifiers. It is not possible to separate the letters of an identifier
by attaching an operator.

(b} Any parenthesized expression, no matter what appears inside the
parentheses. It is the purpose of parentheses to prevent what is inside
from becoming the operand of any operator outside the parentheses.

2. A term is an expression that cannot be broken by the + operator. In our
example, where + and * are the only operators, a term is a product of
one or more factors. For instance, the term a «6 can be “broken” if we
use left associativity and place al* to its left. That is, al xa*b is grouped
(al « a) * 5, which breaks apart the a * 6. However, placing an additive
term, such as al+, to its left or tal to its right cannot break a*6. The
proper grouping of al +a*bis al + (a6), and the proper grouping of
axb+al is (a*6)+al.

3. An expression will henceforth refer to any possible expression, including
those that can be broken by either an adjacent + or an adjacent +. Thus,
an expression for our example is a suim of one or more terms.

I + a@|fb|ia| | 70; 7
F > I|(&)
T + F|T4«F
Eos T|£+T

Figure 5.19: An unambiguous expression grammar

Example 5.27: Figure 5.19 shows an unambiguous grammar that generates
the same language as the grammar of Fig. 5.2. Think of F, T', and & as the


--- Page 226 ---
210

CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

variables whose languages are the factors, terms, and expressions, as defined
above. For instance, this grammar allows only one parse tree for the string
a+axq; it is shown in Fig. 5.20.

| _

E

T T * F
| | |

F F i
| | |

fi fi a
||

a a

Figure 5.20: The sole parse tree fora +a*a

The fact that this grammar is unambiguous may be far from obvious. Here
are the key observations that explain why no string in the language can have
two different parse trees.

Any string derived from 7, a term, must be a sequence of one or more
factors, connected by #’s. A factor, as we have defined it, and as follows
from the productions for F in Fig. 5.19, is either a single identifier or any
parenthesized expression.

Because of the form of the two productions for T, the only parse tree for
a sequence of factors is the one that breaks f\ * fo+#---*f,,, for n > 1 into
aterm f; * fo*---* f,-1 and a factor f,. The reason is that F cannot
derive expressions like f,-; * f, without introducing parentheses around
them. Thus, it is not possible that when using the production T 4 T'#F,
the F derives anything but the last of the factors. That is, the parse tree
for a term can only look like Fig. 5.21.

Likewise, an expression is a sequence of terms connected by +. When
we use the production & —~ E+ T to derive ty + t2 +---+ ty, the T
must. derive only t,, and the & in the body derives t; + t2 +---+ty-1.
The reason, again, is that T cannot derive the sum of two or more terms
without putting parentheses around them.


--- Page 227 ---
5.4. AMBIGUITY IN GRAMMARS AND LANGUAGES 211

aN,
aN.

ao

* F

~~ —

Figure 5.21: The form of all parse trees for a term

5.4.3 Leftmost Derivations as a Way to Express
Ambiguity

While derivations are not necessarily unique, even if the grammar is unambigu-
ous, it turns out that, in an unambiguous grammar, leftmost. derivations will
be unique, and rightmost derivations will be unique. We shall consider leftmost
derivations only, and state the result for rightmost derivations.

Example 5.28: As an example, notice the two parse trees of Fig. 5.18 that
each yield E+ E+E. If we construct leftmost derivations from them we get
the following leftmost. derivations from trees (a) and (b), respectively:

a) E> E+E = [+ES a+E= a+E+E> atTeh = ataxE >
m™m
atarl= ataka.

b} B= Bsb= E+E«E= I+E+E= at+BeE= a+i*E=>
atasE> tas] > atara

Note that these two leftmost derivations differ. This example does not prove
the theorem, but demonstrates how the differences in the trees force different
steps to be taken in the leftmost derivation. O

Theorem §.29; For each grammar G = (V,T, P, 3) and string w in T*, w has
two distinct parse trees if and only if w has two distinct leftmost derivations
from S.


--- Page 228 ---
212 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

PROOF: (Only-if) If we examine the construction of a leftmost derivation from
a parse tree in the proof of Theorem 5.14, we see that wherever the two parse
trees first have a node at which different productions are used, the leftmost
derivations constructed will also use different productions and thus be different
derivations.

(If) While we have not previously given a direct construction of a parse tree
from a leftmost derivation, the idea is not hard. Start constructing a tree with
only the root, labeled S. Examine the derivation one step at a time. At each
step, a variable will be replaced, and this variable will correspond to the leftmost
node in the tree being constructed that has no children but that has a variable
as its label. From the production used at this step of the leftmost derivation,
determine what the children of this node should be. If there are two distinct
derivatious, then at the first step where the derivations differ, the nodes being
constructed will get different lists of children, and this difference guarantees
that the parse trees are distinct. O

5.4.4 Inherent Ambiguity

A context-free language £ is said to be inherently ambiguous if all its gram-
mars are ambiguous. If even one grammar for £ is unambiguous, then ZL is an
unambiguous language. We saw, for example, that the language of expressions
generated by the grammar of Fig. 5.2 is actually unambiguous. Even though
that. grammar is ambiguous, there is another grammar for the same language
that is unambiguous — the grammar of Fig. 5.19.

We shall not prove that there are inherently ambiguous languages. Rather
we shall discuss one example of a language that can be proved inherently am-
biguous, and we shall explain intuitively why every grammar for the language
must be ambiguous. The language £ in question is:

L = {atb'e™a™ [n> lm > lu {ae e"d" | n> 1Lm> lV}
That is, L consists of strings in atbtc*d™ such that either:

1. There are as many a’s as $’s and as many c’s as d’s, or
2. There are as mary a's as d’s and as many b's as e’s.

L is a context-free language. The obyious grammar for £ is shown in
Fig. 5.22, It uses separate sets of productions to generate the two kinds of
strings in L.

This grammar is ambiguous. For example, the string aabbccdd has the two
leftmost derivations:

1.S> AB=> eABB> aabdbB => aabbcBd = aabbecdd
mi

far fra im im

283 C> aCd= auDdd = aabDedd = aabbeedd
ni ™.

fm tm fm


--- Page 229 ---
5.4. AMBIGUITY IN GRAMMARS AND LANGUAGES 213

S > AB|C
A -» a@Ab| ab
B -+ cBd| ed
C - ald| add
D += 6bDe| be

Figure 5.22: A grammar for an inherently ambiguous language

a |

can aN aN

a /\ AN
iN

\

(a) (b)

Figure 5.23: Two parse trees for aabbecdd

and the two parse trees shown in Fig. 5.23.

The proof that all grammars for Z must be ambiguous is complex. However,
the essence is as follows. We need to argue that all but a finite number of the
strings whose counts of the four symbols a, 5, c, and d, are all equal must be
generated in two different ways: one in which the a’s and b are generated to be
equal and the c’s and d’s are generated to be equal, and a second way, where
the a’s and d’s are generated to be equal and likewise the b’s and c’s.

For instance, the only way to generate strings where the a’s and b’s have
the same number is with a variable like A in the grammar of Fig. 5.22. There
are variations, of course, but these variations do not change the basic picture.
For instance:

e Some small strings can be avoided, say by changing the basis production
A— ab to A > aaabbdb, for instance.


--- Page 230 ---
—

—

214 CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

e We could arrange that 4 shares its job with some other variables, e.g., by
using variables 4, and Ag, with A, generating the odd numbers of a’s and
Ag generating the even numbers, as: 4; — aAgb | ab; Ap > aAzb | ab.

*® We could also arrange that the numbers of a@’s and b’s generated by A
are not exactly equal, but off by some finite number. For instance, we
could start with a production like S$ 4 AbB and then use 4 - aAb | a
to generate one more @ than b's.

However, we cannot avoid some mechanism for generating a’s in a way that
matches the count of 8's.

Likewise, we can argue that there must be a variable like B that generates
matching c’s and d’s. Also, variables that play the roles of C (generate match-
ing a@’s and d’s) and D (generate matching 6’s and c’s) must be available in
the grammar. The argument, when formalized, proves that no matter what
modifications we make to the basic grammar, it will generate at least seme of
the strings of the form a"b"c"d" in the two ways that the grammar of Fig. 5.22
does.

5.4.5 Exercises for Section 5.4

Exercise 5.4.1: Consider the grammar
S73 aS | aSbS | ¢
This grammar is ambiguous. Show in particular that the string aad has two:
a) Parse trees.
b)} Leftmost derivations.
c) Rightmost derivations.

Exercise 5.4.2: Prove that the grammar of Exercise 5.4.1 generates all and
only the strings of a’s and 6’s such that every prefix has at least as many a’s as
b's.

Exercise 5.4.3: Find an unambiguous grammar for the language of Exer-
cise 5.4.1.

Exercise 5.4.4: Some strings of a’s and b’s have a unique parse tree in the
grammar of Exercise 5.4.1. Give an efficient test to tell whether a given string
is one of these. The test “try all parse trees to see how many yield the given
string” ig not adequately efficient.

Exercise 5.4.5: This question concerns the grammar from Exercise 5.1.2,
which we reproduce here:


--- Page 231 ---
5.5. SUMMARY OF CHAPTER 5 215

a)
b)

S —+ AB
A + OAle
B > 0B/1Ble

Show that this grammar is unambiguous.

Find a grammar for the same language that is ambiguous, and demon-
strate its ambiguity.

*! Exercise 5.4.6: Is your grammar from Exercise 5.1.5 unambiguous? If not,
redesign it to be unambiguous.

Exercise 5.4.7: The following grammar generates prefiz expressions with
operands x and y and binary operators +, —, and *:

a)

1b)

5.0

+

EO4+EE|«EE|-EE|z|y

Find leftinost. and rightmost derivations, and a derivation tree for the
string +*-xyxy.

Prove that this grammar is unambiguous.

Summary of Chapter 5

Context-Free Grammars: A CFG is a way of describing languages by
recursive rules called productions. A CFG consists of a set of variables, a
set of terminal symbols, and a start variable, as well as the productions.
Each production consists of a head variable and a body consisting of a
string of zero or more variables and/or terminals.

Derivations and Lunguages: Beginning with the start symbol, we derive
terminal strings by repeatedly replacing a variable by the body of some
production with that variable in the head. The language of the CFG is
the set of terminal strings we can so derive; it is called a context-free
language.

Leftmest and Rightmost Derivations: If we always replace the leftmost
(resp. rightmost) variable in a string, then the resulting derivation is a
leftmost. (resp. rightmost) derivation. Every string in the language of a
CFG has at least one leftmost and at least one rightmost derivation.

Sentential Forms: Any step in a derivation is a string of variables and/or
terminals. We call such a string a sentential form. If the derivation is
leftmost (resp. rightmost), then the string is a left- (resp. right-) sentential
form.


--- Page 232 ---
216

CHAPTER 5. CONTEXT-FREE GRAMMARS AND LANGUAGES

+ Parse Trees: A parse tree is a tree that shows the essentials of a derivation.

Interion nodes are labeled by variables, and leaves are labeled by terminals
or «. For each internal node, there must be a production such that the
head of the production is the lahel of the node, and the labels of its
children, read from left to right, form the body of that production.

Equivalence of Parse Trees and Derivations: A terminal string is in the
language of a grammar if and only if it is the yield of at least one parse
tree. Thus, the existence of leftmost derivations, rightmost derivations,
and parse trees are equivalent conditions that each define exactly the
strings in the language of a CFG.

Ambigueus Grammars: For some CFG's, it is possible to find a terrnina!
string with more than one parse tree, or equivalently, more than one left-
most derivation or more than one rightmost derivation. Such a grammar
is called ambiguous.

Eliminating Ambiguity: For many useful grammars, such as those that
describe the structure of programs in a typical programming language,
it is possible to find an unambiguous grammar that generates the same
language. Unfortunately, the unambiguous grammar is frequently more
complex than the simplest ambiguous grammar for the language. There
are also some context-free languages, usually quite contrived, that are
inherently ambiguous, meaning that every grammar for that language is
ambiguous.

Parsers: The context-free grammar is an essential concept for the im-
plementation of compilers and other programming-language processors.
Tools such as YACC take a CFG as input and produce a parser, the com-
ponent of a compiler that deduces the structure of the program being
compiled.

Decument Type Definitions: The emerging XML standard for sharing
information through Web documents has a notation, called the DTD,
for describing the structure of such documents, through the nesting of
semantic tags within the document. The DTD is in essence a context-free
grammar whose language is a class of related documents.

5.6 References for Chapter 5

The context-free grammar was first proposed as a description method for nat-
ural languages by Chomsky [4]. A similar idea was used shortly thereafter to
describe computer languages — Fortran by Backus [2] and Algol by Naur [7].
As a result, CFG’s are sometimes referred to as “Backus-Naur form grammars.”

Ambiguity in grammars was identified as a problem by Cantor [3] and Floyd

[5] at about the same time. Inherent ambiguity was first addressed by Gross

[6].


--- Page 233 ---
5.6. REFERENCES FOR CHAPTER 5 217

For applications of CFG’s in compilers, see [1]. DTD’s are defined in the
standards document for XML [8].

1. A. ¥. Aho, R. Sethi, and J. D. Ullman, Compilers: Principles, Techniques,
and Tools, Addison-Wesley, Reading MA, 1986.

2. J. W. Backus, “The syntax and semantics of the proposed international
algebraic language of the Zurich ACM-GAMM conference,” Proc. Init.
Conf. on Information Processing (1959), UNESCO, pp. 125-132.

3. D. C. Cantor, “On the ambiguity problem of Backus systems,” J. ACM
9:4 (1962), pp. 477-479.

4, N. Chomsky, “Three models for the description of language,” IRE Trans.
on Information Theory 2:3 (1956), pp. 113-124.

5. R. W. Floyd, “On ambiguity in phrase-structure languages,” Comm, ACM
5:10 (1962), pp. 526-534.

6. M. Gross, “Inherent ambiguity of minimal linear grammars,” Information
and Control 7:3 (1964), pp. 366-368.

7. P. Naur et al., “Report on the algorithmic language ALGOL 60,” Comm.
ACM 3:5 (1960), pp. 299-314. See also Comm. ACM 6:1 (1963), pp. 1-17.

8. World-Wide-Web Consortium, http: //www.w3.org/TR/REC-xml (1998).


--- Page 234 ---


--- Page 235 ---
Chapter 6

Pushdown Automata

The context-free languages have a type of antomaton that defines them. This
automaton, called a “pushdown automaton,” is an extension of the nondeter-
ministic finite automaton with ¢-transitions, which is one of the ways to define
the regular langnages. The pushdown automaton is essentially an «-NFA with
the addition of a stack. The stack can be read, pushed, and popped only at the
top, just like the “stack” data structure.

In this chapter, we define two different versions of the pushdown automaton:
one that accepts by entering an accepting state, like finite automata do, and
another version that accepts by emptying its stack, regardless of the state it is in.
We show that these two variations accept exactly the context-free languages;
that is, grammars can be converted to equivalent pushdown automata, and
vice-versa. We also consider briefly the subclass of pushdown automata that is
deterministic. These accept all the regular languages, but only a proper subset
of the CFL’s. Since they resemble closely the mechanics of the parser in a
typical compiler, it is important to observe what language constructs can and
cannot be recognized by deterministic pushdown automata.

6.1 Definition of the Pushdown Automaton

In this section we introduce the pushdown automaton, first informally, then as
a formal construct.

6.1.1 Informal Introduction

The pushdown automaton is in essence a nondeterministic finite automaton
with ¢-transitions permitted and one additional capability: a stack on which it
can store a string of “stack symbols.” The presence of a stack means that, unlike
the finite automaton, the pushdown automaton can “remember” an infinite
amount of information. However, unlike a general-purpose computer, which
also has the ability to remember arbitrarily large amounts of information, the

219


--- Page 236 ---
220 CHAPTER 6. PUSHDOWN AUTOMATA

pushdown automaton can only access the information on its stack in a last-in-
first-out way.

As a result, there are languages that could be recognized by some computer
program, but are not recognizable by any pushdown automaton. In fact, push-
down automata recognize all and only the context-free languages. While there
are many languages that are context-free, including some we have seen that are
not regular languages, there are also some simplc-to-describe languages that are
not context-free, as we shal] see in Section 7.2. An example of a non-context-
free language is {0"1"2" | n > 1}, the set of strings consisting of eqnal groups
of 0's, 1’s, and ?’s.

Finite
state ‘
Input control Accept/reject

Stack

Figure 6.1: A pushdown automaton is essentially a finite automaton with a
stack data structure

We can view the pushdown automaton informally as the device suggested
in Fig. 6.1. A “finite-state control” reads inputs, one symbol at a time. The
pushdown automaton is allowed to observe the symbol at the top of the stack
and to base its transition on its current state, the input symbol, and the symbol
at the top of stack. Alternatively, it may make a “spontaneous” transition, using
€ as its input instead of an input symbol. In one transition, the pushdown
automaton:

1. Consumes from the input the syrubol that it uses in the transition. If e is
used for the input, then no input symbol is consumed.

2. Goes to a new state, which may or may not be the same as the previous
state.

3. Replaces the symbol at the top of the stack by any string. The string
could be ¢, which corresponds to a pop of the stack. It could be the same
syinbol that appeared at the top of the stack previously; i-e., no change
to the stack is made. It could also replace the top stack symbol by one
other symbol, which in effect chanyes the top of the stack but does not
push or pop it. Finally, the top stack symbol could be replaced by two or
more symbols, which has the effect of (possibly) changing the top stack
symbol, and then pushing one or more new symbols onto the stack.


--- Page 237 ---
6.1. DEFINITION OF THE PUSHDOWN AUTOMATON 221

Example 6.1: Let us consider the language
Lewr = {ww | w isin (04+ 1)*}

This language, often referred to as “w-w-reversed,” is the even-length palin-
dromes over alphabet {0,1}. It is a CFL, generated by the grammar of Fig. 5.1,
with the productions P > 0 and P > 1 omitted.

We can design an informal pushdown automaton accepting Lywr, 48 fol-
lows.?

1. Start in a state go that represents a “guess” that we have not yet seen the
middle; i.e., we have not seen the end of the string w that is to be followed
by its own reverse. While in state gg, we read symbols and store them on
the stack, by pushing a copy of each input symbol onto the stack, in turn.

2. At any time, we may guess that we have seen the middle, i.e., the end of
w. At this time, w will be on the stack, with the right end of w at the top
and the left end at the bottom. We signify this choice by spontaneously
going to state q,. Since the automaton is nondeterministic, we actually
make both guesses: we guess we have seen the end of w, but we also stay
in state go and continue to read inputs and store them on the stack.

3. Once in state g), we compare input symbols with the symbol at the top
of the stack. If they match, we consume the input symbol, pop the stack,
and proceed. If they do not match, we have guessed wrong; our guessed
w was not followed by w®. This branch dies, although other branches
of the nondeterministic automaton may survive and eventually lead to
acceptance.

4, If we empty the stack, then we have indeed seen some input w followed
by w®. We accept the input that was read up to this point.

DO

6.1.2 The Formal Definition of Pushdown Automata

Our formal notation for a pushdown automaton (PDA) involves seven compo-
nents. We write the specification of a PDA P as follows:

P= (Q, 2, 0,6, go, 20, F)
The components have the following meanings:

Q: A finite set of states, like the states of a finite automaton.

lye could also design a pushdown automaton for Lyut, which is the language whose
grammar appeared in Fig. 5.1. However, Luw,r is slightly simpler and will allow us to focus
on the important ideas regarding pushdown automata.


--- Page 238 ---
222 CHAPTER 6. PUSHDOWN AUTOMATA

No “Mixing and Matching”

There may be several pairs that are options for a PDA in some situation.
For instance, suppose d(g,a,X) = {(p, YZ), (r,€)}. When making a move
of the PDA, we have to chose one pair in its entirety; we cannot pick a
state from one and a stack-replacement string from another. Thus, in state
q, with X on the top of the stack, reading input a, we could go to state p
and replace X by YZ, or we could go to state r and pop X, However, we
cannot go to state p and pop A, and we cannot go to state r and replace
A by YZ.

x: A finite set of input symbols, also analogous to the corresponding compo-
nent of a finite automaton.

lr: A finite séack alphabet. This component, which has no finite-autornaton
analog, is the set of symbols that we are allowed to push onto the stack.

6: The transition function. As for a finite automaton, é governs the behavior
of the automaton. Formally, é takes as argument a triple 6(q,@, X), where:

1. g is a state in Q.

2. ais either an input symbol in © or a = ¢, the empty string, which is
assumed not to be an input symbol.

3. A is a stack symbol, that is, a member of I.

The output of é is a finite set of pairs (p, y), where p is the new state, and
7 is the string of stack symbols that replaces X at the top of the stack.
For instance, if y = ¢, then the stack is popped, if + = X, then the stack
is unchanged, and if y= YZ, then X is replaced by Z, and Y is pushed
onto the stack.

qo: The start state. The PDA is in this state before making any transitions.

Zo: The start symbol. Initially, the PDA’s stack consists of one instance of
this symbol, and nothing else.

F: The set of accepting states, or final states.

Example 6.2: Let us design a PDA P to accept the language Luu, of Exam-
ple 6.1. First, there are a few details not present in that example that we need
to understand in order to manage the stack properly. We shall use a stack sym-
bol Zp to mark the bottom of the stack. We need to have this symbol present.
so that, after we pop w off the stack and realize that we have seen ww on the


--- Page 239 ---
6.1. DEFINITION OF THE PUSHDOWN AUTOMATON 223

input, we still have something on the stack to permit us to make a transition
to the accepting state, g2.. Thus, our PDA for Ly... can be described as

where 4 is defined by the following rules:

1. 6{qo.0.2Z0) = {(go,0Z9}} and 6(qo.1.Z0) = {(go.1Z}}. One of these
rules applies initially, when we are in state gg and we see the start symbol
Zo at the top of the stack. We read the first input, and push it onto the
stack, leaving Zo below to mark the bottom.

2. 6(q9.0-0) = {(qo.00)}, 6(qo, 0,1) = {(q0,04)}, 5(4o,1,0) = {(qa, 10)}, and
8(qo,1,1) = {(go,11)}. These four, similar rules allow us to stay in state
qo and read inputs, pushing each onto the top of the stack and leaving
the previous top stack symbol alone.

These three rules allow P to go from state gg to state gy spontaneously
(on € input). leaving intact whatever symbol is ai the top of the stack.

4. 8(g,,0.0) = {(¢1,e)}, and d(qi.1.1) = {(qi,€)}. Now, in state g, we can
match input symbols against the top symbols on the stack, and pop when
the symbols match.

5. O(gi.€. Zo) = {(g2, Zo)}. Finally, if we expose the bottom-of-stack marker
Zo and we are in state g,. then we have found an input of the form wrt,

We go to state qo and accept.

Oo

6.1.3. A Graphical Notation for PDA’s

The list of 6 facts, as in Example 6.2, is not too easv to follow. Sometimes. a
diagram, generalizing the transition diagram of a finite automaton, will make
aspects of the hehavior of a given PDA clearer. We shall therefore introduce
and subsequently use a transttien diagram for PDA’s in which:

a) The nodes correspond to the states of the PDA.

b) An arrow labeled Start indicates the start state. and doubly circled states
are accepting, as for finite automata.

c) The arcs correspond to transitions of the PDA in the following sense. An
arc labeled a, .X/o from state g to state p means that d(g.a.A) contains
the pair (p,a), perhaps among other pairs. That is, the are label tells
what input is used, and also gives the old and new tops of the stack.


--- Page 240 ---
224 CHAPTER 6. PUSHDOWN AUTOMATA

0,Z,/0Z,
1, Zy/1Zy
0, 0/00
0, 1/01
1, 0/10 0,0/¢€e
1, 1/t1 Il. 1l/e
sat Ge ©
) &, are £, ~®
e, 0/0
& 1/1

Figure 6.2: Representing a PDA as a generalized transition diagram

The only thing that the diagram does not tell us is which stack symbol is the
start symbol. Conventionally, it is 2), unless we indicate otherwise.

Example 6.3: The PDA of Example 6.2 is represented by the diagram shown
in Fig. 6.2. O

6.1.4 Instantaneous Descriptions of a PDA

To this point, we have only an informal notion of how a PDA “computes.” Intu-
itively, the PDA goes from configuration to configuration, in response to input
symbols (or sometimes ¢€), but unlike the finite automaton, where the state is
the only thing that we need to know about the automaton, the PDA's config-
uration involves both the state and the contents of the stack. Being arbitrarily
large, the stack is often the more important part of the total configuration of
the PDA at any time. It is also useful to represent as part of the configuration
the portion of the input that remains.

Thus, we shall represent the configuration of a PDA by a triple (¢,w,7),
where

1. g is the state,
2. w is the remaining input, and
3. 7 is the stack contents.

Conventionally, we show the top of the stack at the left end of y and the bottom
at the right end. Such a triple is called an instantaneous description, or ID, of
the pushdown automaton.

For finite automata, the 6 notation was sufficient to represent sequences
of instantaneous descriptions through which a finite automaton moved, since


--- Page 241 ---
6.1. DEFINITION OF THE PUSHDOWN AUTOMATON 225

the ID for a finite automaton is just its state, However, for PDA’s we need a
notation that describes changes in the state, the input, and stack. Thus, we
adopt the “turnstile” uotation for connecting pairs of ID’s that represent one
or many moves of a PDA.

Let P = (Q,2,T,6,¢0, 2, £) be a PDA. Define be or just F when P is

understood, as follows. Suppose d(g, a, X) contains (p,a). Then for all strings
win }* and #inT*:
This move reflects the idea that, by consuming « (which may be ¢) from the
input and replacing X on top of the stack by a, we can go from state g to state
p. Note that what remains on the input, w, and what is below the top of the
stack, 3, do not influence the action of the PDA; they are merely carried along,
perhaps to influence events later.

We also use the symbol F , Or when the PDA P is understood, to represent

gero or more moves of the PDA. That. is:
BASIS: JF J for any ID J.
INDUCTION: JF J if there exists some ID K such that I K and & Fy.

That is, J J if there is a sequence of ID’s Ky, K2,...,, such that J = ky,
J = Ky, and for alli =1,2,...,.2 —1, we have Aj b Kips.

Example 6.4: Let us consider the action of the PDA of Example 6,2 on the
input 1111. Since go is the start state and Zo is the start symbol, the initial ID
is (go, 1111, Zp). On this input, the PDA has an opportunity to guess wrongly
several times. The entire sequeuce of ID’s that the PDA can reach from the
initial ED (go, 1111, Zo) is shown in Fig. 6.3. Arrows represent the F relation.

From the initial ID, there are two choices of move. The first guesses that
the middle has not been seen and leads to ID (go, 111,12). In effect, a 1 has
been removed from the input and pushed onto the stack.

The second choice from the initial ID guesses that the middle has been
reached. Without consuming input, the PDA gocs to state q;, leading to the
ID (q,, 1141, Z)). Since the PDA may accept if it is in state q and sees Zp on
top of its stack, the PDA goes from there to ID (q2,1111, 2). That ID ts not
exactly an accepting ID, since the input has not been completely consumed.
Had the input. been ¢ rather than 1111, the same sequence of moves would have
led to ID (g2,€, Zg), which would show that ¢ is accepted.

The PDA may also guess that it has seen the middle after reading one |, that
is, when it is in the ID (gy,111,1Z). That guess also leads to failure, since
the entire input cannot be consumed. The correct guess, that the middle is
reached after reading two 1’s, gives us the sequence of ID's (go, 1111, Zo) F
Ci 111, 1249) - (go. 11, 11Z) F (q1 ; 11, i1Z) F (Hh: 1, 120) F (mH it, Zo) [~
(42,€,Z0). O

There are three important principles about TD’s and their transitions that
we shall need in order to reason about PDA’s:


--- Page 242 ---
226 CHAPTER 6. PUSHDOWN AUTOMATA

(q+ 11LZ,)

oN

(q» lll 124) (¢,. M1L,Z)) ~ (%, 1111.2)

_—

(q+ ILMZy) = Cg, WE1Z)) (gs ZQ)

_—

(q+ 1,111Z,) (q+ 11 1MZ,y ) (%,11,Zy)

Po

(q.e@.1llZy) (g. LZ) (q,.11Z9)

| '

'

(%,€,Z)

Figure 6.3: ID’s of the PDA of Example 6.2 on input 1111

1. If a sequence of ID’s (computation) is legal for a PDA P, then the com-
putation formed by adding the same additional input string to the end of
the input (second component) in each ID is also legal.

2. If a computation is legal for a PDA P, then the computation formed by
adding the same additional stack symbols below the stack in each ID is
also legal.

3. Tf a computation is legal for a PDA P, and some tail of the input is not
consumed, then we can remove this tail from the input in each ID, and
the resulting computation will still be legal.

Intuitively, data that P never looks at cannot affect its computation. We for-
malize points (1) and (2) in a single theorem.


--- Page 243 ---
6.1. DEFINITION OF THE PUSHDOWN AUTOMATON 227

Notational Conventions for PDA’s

We shall continue using conventions regarding the use of symbols that
we introduced for finite automata and grammars. In carrying over the
notation, it is useful to realize that the stack symbols play a role analogous
to the union of the terminals and variables in a CFG. ‘Thus:

1. Symbols of the input alphabet will be represented by lower-case let-
ters near the beginning of the alphabet, e.g., a, b.

_ States will be represented by q and p, typically, or other letters that
are nearby in alphabetical order.

. Strings of input symbols will be represented by lower-case letters
near the end of the alphabet, e.g., w or 2.

. Stack symbols will be represented by capital letters near the end of
the alphabet, e.g., X or Y.

. Strings of stack symbols will be represented by Greek letters, e.g., a
ory.

Theorem 6.5: If P = (Q,=,T,4,q0, Zo, F) is a PDA, and (q,2, a) r (py, B)s
then for any strings w in &* and + in I, it is also true that

(q, 2w, a7) F (p, yw, BY)

Note that if y = e, then we have a formal statement of principle (1) above, and
if w = e, then we have the second principle.

PROOF: The proof is actually a very simple induction on the number of steps

in the sequence of ID’s that take (¢, rw, avy) to (p, yw, f7). Each of the moves

in the sequence (q,z,c) F (p,y, 8) is justified by the transitions of P without
P

using w and/or 7 in any way. Therefore, each move is still justified when these
strings are sitting on the input and stack. O

Incidentally, note that the converse of this theorem is false. There are things
that a PDA might be able to do by popping its stack, using some symbols of +,
and then replacing them on the stack, that it couldn’t do if it never looked at
+. However, as principle (3) states, we can remove unused input, since it is not
possible for a PDA to consume input symbols and then restore those symbols
to the input. We state principle (3) formally as:


--- Page 244 ---
228 CHAPTER 6. PUSHDOWN AUTOMATA

ID’s for Finite Automata?

One might wonder why we did not introduce for finite automata a notation
like the ID’s we use for PDA’s. Although a FA has no stack, we could use
a pair (g,w), where g is the state and w the remaining input, as the ID of
a finite automaton,

While we could have done so, we would not glean any more informa-
tion from reachability among ID’s than we obtain from the é notation.
That is, for any finite automaton, we could show that &(q, w) = pif and
only if (g,wx) F (p,2) for all strings x. The fact that 2 can be anything
we wish without influencing the behavior of the FA is a theorem analogous
to Theorems 6.5 and 6.6.

Theorem 6.6: If P = (Q,2,T,4,¢0, 20, F) is a PDA, and
(q, ow, a) F (p, yu, 3)

then it is also true that (q, 7,0) F (ay, 3). O

6.1.5 Exercises for Section 6.1

Exercise 6.1.1: Suppose the PDA P = ({g,p}, {0,1}, {Zo, X},4,¢@, Zo, {p})
has the following transition function:

2. 6(¢,0,X) = {(g, XX)}.
3. 6(q,1,X) = {(@, X)}-
4. 3(q,€,X) = {(p.e}}.
5. 6(p,€,.X) = {(p,6)}-
6. 6(p,1,X) = {(p, XX}.
7. d(p, 1, Zo) = {(p, €)}-

Starting from the initial ID (g,w, Zp), show all the reachable ID’s when the
input w is:

* a) OL.
b) 0011.
c) O10.


--- Page 245 ---
6.2. THE LANGUAGES OF A PDA 229

6.2 The Languages of a PDA

We have assumed that a PDA accepts its input by consuming it and entering
an accepting state. We call this approach “acceptance by final state.” There
is a second approach to defining the language of a PDA that has important
applications. We may also define for any PDA the language “accepted by
empty stack,” that is, the set of strings that cause the PDA to empty its stack,
starting from the initial ID.

These two methods are equivalent, in the sense that a language £ has a
PDA that accepts it by final state if and only if L has a PDA that accepts it
by empty stack. However, for a given PDA FP, the languages that P accepts
by final state and by empty stack are usually different. We shall show in this
section how to convert a PDA accepting D by final state into another PDA that
accepts L by empty stack, and vice-versa.

6.2.1 Acceptance by Final State

Let P = (Q,5,T,4,¢0, 20, F) be a PDA. Then L{P), the language accepted by
P by final state, is
{w | (qo, w, Zo) r (9, €,a)}

for some state g in F’ and any stack string a. That is, starting in the initial
ID with w waiting on the input, P consumes w from the input and enters an
accepting state. The contents of the stack at. that time is irrelevant.

Example 6.7: We have claimed that the PDA of Example 6.2 accepts the
language Lyyr, the language of strings in {0,1}* that have the form wu, Let
us see why that statement is truce. The proof is an if-and-only-if statement: the
PDA P of Example 6.2 accepts string x by final state if and only if z is of the
form wo.

(If) This part is easy; we have only to show the accepting computation of P. If
2 = ww, then observe that

(go, ww®, Zo) F (qo, w®,w® Zo) k (qi, w®, w* Zp) F (a1,€, Zo) & (42, €, Zo)

That is, one option the PDA has is to read w from its input and store it on its
stack, in reverse. Next, it goes spontaneously to state g,; and matches w® on
the input with the same string on its stack, and finally goes spontancously to
state qd.

(Only-if) This part is harder. First, observe that the only way to enter accepting
state gz is to be in state g, and have Zp at the top of the stack. Also, any
accepting computation of P will start in state gj, make one transition to 4,
and never return to go. Thus, it is sufficient to find the conditions on z such
that (go. 2, 40) F (q,€, Zo); these will be exactly the strings x that P accepts
by final state. We shall show by induction on || the slightly more general
statement:


--- Page 246 ---
230 CHAPTER 6. PUSHDOWN AUTOMATA

« Tf (go...) - (q1,€,), then = is of the form ww*.

BASIS: If c = ¢, then z is of the form ww* (with w =). Thus, the conclusion is
true, so the statement is true. Note we do not have to argue that the hypothesis
(go, €, a} F (m,€,@) is true, although it is.

INDUCTION: Suppose « = @,a@2--+a, for some n > 0. There are two moves
that P van make from ID (go, #, a):

1. (go,c,a) + (q,2,0). Now P van only pop the stack when it is in state
qi. P must pop the stack with every input symbol it reads, and |z| > 0.
Thus, if (¢,2,0) F (q1.¢€, 3), then $ will be shorter than @ and cannot
be equal to «.

2. (G9, @102*--Gn, a) F (gp, @2°->-Gn, 4:0). Now the only way a sequence of
moves can end in (q,,¢,«) is if the last move is a pop:

(q1 Gn, ac) b (q 1 €y a)
In that case, it must be that a, = a,. We also know that
(Go, a2°**Gn, 010) F (gi, Gn, 61a)

By Theorem 6.6, we can remove the symbol a, from the end of the input,
since it is not used. Thus,

(Go, a2 **1@n-1, 41) F (q1,€, 410)

Since the input for this sequence is shorter than mn, we may apply the
inductive hypothesis and conclude that az ---@n_, is of the form yy” for
some y. Since x = a, yya,, and we know a, = a,, we conclude that = is
of the form ww; specifically w = ayy.

The above is the heart of the proof that the only way to accept z is for z
to be equal to ww” for some w. Thus, we have the “only-if” part of the proof,
which, with the “if” part proved earlier, tells us that P accepts exactly those
strings in Louw O

6.2.2 Acceptance by Empty Stack
For each PDA P = (Q,2,1T,6, qo, Zo, F), we also define

for any state g. That is, N(P) is the set of inputs w that P can consume and
at the same time empty its stack.’

2The N in N{F) stands for “nuil stack,” a synonym for “empty stack.”


--- Page 247 ---
6.2. THE LANGUAGES OF 4 PDA 231

Example 6.8: The PDA P of Example 6.2 never empties its stack, so N(P) =
§. However, a small modification will allow P to accept Ly», by empty stack
as well as by final state. Instead of the transition 5(q),¢€, 29) = {(g2, Za)}, use
5(q1,€,20) = {(g2,¢)}. Now, P pops the last symbol off its stack as it accepts,
and L(P) = N(P) = Luu O

Since the set of accepting states is irrelevant, we shall sometimes leave off
the last (seventh) component from the specification of a PDA P, if all we care
about is the language that P accepts by empty stack. Thus, we would write P
as a six-tuple (Q, 5,1, 4,¢o, Zo).

6.2.3. From Empty Stack to Final State

We shall show that the classes of languages that are L(P) for some PDA P is
the same as the class of languages that are N(P) for some PDA P. This class
is also exactly the context-free languages, as we shall see in Section 6.3. Our
first construction shows how to take a PDA Pw that accepts a language £ by
empty stack and construct a PDA Pr that accepts Z by final state.

Theorem 6.9: If £ = N(Px} for some PDA Pa = (Q,2,T, dn, ¢0, Zo). then
there is a PDA Pr such that E = E(Pr).

PROOF: The idea behind the proof is in Fig. 6.4. We use a new symbol Xp,
which must not be a symbol of TP; Xp is both the start symbol of Pr and a
marker on the bottom of the stack that lets us know when Px has reached an
empty stack. That is, if Pe sees Xp on top of its stack, then it knows that Py
would empty its stack on the same input.

& X,/e

Figure 6.4: Pr simulates P;, and accepts if Py empties its stack

We also need a new start state, 99, whose sole function is to push Zy, the
start symbol of Py, onto the top of the stack and enter state go, the start


--- Page 248 ---
232 CHAPTER 6. PUSHDOWN AUTOMATA

state of Py. Then, Py simulates Py, until the stack of Py is empty, which Pr
detects because it secs Xp on the top of the stack. Finally, we need another
new state, py, which is the accepting state of Pr; this PDA transfers to state
py whenever it discovers that Py would have emptied its stack.

The specification of Pr is as follows:

where 6, is defined by:

1. d¢(po,e, Xo) = {(40, Z0-Xo)}. In its start state, Pr makes a spontaneous
transition to the start state of Pa, pushing its start symbol Zp onto the
stack.

2. For all states gq in Q, inputs a in £ or a = e, and stack symbols Y in I,
6y-(q,@, ¥) contains all the pairs in dy(g,a, Y).

3. In addition to rule (2), ér{g,¢,Xo) contains (p;,¢} for every state g in Q.

We must show that w is in L( Pr} if and only if w is in N{ Py).

(If} We are given that (go, w, Zo} FP aes €) for some state g. Theorem 6.5 lets

us insert Xp at the bottom of the stack and conclude (go, w, Zo.Xo) | (q,€, Xo).

Since by rule (2) above, Pr has all the moves of Py, we may also conclude that
(go, 2, Zn Xo} C (g,€, Xo}. If we put this sequence of moves together with the

initial and final moves from rules (1) and (3) above, we get:

Thus, Pr accepts w by final state.

(Only-if} The converse requires only that we observe the additional transitions
of rules (1) and (3) give us very limited ways to accept w by final state. We must
use rule (3) at the last step, and we can only use that rule if the stack of Pr
contains only No. No X's ever appear on the stack except at the bottommost
position. Further, rule (1) is only used at the first step, and it must be used at
the first step.

Thus, any computation of Pr that accepts w must look like sequence (6.1).
Moreover, the middle of the computation — all but the first and last steps —
must. also be a computation of Py with X below the stack. The reason is that,
except for the first and last steps, P; cannot use any transition that is not also
a transition of P»;,, and Xj, cannot be exposed or the computation would end at
the next step. We conclude that (qo, w, Zo) F (q,€,€). That is, w is in N(Pw).
oO N


--- Page 249 ---
6.2. THE LANGUAGES OF A PDA 233

Example 6.10: Let us design a PDA that processes sequences of if’s and
else’s in a C program, where i stands for if and e stands for else. Recall
from Section 5.3.1 that there is a problem whenever the number of else’s in
any prefix exceeds the number of if’s, becatise then we cannot match each
else against its previous if. Thus, we shall use a stack symbol Z to count the
difference between the number of é’s seen so far and the number of e’s. This
simple, one-state PDA, is suggested by the transition diagram of Fig. 6.5.

€é, Z/E
sun)
———\{!{__

Figure 6.5: A PDA that accepts the iffelse errors by empty stack

We shall push another Z wherever we see an ¢ and pop a Z whenever we sce
an e. Since we start with one Z on the stack, we actually follow the rule that if
the stack is Z", then there have been n — 1 more ?’s than e’s. In particular, if
the stack is empty, than we have seen one more e than i, and the input read so
far has just become illegal for the first time. It is these strings that our PDA
accepts by empty stack. The formal specification of Py is:

Pw = ({a}, ft,e}, {2}, 4n. % 2)
where éy is defined by:
1. 6n(q.i, Z) = {(g,ZZ)}. This rule pushes a Z when we see an #.
2. én(g,¢e, Z) = {(g,e)}. This rule pops a Z when we see an e.

é, Z/E
i, Z/ZZ

Start C
6) &, X YZX 4 ie €,X_ /€ @)

Figure 6.6: Construction of a PDA accepting by final state from the PDA of
Fig. 6.5

Now, let us construct from Py a PDA Pp that accepts the same language
by final state; the transition diagram for P; is shown in Fig. 6.6.2 We introduce

3Doe not be concerned that we are using new states p and 7 here, while the construction
in Theorem 6.9 used po and py, Names of states are arbitrary, of course.


--- Page 250 ---
234 CHAPTER 6. PUSHDOWN AUTOMATA

anew start state p and an accepting state r. We shall use Xp as the bottom-of
stack marker. Py is formally defined:

Pr= ({p, qr}, {i,e}, {Z, Xo}, dr, p, Xo, {r})
where é- consists of:

1. de(p,e, Xo) = {(q, ZXo)}. This rule starts Pp simulating Py, with Xo as
a, bottom-cf-stack-marker.

2. r(q,i, Z) = {(q,ZZ)}. This rule pushes a Z when we see an #; it simu-
lates Py.

3. 6e(q,e,Z) = {(¢,¢)}. This rule pops a Z when we see an e; it also
simulates Px.

4. dr(q,e,Xo) = {(r,6.¢)}. That is, Pr accepts when the simulated Py
would have emptied its stack.

Oo

6.2.4 From Final State to Empty Stack

Now, let us go in the opposite direction: take a PDA Pr that accepts a language
L by final state and constract another PDA Py that accepts LZ by empty stack.
The construction is simple and is suggested in Fig. 6.7. From each accepting
state of Py, add a transition on ¢ to a new state p. When in state p, Py pops its
stack and does not consume any input. Thus, whenever Pp enters an accepting
state after consuming input w, Px will empty its stack after consuming w.

To avoid simulating a situation where Pp accidentally empties its stack
without accepting, Py must also use a marker Xo on the bottom of its stack.
The marker is Py’s start symbol, and like the construction of Theorem 6.9, Py
must start in a new state pp, whose sole function is to push the start symbol of
Pp on the stack aud go to the start state of Pp. The construction is sketched
in Fig. 6.7, and we give it formally in the next theorem.

Figure 6.7: Py simulates Pp and empties its stack when and only when Py
enters an accepting state

Theorem 6.11: Let L be L(Pr) for some PDA Pr = (Q,2,T, 467,90, Zo, F)-
Then there is a PDA Pw such that L = N(Py).


--- Page 251 ---
6.2. THE LANGUAGES OF A PDA 235

PROOF: The construction is as suggested in Fig. 6.7. Let
where éy is defined by:

1. 6n(po,€,Xo) = {(g0, Z0Xo)}. We start by pushing the start symbol of Pr
onto the stack and going to the start state of Pp.

9. For all states g in Q, input symbols a in ¥ or a = ¢, and Y inT, 6n(q,a, ¥)
contains every pair that is in ée(q,a,¥). That is, Pw simulates Py.

3. For all accepting states g in F and stack symbols Y in T or ¥ = Xp,
én(q,€,¥) contains (p,e). By this rule, whenever Pp accepts, Py can
start emptying its stack without consuming any snore input.

4. For all stack symbols Y in T’ or ¥ = Xo, Sn{p,€,Y) = {(p,€)}. Once in
state p, which only occurs when Pr has accepted, Py pops every symbol
on its stack, until the stack is empty. No further input is consumed.

Now, we must prove that w is in N(Py) if and only if w is in L(Pr).
The ideas are similar to the proof for Theorem 6.9. The ‘if? part is a direct
simulation, and the ‘only-if” part requires that we examine the limited number
of things that the constructed PDA Py can do.

({f} Suppose (go, w, Zo) E (q,€,@) for some accepting state g and stack string
Fr
a. Using the fact that every transition of Pr is a move of Py, and invoking

Theorem 6.5 to allow us to keep Xp below the symbols of I on the stack, we
know that (go, , Zo.Xo) FE (q,€,@Xo). Then Py can do the following:
w

Xx ZpXo)F Xo) FE (pe,
(po. w, oF (go,w, ZoX0) Fg &: 2X0) EWP €,€)

if

The first move is by rule (1) of the construction of Px, while the last sequence
of moves is by rules (3) and (4). Thus, w is accepted by Px, by empty stack.
(Only-if) The only way Py can empty its stack is by entering state p, since
Xp is sitting at the bottom of stack and Xo is not a symbol on which Pr has
any moves. The only way Py can enter state p is if the simulated Py outers
an accepting state. The first move of Pw is surely the move given in rule (1).
Thus, every accepting computation of Py looks like

(po, W, xo) F (go, W, ZoXo) F (@,€; aX) F (p, €,€)
Py Px Py

where q is an accepting state of Pr.

Moreover, between ID’s (gp, w,ZoXo) and (¢,€,@Xo), all the moves are
moves of Pp. In particular, Xp was never the top stack symbol prior to reaching
ID (g,€,aXp).* Thus, we conclude that the same computation can occur in Pr,
without the Xp, on the stack; that is, (go,w, Zo) F (q,€,a). Now we see that

F

Pp accepts w by final state, sow isin Z(P). O

4 Although a could be ¢, in which case Pr haa emptied its atack at the same time it accepts.


--- Page 252 ---
—

—

236 CHAPTER 6. PUSHDOWN AUTOMATA

6.2.5 Exercises for Section 6.2

Exercise 6.2.1: Design a PDA to accept each of the following languages. You
may accept cither by final state or by empty stack, whichever is more convenient.

* a) {0P1" | n> I}.

b) The set of all strings of 0’s and 1’s such that no prefix has more 1’s than
0's.

c) The set. of all strings of 0’s and 1’s with an equal number of 0’s and 1's.
Exercise 6.2.2: Design a PDA to accept each of the following languages.

*a) {aldic® |i — jf or j — k}. Note that this language is different from that
of Exercise 3.1.1(b).

b) The set of all strings with twice as many 0’s as 1’s.
if Exercise 6.2.3: Design a PDA to accept cach of the following languages.
a) {a*bc® |i Aj or 7 AK}.

b) The set of ail strings of a’s and 8’s that are not of the form ww, that is,
not equal to any string repeated.

Exercise 6.2.4: Let P be a PDA with empty-stack language L = N(P), and
suppose that ¢ is not in L. Describe how you would modify P so that it accepts
EU {e} by empty stack.

Exercise 6.2.5: PDA P = ({q0,%,9.493; f}, {e,6}, {20, A, B}, 4, go, 20, {F})
has the following rules defining 6:

8(q3,€,B) = (q2, €) 6(93,€, 20) = (41, AZo)

Note that, since each of the sets above has only one choice of move, we have
omitted the set brackets from each of the rules.

* a) Give an execution trace (sequence of ID’s) showing that string bab is in
LP).

b) Give an execution trace showing that abd is in L(P).
c) Give the contents of the stack after P has read 8’a4 from its input.
1d) Informally describe L(P).

Exercise 6.2.6: Consider the PDA P from Exercise 6.1.1,


--- Page 253 ---
*

6.3. EQUIVALENCE OF PDA'S AND CFG'S 237

a) Convert P to another PDA P, that accepts by empty stack the same
language that P accepts by final state; i.e. N(P)) = ECP).

b) Find a PDA & such that L(P:) = N(P); Le, P2 accepts by final state
what P accepts by empty stack.

Exercise 6.2.7: Show that if P is a PDA, then there is a PDA /: with only
two stack symbols, such that L(P2.) = L(P). Hint: Binary-code the stack
alphabet of P.

Exercise 6.2.8: A PDA is called restricted if on any transition it can increase
the height of the stack by at most one symbol. That is, for any rule é(q.@, 2)
contains (p, 7), it must be that |y| < 2. Show that if P is a PDA, then there is
a restricted PDA Ps such that E(P} = DCP).

6.3. Equivalence of PDA’s and CFG’s

Now, we shal] demonstrate that the languages defined by PDA's are exactly the
context-free languages. The plan of attack is suggested by Pig. 6.8. The goal
is to prove that the following three classes of languages:

1. The context-free languages, }.e., the languages defined by CFG’s.
2. The languages that are accepted by final state by some PDA.
3. The languages that are accepted by empty stack by some PDA.

are all the same class. We have already shown that (2) and (3) are the same.
It turns out to be easiest next to show that (1) and (3) are the same, thus
implying the equivalence of all three.

PDA by
empty siack

PDA by
final state

Figure 6.8: Organization of constructions showing equivalence of three ways of
defining the CFL’s

6.3.1 From Grammars to Pushdown Automata

Given a CFG G, we construct a PDA that simulates the leftmost derivations
of G. Any left-sentential form that is not a terminal string can be written as
rAa, where A is the leftmost variable, x is whatever terminals appear to its
left, and o is the string of terminals and variables that appear to the right of A.


--- Page 254 ---
238 CHAPTER 6. PUSHDOWN AUTOMATA

We call Aa the tail of this left-sentential form. If a left-sentential form consists
of terminals only, then its tail is e.

The idea behind the construction of a PDA from a grammar is to have
the PDA simulate the sequence of left-sentential forms that the grammar uses
to generate a given terminal string w. The tail of each sentential form zAa
appears on the stack, with A at the top. At that time, x will be “represented”
by our having consumed « from the input, leaving whatever of w follows its
prefix v. That is, if w = zy, then y will remain on the input.

Suppose the PDA is in an ID (q,y, Aa), representing left-sentential form
zAa. It guesses the production to use to expand A, say A > 8. The move of
the PDA is to replace A on the top of the stack by f, entering ID (g,y, Sa).
Note that there is only one state, q, for this PDA.

Now (gq, y,3a@) may not be a representation of the next left-sentential form,
because # may have a prefix of terminals. In fact, @ may have no variables at
all, and @ may have a prefix of terminals. Whatever terminals appear at the
beginning of Ja need to be removed, to expose the next variable at the top of
the stack. These terminals are conipared against the next input symbols, to
make sure cur guesses at the leftmost derivation of input string w are correct;
if not, this branch of the PDA dies.

If we succecd in this way to guess a leftmost derivation of w, then we shall
eventually reach the left-sentential form w. At that point, all the symbols on
the stack have either been expanded (if they are variables) or matched against
the input (if they are terminals). The stack is empty, and we accept by empty
stack.

The above informal construction can be made precise as follows. Let G =
(V.T, Q, 5) be a CFG. Construct the PDA P that accepts L(G) by empty stack
as follows:

where transition function 6 is defined by:

1. For each variable A,
6(q,¢€, A) = {(¢, 8) | A 4 J is a production of G}

2. For each terminal a, 6(¢,a,a) = {(¢,€)}-

Example 6.12: Let us convert the expression grammar of Fig. 5.2 toa PDA.
Recall this grammar is:

Io + alb|Jfa| fb] I0| 71

The set of terminals for the PDA is {a,b,0,1,(,),+,*}. These eight symbols
and the symbols I and / form the stack alphabet. The transition function for
the PDA is:


--- Page 255 ---
6.3. EQUIVALENCE OF PDA’S AND CFG’S 239

a) 5(g,¢,2) = {(4,@), (@,8), (¢,fa), (¢, 78), (¢. 10), (@, 21)}-
b) 6(¢,€,£) = {(q, 1 (¢, E+E), (q,E*E), (4, (E))}.

c) 5(q, a, a) (las i (as }) = ak i 6(q,0,0) = {qe iB 6(q, 1,1)
igen = Teor

Note that (a) and (b) come from rule (1), while the eight transitions of (c) come
from rule (2). Also, 6 is empty except as defined by {a} through (c). O
Theorem 6.13: If PDA P is constructed from CFG G by the construction
above, then N(P} = L(G).

PROOF: We shall prove that w is in N(P) if and only if w is in L(G).

0,0);

(If} Suppose w is in L(G). Then w has a leftmost derivation
S=n1> Pe > sw
im im im

We show by induction on é that (q,w,S)F (q, yi,ai), where y; and a; are a
representation of the left-sentential form 7;. That is, let a; be the tail of -y,
and let 7; = z;0;-. Then y; is that string such that «iy; = wy; i-e., it is what
remains when x; is removed from the input.

BASIS: For? = 1,7, = S. Thus, 2; = ¢, and y, = w. Since (¢g,w, 5) F (g,w, S)
by 0 moves, the basis is proved.

INDUCTION: Now we consider the case of the second and subsequent left-
sentential forms. We assume

(q,w,S)F (q,yi,0%)

and prove (q,w, 5) F ( (¢; ¥ie1; @i¢1). Since a; is a tail, it begins with a variable
A. Morcover, the step of the derivation +; => 741 involves replacing A by one of
its production bodies, say 6. Rule (1) of the construction of P lets us replace A
at the top of the stack by 3, and rule (2) then allows us to match any terminals
on top of the stack with the next input symbols. As a result, we reach the ID
(9, ¥iz1, 441), Which represents the next left-sentential form yj41.

To complete the proof, we note that a, = «, since the tail of 7, (which is w)
is empty. Thus, (g¢,w, 5S) Ff (q,€,€), which proves that P accepts w by empty
stack,

(Only-if}) We need to prove something more general: that if P executes a se-
quence of moves that has the net effect of popping a variable A from the top of

its stack, without ever going below A on the stack, then A derives, in G, what-
ever input string was consumed from the input during this process. Precisely:

e If (q,2, A) F (q,€,€), then A > z.

The proof is an induction on the number of moves taken by P.


--- Page 256 ---
240 CHAPTER 6. PUSHDOWN AUTOMATA

BASIS: One move. The only possibility is that A — ¢ is a production of G, and
this production is used in a rule of type (1) by the PDA P. In this case, x = c,
and we know that A => e.

INDUCTION: Suppose FP takes 1 moves, where n > 1. The first move must be
of type (1), where A is replaced by one of its production bodies on the top of
the stack. The reason is that a rule of type (2) can only be used when there is a
terminal on top of the stack. Suppose the production used is 4 4 Y,¥2---Yé,
where each Y; is either a terminal or variable.

The next » — 1 moves of P must consume x from the input and have the
net eflect of popping each of ¥, ¥Y2, and so on from the stack, one at a time.
We can break z into 2122 ---2,, where x, is the portion of the input consumed
until ¥; is popped off the stack (i.e., the stack first is as short. as k - 1 symbols).
Then 2 is the next portion of the input that is consumed while popping Y2 off
the stack, and so on.

Figure 6.9 suggests how the input x is broken up, and the corresponding
effects on the stack.. There, we suggest that 8 was BaC,, so zx is divided into
three parts ©1943, where ro = a. Note that in general, if Y; is a terminal, then
x; must be that terminal.

B
a
Cc
x x x
1 2 3

Figure 6.9: The PDA P consumes z and pops BaC from its stack

Formally, we can conclude that (gq, 7;4:41 --- x, Yi) F (4, B41 ++? ER, €) for
all i = 1,2,...,4. Moreover, none of these sequences can be more than 7 — 1
moves, so the inductive hypothesis applies if Y; is a variable. That is, we may
conclude ¥; > z;. ,

If Y; is a terminal, then there must be only one move involved, and it matches
the one symbol of x; against Y;, which are the same. Again, we can conclude


--- Page 257 ---
6.3. EQUIVALENCE OF PDA’S AND CFG'S 241

Yi 4 zi; this time, zero steps are used. Now we have the derivation
A= ¥¥2---¥, 4 m1 ¥o°-:¥z 4 mae +, 2B Lp

That is, A og.

To complete the proof, we let A = 5 and « = w. Since we are given that
w is in N(P), we know that (q,w,S)F (g,¢,€). By what we have just proved
inductively, we have S > wyie., wisin L(G). O

6.3.2 From PDA’s to Grammars

Now, we complete the proofs of equivalence by showing that for every PDA P,
we can find a CFG G whose language is the same language that P accepts by
empty stack. The idea behind the proof is to recognize that the fundamental
event in the history of a PDA’s processing of a given input Is the net popping
of one symbol off the stack, while consuming some input. A PDA may change
state as it pops stack symbols, so we should aiso note the state that it enters
when it finally pops a level off its stack.

Pr
i a_i
al X5 X

Figure 6.10: A PDA makes a sequence of moves that have the net effect, of
popping a symbol off the stack

Figure 6.10 suggests how we pop a sequence of symbols ¥1, Y2,... Vx off the
stack. Some input. x, is read while Y, is popped. We should emphasize that
this “pop” is the net effect of (possibly} many moves. For example, the first
move may change Y; to some other symbol Z. The next move may replace Z
by UV, later moves have the effect of popping U, and then other moves pop V.


--- Page 258 ---
242 CHAPTER 6. PUSHDOWN AUTOMATA

The net effect is that Y; has been replaced by nothing; i.e., it has been popped,
and all the input symbols consumed so far constitute 21.

We also show in Fig. 6.10 the net change of state. We suppose that the PDA
starts out In state po, with Y; at the top of the stack. After all the moves whose
net effect is to pop Yi, the PDA is in state p,. It then proceeds to (net) pop
Yo, while reading input string 2 and winding up, perhaps after many moves,
in state pe with Yo off the stack. The computation proceeds until each of the
symbols on the stack is removed.

Our construction of an equivalent grammar uses variables each of which
represents an “event” consisting of:

1. The net popping of some symbol X from the stack, and

2. A change in state from some p at the beginning to q when X has finally
been replaced by € on the stack.

We represent such a variable by the composite symbol [pXg]. Remember that
this sequence of characters is our way of describing one variable; it is not five
grammar symbols. The formal construction is given by the next theorem.

Theorem 6.14: Let P = (Q, 2,1, 6, q0, Zo) bea PDA. Then there is a context-
free grammar G such that L(G) = N(P).

PROOF: We shall construct G = (V,Z,R,S), where the set of variables V
consists of:

1. The special symbol S, which is the start symbol, and

2. All symbols of the form [pXq], where p and gq are states in Q, and X isa
stack symbol, in T.

The productions of G are as follows:

a) For all states p, G has the production S > [go Zop]. Recall our intuition
that a symbol like [go Zop] is intended to generate all those strings w that
cause P to pop Zp from its stack while going from state gp to state p.
That is, (go, w, Zo) F (p, €,€). If so, then these productions say that start
symbol $ will generate all strings w that cause P to empty its stack, after
starting in its initial ID.

o
—

Let d(g,4,.X) contain the pair (r, ¥;¥2---¥,), where:

1. ais either a symbol in } or a =e.

2. & can be any number, including 0, in which case the pair is (r, €).

‘Then for all lists of states r1,r2,...,7%, G has the production

[gXrx] > afr¥iri][riYore] --- (rea Yere]


--- Page 259 ---
6.3. EQUIVALENCE OF PDA’S AND CFG’S 243

This production says that one way to pop X and go from state g to state
ry, is to read a (which may be ¢), then use some input to pop Yj off the
stack while going from state r to state r,, then read some more input that
pops Yo off the stack and goes from state 7; to rg, and so on.

We shall now prove that the informal interpretation of the variables [¢Xp] is
correct:

© (¢Xp| => w if and only if (g,w, X) F (p, €,€).

(If) Suppose (g, w, X) F (p,¢,e). We shall show [¢Xp] => w by induction on
the number of moves made by the PDA.

BASIS: One step. Then (p,e) must be in 6(g,w,X), and w is either a sin-
gle symbol or «. By the construction of G, [¢Xp] 4 w is a production, so
[¢Xp] > w.

INDUCTION: Suppose the sequence (g,w,X) F (p,€,€) takes n steps, and
n> 1. The first move must look like

(q,w, X) i (ro, 2, ¥1¥o--: Yz) F (p,€,€)

where w = az for some a that is either ¢ or a symbol in ©. It follows that the
pair (ro, Yi: Yo:-+Yx) must be in 6(g,a,X). Further, by the construction of G,
there is a production [¢Xr;,] > a[ro¥iri][ri Yare) +++ [rx-1¥ 47%), where:

lms Pp, and
2. 1,72,+.-,Tk—1 are any states in Q.

In particular, we may observe, as was suggested in Fig. 6.10, that each of
the symbols ¥1, Y2,..., ¥4 gets popped off the stack in turn, and we may choose
p; to be the state of the PDA when Y¥; is popped, for 7 = 1,2,. —1. Let
= WyWe::-w,, where w; is the input consumed while Y; is “ped off the
stack. Then we know that (ri-1, wi, ¥3) F (r;,€,€).

As none of these sequences of moves can take as many as m Moves, the
inductive hypothesis applies to them. We conclude that [ri-1Yiri] = w;. We
may put these derivations together with the first production used to conclude:

[gX rg] > alroYiri|[mYiral: + [Pk 1¥ers] =>
awn [Py Yare][r2¥3rs] «++ [re Yara] >
aw, wa[re¥ars] +++ (re—1¥ere] =>

Ay We 6° Wy, = WwW

where r, = p.

(Only-if) The proof is an induction on the number of steps in the derivation.


--- Page 260 ---
244 CHAPTER 6. PUSHDOWN AUTOMATA

BASIS: One step. Then [gp] > w must be a production. The only way for
this production to exist is if there is a transition of P in which X is popped
and state ¢ becomes state p. That is, (p,¢) must be in é(g,a,X}, and a = w.
But then (q,w,X)+ (p,€,€).

INDUCTION: Suppose [¢Xp] *. w by n steps, where n > 1. Consider the first
sentential form explicitly, which must look like

[aXrx] > afro¥iri][r Yara} ---[re—1 Vere] ow

where r, = p. This production must come from the fact that (ro, ¥, ¥a---¥;)
is in (g,a,X)}.

We can break w into w = aw,we--+w, such that [ri_1¥iri] = w; for all
4=1,2,...,%. By the inductive hypothesis, we know that for all 2,

(ri-1, Wi, Yi) F (ri, €,€)

If we use Theorem 6.5 to put the correct strings beyond w; on the input and
below ¥; on the stack, we also know that

x
(rina, Wiwigr + We YGVi41 Ye) F (ri, wig +++ we, Yin --* Ye)
If we put all these sequences together, we see that

(g, awy we --- wy, X) + (ro, wiwe +R, NiYa- VF
(ry, wows --- we, Yo¥a--- Ye) F (ro, wa --- wes Ve--- Ya) oF (re, 6)

Since rz = p, we have shown that (q,w,X)t (p,¢,€).

We complete the proof as follows. $ +. w if and only if [go Zop| = w for some
p, because of the way the rules for start symbol S are constructed. We just
proved that {qoZop] > w if and only if (¢,w, Zo) F (p,€,€), Le, if and only if
P accepts « by empty stack. Thus, L(G) = N(P). O

Example 6.15: Let us convert the PDA Py = ({g}, {t,e}, {2}, 6n,¢, Z) from
Example 6.10 to a grammar. Recall that Py accepts all strings that violate, for
the first time, the rule that every e (else) must correspond to some preceding
é (if). Since Py has only one state and one stack symbol, the construction is
particularly simple. There are only two variables in the grammar G:

a) S, the start symbol, which is in every grammar constructed by the method
of Theorem 6.14, and

b) [¢Zq], the only triple that can be assembled from the states and stack
symbols of Py.

The productions of grammar G are as follows:


--- Page 261 ---
6.3. EQUIVALENCE OF PDA’S AND CFG’S 245

1. The only production for S is S + {gZg]. However, if there were n states
of the PDA, then there would be n productions of this type, since the last
state could be any of the n states. The first state would have to be the
start state, and the stack symbol would have to be the start symbol, as
in our preduction above.

2. From the fact that da(q,i,Z)} contains (q,ZZ), we get the production
[@Zq] - i[gZq][qgZq]. Again, for this simple example, there is only one
production. However, if there were nm states, then this one rule would
produce 7? productions, since the middle two states of the body could be
any one state p, and the last states of the head and body could also be
any one state. That is, if p and r were any two states of the PDA, then
production [¢Zp] + i[gZr][rZp] would be produced.

3. From the fact that év(g,e,Z) contains (¢,¢), we have production
[gZq] > €

Notice that in this case, the list of stack symbols by which Z is replaced
is empty, so the only symbol in the body is the input symbol that caused
the move.

We may, for convenience, replace the triple [¢Zg] by some less complex
symbol, say A. If we do, then the complete grammar consists of the productions:

S4A
A+iAAle

In fact, if we notice that A and S derive exactly the same strings, we may
identify them as one, and write the complete grammar as

G = {5}, fi,e}, {S 2 iSS | e}, 8)

Oo

6.3.3 Exercises for Section 6.3

* Exercise 6.3.1: Convert the grammar

S 7 OSI|A
A > 140[Sle

to a PDA that accepts the same language by empty stack.
Exercise 6.3.2: Convert the grammar

S + a@AA
A > aS|bS|a


--- Page 262 ---
*

*

246 CHAPTER 6. PUSHDOWN AUTOMATA

to a PDA that accepts the same language by empty stack.

Exercise 6.3.3: Convert the PDA P = ({p,q}, {0,1},{X, Zo}, 4,¢, 29) to a
CFG, if 6 is given by:

1. 6(4,1, Zo) = {(¢, XZo)}.
2. 6(q,1,X) = {(¢, XX)}.
3. 5(g,0,X) = {(p, X)}.-
4, 8(¢,€,X) = {(¢,€)}-
5. d(p,1,X) = {(p, 6}}.-
6. d(p,0, Zo) = {(g, Zo)}.-
Exercise 6.3.4: Convert the PDA of Exercise 6.1.1 to a context-free grammar.

Exercise 6.3.5: Below are some context-free languages. For each, devise a
PDA that accepts the language by empty stack. You may, if you wish, first
construct a grammar for the language, and then convert to a PDA.

a) {a™b™c2"t™) | pn > 0, m > O}-
b) {atbick | i = 27 or j = 2k}.

Yo) {071" | n <m < Qn}.

! Exercise 6.3.6: Show that if P is a PDA, then there is a one-state PDA P,

such that N(P,\) = N(P).

Exercise 6.3.7: Suppose we have a PDA with s states, ¢ stack symbols, and
no rule in which a replacement stack string has length greater than u. Give a
tight upper bound on the number of variables in the CFG that we construct
for this PDA by the method of Section 6.3.2.

6.4 Deterministic Pushdown Automata

While PDA’s are by definition allowed to be nondeterministic, the determin-
istic subcase is quite important. In particular, parsers generally behave like
deterministic PDA’s, so the class of languages that can be accepted by these
automata is interesting for the insights it gives us into what constructs are
suitable for use in programming languages. In this section, we shall define
deterministic PDA’s and investigate some of the things they can and cannot
do.


--- Page 263 ---
6.4. DETERMINISTIC PUSHDOWN AUTOMATA 247

6.4.1 Definition of a Deterministic PDA

Intuitively, a PDA is deterministic if there is never a choice of move in any
situation, These choices are of two kinds. If 6(q,a,X)} contains more than one
pair, then surely the PDA is nondeterministic because we can choose among
these pairs when deciding on the next move. However, even if 6(g,a,X) is al-
ways a singleton, we could still have a choice between using a real input symbol,
or making a move on e. Thus, we define a PDA P = (Q,¥4,1',4, qo, 20, F) to
be deterministic (a deterministic PDA or DPDA), if and only if the following
conditions are met:

1. 5(g,a,X) has at most one member for any qin Q, a in & or a = €¢, and
X inT,

2. If 6(g,a,X) is nonempty, for some a in E, then d(g,€,.X) must be empty.

Example 6.16: It turns out that the language Luw, of Example 6.2 is a CFL
that has no DPDA. However, by putting a “center-marker” c in the middle, we
can make the language recognizable by a DPDA. That is, we can recognize the
language Lwews = {wew® | w isin (0+ 1)"} by a deterministic PDA.

The strategy of the DPDA is to store 0’s and L’s on its stack, until it sees
the center marker c. it then goes to another state, in which it matches input
symbols against stack symbols and pops the stack if they match. If it ever finds
a nonmatch, it dies; its input cannot be of the form wew®. If it succeeds in
popping its stack down to the initial symbol, which marks the bottom of the
stack, then it accepts its input.

The idea is very much like the PDA that we saw in Fig. 6.2. However, that
PDA. is nondeterministic, because in state go it always has the choice of pushing
the next input symbol onto the stack or making a transition on € to state qi;
i.e., it has to guess when it has reached the middle. The DPDA for Lycur is
shown as a transition diagram in Fig. 6.11.

This PDA is clearly deterministic. It never has a choice of move in the same
state, using the same input and stack symbol. As for choices between using a
real input symbol or ¢, the only ¢-transition it makes is from q to gz with Zo
at the top of the stack. However, in state q), there are no other moves when
Zq is at the stack top. O

6.4.2 Regular Languages and Deterministic PDA’s

The DPDA’s accept a class of languages that is between the regular languages
and the CFL’s. We shall first prove that the DPDA languages include all the
regular languages.

Theorem 6.17: If Z is a regular language, then £ = L(P) for some DPDA P.


--- Page 264 ---
248 CHAPTER 6. PUSHDOWN AUTOMATA

0,Z,)1/0Z,
1, Zg/lZo
0, 0/00
0, 1/01
1, 0/10 0,0/¢
1, 1/11 1

c, O/ O
ce, lf]

Figure 6.11: A deterministic PDA accepting Lycur

PROOF: Essentially, a DPDA can simulate a deterministic finite automaton.
The PDA keeps some stack symbol 2, on its stack, because a PDA has to have
a stack, but really the PDA ignores its stack and just uses its state. Formally,
let A = (Q,%,64,¢0,F) be a DFA. Construct DPDA

by defining ép(g,a,Z0) = {(p,2Zo)} for all states p and g in @, such that
da(q,@) = p. .

We claim that (go, w, Zo) E (p,€, Zo) if and only if d4(¢,w) = p. That is,
P simulates A using its state. The proofs in both directions are easy inductions
on |w|, and we leave them for the reader to complete. Since both A and P
accept by entering one of the states of F’, we conclude that their languages are
the same. O

If we want the DPDA to accept by empty stack, then we find that our
language-recognizing capability is rather limited. Say that a language L has
the prefiz property if there are no two different strings z and y in L such that
x is a prefix of y.

Example 6.18: The language Lieu; of Example 6.16 has the prefix property.
That is, it is not possible for there to be two strings wew” and zez®, one of
which is a prefix of the other, unless they are the same string. To see why,
suppose wew is a prefix of zea, and w # x. Then w must be shorter than
z. Therefore, the c in wew” comes in a position where zcx® has a 0 or 1; it is
a position in the first x. That point contradicts the assumption that wew* js
a prefix of zcr®.

On the other hand, there are some very simple languages that do not have
the prefix property. Consider {0}*, i.e., the set of all strings of 0’s. Clearly,


--- Page 265 ---
6.4. DETERMINISTIC PUSHDOWN AUTOMATA 249

there are pairs of strings in this language one of which is a prefix of the other,
so this language does not have the prefix property. In fact, of any two strings,
one is a prefix of the other, although that condition is stronger than we need
to establish that the prefix property does not hold. ©

Note that the language {0}* is a regular language. Thus, it is not even true
that every regular language is N(P) for some DPDA P. We leave as an exercise
the following relationship:

Theorem 6.19: A language L is N(P) for some DPDA P if and only if L has
the prefix property and f. is L(P’) for some DPDA P’. O

6.4.3. DPDA’s and Context-Free Languages

We have already seen that a DPDA can accept languages like Leecur that are
not regular. To see this language is not regular, suppose it were, and use the
pumping lemma. If n is the constant of the pumping lemma, then consider the
string w = 0"c0", which is in Lucwr- However, when we “pump” this string, it
is the first group of 0’s whose length must change, so we get in Lwcwr strings
that have the “center” marker not in the center. Since these strings are not in
Lwewr, We have a contradiction and conclude that Lwewr is not regular.

On the other hand, there are CFL’s like Lwwr that cannot be £(P) for any
DPDA P. A formal proof is complex, but the intuition is transparent. If P is
a DPDA accepting Lyw,, then given a sequence of 0’s, it must store them on
the stack, or do something equivalent to count an arbitrary number of Q’s. For
instance, it could store one X for every two 0’s it sees, and usc the state to
remember whether the number was even or odd.

Suppose P has seen n 0’s and then sees 110”. It -must verify that there
were 7 0’s after the 11, and to do so it must pop its stack.° Now, P has seen
0" 110". If it sees an identical string next, it must accept, becausc the complete
input is of the form ww, with w = 0"110". However, if it sees 0110 for
some m #n, P must not accept. Since its stack is empty, it cannot remember
what arbitrary integer n was, and must fail to recognize Luwr correctly. Our
conclusion is that:

e The languages accepted by DPDA’s by final state properly include the
regular languages, but are properly included in the CFL’s.
6.4.4 DPDA’s and Ambiguous Grammars

We can refine the power of the DPDA’s by noting that the languages they accept
ali have unambiguous grammars. Unfortunately, the DPDA languages are not

‘This statement is the intuitive part that requires a (hard) formal proof; could there be
some other way for P to compare equal blocks of 0's?


--- Page 266 ---
250 CHAPTER 6. PUSHDOWN AUTOMATA

exactly equal to the subset of the CFL’s that are not inherently ambiguous.
For instance, Lyy, has an unambiguous grammar

S +050} 181] €

even though it is not a DPDA language. The following theorems refine the
bullet point above.

Theorem 6.20: If L = N(P)} for some DPDA P, then L has an unambiguous
context-free grammar.

PROOF: We claim that the construction of Theorem 6.14 yields an unambiguous
CFG G when the PDA to which it is applied is deterministic. First recall from
Theorem 5.29 that it is sufficient to show that the grammar has unique leftmost
derivations in order to prove that G is unambiguous.

Suppose P accepts string w by empty stack. Then it does so by a unique
sequence of moves, because it is deterministic, and cannot move once its stack
is empty. Knowing this sequence of moves, we can determine the one choice of
production in a leftmost derivation whereby G derives w. There can never be a
choice of which rule of P motivated the production to use. However, a rule of
P, say (g,a,X) = {(r, 4 ¥o---¥,)} might cause many productions of G, with
different states in the positions that reflect the states of P after popping each
of ¥1,¥2,..-,¥g-1. Because P is deterministic, only one of these sequences of
choices will be consistent with what P actually does, and therefore, only one of
these productions will actually lead to derivation of w. O

However, we can prove more: even those languages that DPDA’s accept by
final state have unambiguous grammars. Since we only know how to construct
grammars directly from PDA’s that accept by empty stack, we need to change
the language involved to have the prefix property, and then modify the resulting
grammar to generate the original language. We do so by use of an “endmarker”
symbol.

Theorem 6.21: If £ = L(P) for some DPDA P, then Z has an unambiguous
CFG.

PROOF: let $ be an “endmarker” symbol that does not appear in the strings of
L, and let £' = L$. That is, the strings of L’ are the strings of L, each followed
by the symbol §. Then £’ surely has the prefix property, and by Theorem 6.19,
L/ = N(P") for some DPDA P’.® By Theorem 6.20, there is an unambiguous
grammar G' generating the language N(P'), which is L’.

Now, construct from G’ a grammar G such that L(G) = L. To do sa, we
have only to get. rid of the endmarker $ from strings. Thus, treat $ as a variable

®The proof of Theorem 6.19 appears in Exercise 6.4.3, but we can easily see how to
construct P’ from P,. Add a new state q that P’ enters whenever P is in an accepting state
and the next input is $. In state ¢, P’ pops all symbols off its stack. Also, P’ needs its own
bottom-of-stack marker to avoid accidentally emptying its stack as it simulates P.


--- Page 267 ---
6.4. DETERMINISTIC PUSHDOWN AUTOMATA 251

of G, and introduce production $ — ¢; otherwise, the productions of G’ and G
are the same. Since L(G’) = L’, it follows that L(G) = L.

We claim that G is unambiguous. In proof, the leftmost derivations in G' are
exactly the same as the leftmost derivations in G’, except that the derivations
in G have a final step in which $ is replaced by e. Thus, if a terminal w string
had two leftmost derivations in G, then w$ would have two leftmost derivations
in G’. Since we know G' is unambiguous, soisG. O

6.4.5 Exercises for Section 6.4

Exercise 6.4.1: For each of the following PDA’s, tell whether or not it is
deterministic. Either show that it meets the definition of a DPDA or find a
rule or rules that violate it.

a) The PDA of Example 6.2.
* b) The PDA of Exercise 6.1.1.
c) The PDA of Exercise 6.3.3.

Exercise 6.4.2: Give deterministic pushdown automata to accept the follow-
ing languages:

a) {0"1" |n<m}.
b) {0"1™ | x > m}.
c) {0"1"0" | n and m are arbitrary}.
Exercise 6.4.3: We can prove Theorem 6.19 in three parts:
* a) Show that if L = N(P) for some DPDA P, then E.has the prefix property.

1b) Show that if L = N(P) for some DPDA P, then there exists a DPDA P’
such that L = L(P’).

*1c) Show that if Z has the prefix property and is L(P’) for some DPDA P’,
then there exists a DPDA P such that L = N(P).

1! Exercise 6.4.4: Show that the language

E={0"1l"|n>1}u {071" | n> 1}

is a context-free language that is not accepted by any DPDA. Hint: Show that
there must be two strings of the form 0"1" for different values of n, say n, and
ng that cause a hypothetical DPDA for Z to enter the same ID after reading
both strings. Intuitively, the DPDA must erase from its stack almost everything
it placed there on reading the 0’s, in order to check that it has seen the same
number of 1’s. Thus, the DPDA cannot tell whether or not to accept next after
seeing 7 1's or after seeing na 1’s.


--- Page 268 ---
252

6.5
+

CHAPTER 6. PUSHDOWN AUTOMATA

Summary of Chapter 6

Pushdown Automate: A PDA is a nondeterministic finite automaton cou-
pled with a stack that can be used to store a string of arbitrary length.
The stack can be read and modified only at its top.

Moves of a Pushdown Automate: A PDA chooses its next move based
on its current state, the next input symbol, and the symbol at the top
of its stack. It may also choose to make a move independent of the
input symbol and without consuming that symbol from the input. Being
nondeterministic, the PDA may have some finite number of choices of
move; each is a new state and a string of stack symbols with which to
replace the symbol currently on top of the stack.

Acceptance by Pushdown Automata: There are two ways in which we
may allow the PDA to signal acceptance. One is by entering an accepting
state; the other. by emptying its stack. These methods are equivalent, in
the sense that any language accepted by one method is accepted (by some
other PDA) by the other method.

instantaneous Descriptions: We use an ID consisting of the state, re-
maining input, and stack contents to describe the “current condition” of
a PDA. A transition function + between ID’s represents single moves of
a PDA.

Pushdown Automata and Grammars: The languages accepted by PDA’s
either by final state or by empty stack, are exactly the context-free lan-
pages.

Deterministic Pushdown Automata: A PDA is deterministic if it never
has a choice of move for a given state, input symbol (including «), and
stack symbol. Also, it never has a choice between making a move using a
true input and a move using « input.

Acceptance by Deterministic Pushdown Automate: The two modes of ac-
ceptance — final state and empty stack — are not the same for DPDA’s.
Rather, the languages accepted by empty stack are exactly those of the
languages accepted by final state that have the prefix property: no string
in the language is a prefix of another word in the language.

The Languages Accepted by DPDA’s: All the regular languages are ac-
cepted (by final state) by DPDA’s, and there are nonregular languages
accepted by DPDA’s. The DPDA languages are context-free languages,
and in fact are languages that have unambiguous CFG’s. Thus, the DPDA
languages lie strictly between the regular languages and the context-free
languages.


--- Page 269 ---
6.6. REFERENCES FOR CHAPTER 6 253

6.6 References for Chapter 6

The idea of the pushdown automaton is attributed independently to Oettinger
[4] and Schutzenberger [5]. The equivalence between pushdown automata and
context-free languages was also the result of independent discoveries; it appears
in a 1961 MIT technical report by N. Chomsky but was first published by Evey
[1].
The deterministic PDA was first introduced by Fischer [2] and Schutzen-
berger [3]. It gained significance later as a model for parsers. Notably, [3]
introduces the “LR(k) grammars,” a subclass of CFG’s that generate exactly
the DPDA languages. The LR(k) grammars, in turn, form the basis for YACC,
the parser-generating tool discussed in Section 5.3.2.

1. J. Evey, “Application of pushdown store machines,” Proc. Fall Joint Com-
puter Conference (1963), AFIPS Press, Montvale, NJ, pp. 215-227.

2. P. C. Fischer, “On computability by certain classes of restricted Turing
machines,” Proc. Fourth Anni. Symposium on Switching Circuit Theory
and Logical Design (1963), pp. 23-32.

3. D. E. Knuth, “On the translation of languages from left to right,” Infor-
mation and Control 8:6 (1965), pp. 607-639.

4. A. G. Octtinger, “Automatic syntactic analysis and the pushdown store,”
Proc. Symposia on Applied Math. 12 (1961), American Mathematical
Society, Providence, RI.

5. M. P. Schutzenberger, “On context-free languages and pushdown au-
tomata,” Information and Control 6:3 (1963), pp. 246-264.


--- Page 270 ---


--- Page 271 ---
Chapter 7

Properties of Context-Free
Languages

We shall complete our study of context-free languages by learning some of
their properties. Our first task is to simplify context-free grammars; these
simplifications make it easier to prove facts about CFL’s, since we can claim
that if a language is a CFL, then it has a grammar in some special form.

We then prove a “pumping lemma” for CFL’s. This theorem is in the
same spirit as Theorem 4.1 for regular languages, but can be used to prove
a language not to be context-free. Next, we consider the sorts of properties
that we studied in Chapter 4 for the regular languages: closure properties and
decision properties. We shall see that some, but not all, of the closure properties
that the regular languages have are also possessed by the CFL’s. Likewise, some
questions about CFL’s can be decided by algorithms that generalize the tests
we developed for regular languages, but there are also certain questions about
CFL's that we cannot answer.

7.1 Normal Forms for Context-Free Grammars

The goal of this section is to show that every CFL (without €) is generated by a
CFG in which all productions are of the fourm 4 4 BC or A > a, where A, B,
and C are variables, and a is a terminal. This form is called Chomsky Normal
Form. To get there, we need to make a number of preliminary simplifications,
which are themselves useful in various ways:

1. We must eliminate useless symbols, those variables or terminals that do
not appear in any derivation of a terminal string from the start symbol.

2, We must eliminate e-productions, those of the form A — « for some vari-
able A.

250


--- Page 272 ---
256 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

3. We must eliminate unit productions, those of the form 4A — #& for variables
A and B.

7.1.1 Eliminating Useless Symbols

We say a symbol X is useful for a grammar G = (V,T,P,S) if there is some
derivation of the form S$ > aX fs 3 w, where wisin T*. Note that X may be
in either V or 7, and the sentential form aX might be the first or last in the
derivation. If X is not useful, we say it is useless. Evidently, omitting useless
symbols from a grammar will not change the language generated, so we may as
well detect and eliminate all useless symbols.

Our approach to eliminating useless symbols begins by identifying the two
things a symbol has to be able to do to be useful:

1. We say X is generating if X +. w for some terminal string w. Note that
every terminal is generating, since w can be that terminal itself, which is
derived by zero steps.

2. We say X is reachable if there is a derivation S > aX for some a and

8.

Surely a symbol that is useful will be both generating and reachable. If we
eliminate the symbols that are not generating first, and then eliminate from
the remaining grammar those symbols that are not reachable, we shall, as will
be proved, have only the useful symbols left.

Example 7.1: Consider the grammar:

S—>ABla
Ab

All symbols but B are generating; a and b generate themselves; S generates
a, and A generates 6. If we eliminate B, we must eliminate the production
S > AB, leaving the grammar:

Sa
Asoo

Now, we find that only S and a are reachable from S. Eliminating A and
b leaves only the production S 4 a. That production by itself is a grammar
whose language is {a}, just as is the language of the original grammar.

Note that if we start by checking for reachability first, we find that all
symbols of the grammar

S—+AB|a
Ab


--- Page 273 ---
7.1, NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 257

are reachable. If we then eliminate the symbol B because it is not generating,
we are left with a grammar that still has useless symbols, in particular, 4 and
& O

Theorem 7.2: Let G = (V,T,P,S) be a CFG, and assume that L(G) ¥ 9;
i.e., G generates at least one string. Let Gy = (Wi,T;,P1,9) be the grammar
we obtain by the following steps:

1. First eliminate nongenerating symbols and all productions involving one
or more of those symbols. Let Gy = (Vo, Ts, Pe, $) be this new grammar.
Note that S must be generating, since we assume L(G) has at least one
string, so S has not been eliminated.

2. Second, eliminate ail symbols that are not reachable in the grammar G2.

Then G; has no useless symbols, and L(G,) = L(G).

PROOF: Suppose X is a symbol that remains; i.e., X is in ¥; UT). We know
that 4 > w for some w in T*. Moreover, every symbol used in the derivation

of w from X is also generating. Thus, X > w.

Since X was not eliminated i in the second step, we also know that there are
a and @ such that $ > aX. Further, every symbol used in this derivation is
reachable, so 5 2 aX 6.

We know that every symbol in aX is reachable, and we also know that
all these syrnbols are in V2 U JT, so each of them is generating in Go. The
derivation of some terminal string, say aX = zwy, involves only symbols

that are reachable from 5S, because they are reached by symbols in aX 8. Thus,
this derivation is also a deriv ation of G1; that is,

S23 aXP cw
a * Be swy

We conclude that X is useful in G,. Since X is an arbitrary symbol of Gi, we
conclude that G, has no useless symbols.

The last detail is that we must show L(G,) = L(G). As usual, to show two
sets the same, we show each is contained in the other.

L(G) € £(G): Since we have only eliminated symbols and productions from
G to get G1, it follows that L(G,) ¢ L(G).

E(G) C L(G): We must prove that if w is in L(G), then w is in L(G).
wis in Z(G), then $ > w. Each symbol in this derivation is evidently both

reachable and generating, so it is also a derivation of G,. That is, $ = w, and
thus wisin L(G). O


--- Page 274 ---
258 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

7.1.2 Computing the Generating and Reachable Symbols

Two points remain. How do we compute the set of generating symbols of a
grammar, and how do we compute the set of reachable symbols of a grammar?
For both problems, the algorithm we use tries its best to discover symbols of
these types. We shall show that if the proper inductive constructions of these
sets fails to discover a symbol to be generating or reachable, respectively, then
the symbol is not of these types.

Let G = (V,T, P,S} be a grammar. To compute the generating symbols of
G, we perform the following induction.

BASIS: Every symbol of T is obviously generating: it generates itself.

INDUCTION: Suppose there is a production A — a, and every symbol of a
is already known to be generating. Then A is generating. Note that this rule
includes the case where a = ¢; all variables that have ¢ as a production body
are surely generating.

Example 7.3: Consider the grammar of Example 7.1. By the basis, a and }
are generating. For the induction, we can use the production A + 6 to conclude
that A is generating, and we can use the production S > a to conclude that
S is generating. At that point, the induction is finished. We cannot use the
production S — AB, because B has not been established to be generating.
Thus, the set of generating symbols is {a,b,A,S}. O

Theorem 7.4: The algorithm above finds all and only the generating symbols
of G.

PROOF: For one direction, it is an easy induction on the order in which symbols
are added to the set of generating symbols that each symbol added really is
generating. We leave to the reader this part of the proof.

For the other direction, suppose X is a generating symbol, say X = wy.

We prove by induction on the length of this derivation that X is found to be
generating.

BASIS: Zero steps. Then X is a terminal, and X is found in the basis.

INDUCTION: If the derivation takes n steps for n > 0, then X is a variable.
Let the derivation be X > a >> w; that is, the first production used is X > a.
Each symbol of a derives some terminal string that is a part of w, and that
derivation must take fewer than n steps. By the inductive hypothesis, each
symbol of a is found to be generating. The inductive part of the algorithm
allows us to use production X > a to infer that X is generating. O

Now, let us consider the inductive algorithm whereby we find the set of
reachable symbols for the grammar G = (V,T, P,S). Again, we can show that
by trying our best to discover reachable symbols, any symbol we do not add to
the reachable set is really not reachable.


--- Page 275 ---
7.1. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 259

BASIS: 5 is surely reachable.

INDUCTION: Suppose we have discovered that some variable A is reachable.
Then for all productions with A in the head, all the symbols of the bodies of
those productions are also reachable.

Example 7.5: Again start with the grammar of Example 7.1. By the basis,
S is reachable. Since S has production bodies AB and a, we conclude that
A, B, and a are reachable. B has no productions, but A has A — b. We
therefore conclude that 6 is reachable. Now, no more symbols can be added to
the reachable set, which is {S,A,B,a,b}. 0

Theorem 7.6: The algorithm above finds all and only the reachable symbols
of G.

PROOF: This proof is another pair of simple inductions akin to Theorem 7.4.
We leave these arguments as an exercise. O

7.1.3. Eliminating e-Productions

Now, we shall show that ¢-productions, while a convenience in many grammar-
design problems, are not essential. Of course without a production that has
an € body, it is impossible to generate the empty string as a member of the
language. Thus, what we actually prove is that if language L has a CFG, then
L — {ce} has a CFG without «productions. If ¢ is not in L, then J itself is
L — {ce}, so LE has a CFG with out e-productions.

Our strategy is to begin by discovering which variables are “nullable.” A
variable A is nullable if A > ce. If A is nullable, then whenever A appears in
a production body, say B + CAD, A might (or might not) derive «. We make
two versions of the production, one without A ini the body (B 4 CD), which
corresponds to the case where A would have been used to derive ¢, and the
other with A still present (B + CAD). However, if we use the version with A
present, then we cannot allow A to derive ¢. That proves not to be a problem,
since we shall simply eliminate all productions with ¢ bodies, thus preventing
any variable from deriving e.

Let G = (V,T, P,S) be a CFG. We can find all the nullable symbols of G by
the following iterative algorithm. We shall then show that there are no nullable
symbols except what. the algorithm finds.

BASIS: If A > ¢ is a production of G, then A is nullable.

INDUCTION: If there is a production B > C,C2---Cx, where each C; is
nullable, then B is nullable. Note that each C; must be a variable to be nullable,
so we only have to consider productions with all-variable bodies.

Theorem 7.7: In any grammar G, the only nullable symbols are the variables
found by the algorithm above.


--- Page 276 ---
260 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

PROOF: For the “if” direction of the implied “A is nullable if and only if
the algorithm identifies A as nullable,” we simply observe that, by an easy
induction on the order in which nullable symbols are discovered, that the each
such symbol truly derives «. For the “only-if” part, we can perform an induction
on the length of the shortest derivation A> e.

BASIS: One step. Then A — e must be a production, and A is discovered in
the basis part of the algorithm.

INDUCTION: Suppose A > ¢ by nm steps, where n > 1. The first step must
look like A > C\Co---C, 5 €, where each C; derives « by a sequence of
fewer than nm steps. By the inductive hypothesis, each C; is discovered by
the algorithm to be nullable. Thus, by the inductive step, A, thanks to the
production A + C1C.---C,, is found te be nullable. O

Now we give the construction of a grammar without e-productions. Let
G = (V,T,P,S) be a CFG. Determine all the nullable symbols of G. We
construct a new grammar Gj = (V,T,P,,S5), whose set of productions P, is
determined as follows.

For each production A + X1X2---X, of P, where k > 1, suppose that m
of the & X;,’s are nuilable symbols, The new grammar G, will have 2” versions
of this production, where the nullable X;’s, in all possible combinations are
present or absent. There is one exception: if m = &, i.e., all symbols are
nullable, then we do not include the case where all X,’s are absent. Also, note
that if a production of the form A — ¢ is in P, we do not place this production
in Py.

Example 7.8: Consider the grammar

SAB
A7aAAje
B-+bBB|e

First, let us find the nullable symbols. A and B are directly nullable because
they have productions with € as the body. Then, we find that S$ is nullable,
because the production S — AB has a body consisting of nullable symbols only.
Thus, ali three variables are nullable.

Now, let us construct the productions of grammar G,. First consider
S— AB. All symbols of the body are nullable, so there are four ways we
could choose present or absent for A and B, independently. However, we are
not allowed to choose to make all symbols absent, so there are only three pro-
ducticns:

S->AB|[A|B

Next, consider production A — aAA. The second and third positions hold
nullable symbols, so again there are four choices of present /absent. In this case,


--- Page 277 ---
74. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 261

all four choices are allowable, since the nonnullable symbol a will be present in
any case. Our four choices yield productions:

A >aAA|aAd|aAl|a

Note that the two middle choices happen to yield the same production, since it

doesn’t matter which of the A’s we eliminate if we decide to eliminate one of

them. Thus, the final grammar G will only have three productions for A,
Similarly, the production B yields for G1:

B+ bBB|bB|b

The two e-productions of G yield nothing for G;. Thus, the following produc-
tions:

S73 AB|A|B

AwaAAjaA|a

BobBB|bB|b

constitute G,;. O

We conclude our study of the elimination of e-productions by proving that
the construction given above does not change the language, except that € is no
longer present if it was in the language of G. Since the construction obviously
eliminates ¢-productions, we shail have a complete proof of the claim that for
every CFG G, there is a grammar G, with no é-productions, such that

L{G,) = L(G) - {e}
Theorem 7.9: If the grammar G, is constructed from G by the above con-
struction for eliminating e-productions, then L(Gi) = L(G) — {e}-

PROOF: We must show that if w # €, then wis in L(G,) if and only if w
is in L(G). As is often the case, we find it easier to prove a more general
statement. In this case, we need to talk about the terminal strings that each
variable generates, even though we only care what the start symbol 5 generates.
Thus, we shall prove:

e A> w if and only if A > wand w F €.

Gi
In each case, the proof is an induction on the length of the derivation.

(Only-if) Suppose that A = w. Then surely w 4 €, because G, has no ¢-
aL
productions. We must show by induction on the length of the derivation that
*
A> w.
G
BASIS: One step. Then there is a production A > w in Gi. The construction

of G, tells us that there is some production A — a of G, such that a is w, with
gero or more nullable variables interspersed. Then in G, A 2 a => w, where

the steps after the first, if any, derive « from whatever variables there are in a.


--- Page 278 ---
262 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

INDUCTION: Suppose the derivation takes n > 1 steps. Then the derivation
looks like A > XX2-++Xy = w. The first production used must come from

1
a production A 3 ¥,¥-- LY,,. where the Y’s are the X’s, in order, with zero
or more additional, nullable variables interspersed. Also, we can break w into
Wy We We, where XY = w, for? = 1,2,...,%. If X; is a terminal, then

w; = X;, and if Xj is a variable, then the derivation X; = w, takes fewer than
1

n steps. By the inductive hypothesis, we can conclude X; > w;.

Now, we construct a corresponding derivation in G as follows:

A> Viko---Ym > XXq- Ny S wiwe---, = w
G G G

The first step is application of the production 4 3 Y)Yo:--Y, that we know
exists in G, The next group of steps represents the derivation of € from each
of the Y;’s that is not one of the X;’s. The final group of steps represents the
derivations of the w;,’s from the X;’s, which we know exist by the inductive
hypothesis.
(If} Suppose A > wand w #€. We show by induction on the iength n of the

derivation, that A > w.

ry
BASIS: One step. Then A > w is a production of G. Since w # ¢, this
production is also a production of G,, and A > w.

INDUCTION: Suppose the derivation takes n > 1 steps. Then the derivation
looks like 4 > Wk: Kn m= w. We can break w = wy1w2---wm, such that

Y; > w; for? =1,2,...,m. Let X,,Xo,...,X, be those of the Y; 73'S, in order,

G
such that w; # «. We must have k > 1, since w # «. Thus, A> XXp-- Xp
is a production of G,. .

We claim that X)X2---X;, > w, since the only Y;’s that are not present

among the X's were used to derive ¢, and thus do not contribute to the deriva-
: : . . *
tion of w. Since each of the derivations Yj a wi takes fewer than n steps, we

may apply the inductive hypothesis and conclude that, if w; A e, then ¥; > Wy.
Thus, A 4 XX Xp > Ww.

Now, we ‘complete the onoot as follows. We know w is in Z(G,) if and only if
Ss > w. Letting A = S in the above, we know that w is in L(G1) if and only

if $ > wand w 4c. That is, w is in L(G,) if and only if w is in L(G) and
we O

7.1.4 Eliminating Unit Productions

A unit production is a production of the form A — B, where both A and B are
variables. These productions can be useful. For instance, in Example 5.27, we


--- Page 279 ---
7.1. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 263

saw how using unit productions E 4 T and T -> F allowed us to create an
unambiguous grammar for simple arithmetic expressions:

lf -> al|b|fa|fb| 70| 71
F -+ | (&)

t + F|T4«F

Eo T\|£H+T

However, unit productions can complicate certain proofs, and they also in-
troduce extra steps into derivations that technically need not be there. For
instance, we could expand the T in production £& + T in both possible ways,
replacing it by the two productions # 4 F | T+ F. That change still doesn’t
eliminate unit productions, because we have introduced unit production F > F
that was not previously part of the grammar. Further expanding E — F by
the two productions for F' gives us B + [| (£) | T+ F. We still have a unit
production; it is # 4 f. But if we further expand this J in all six possible ways,
we get

E-+alb|fa|ib|f0| 71 | (BE) |TeF

Now the unit production for E is gone. Note that E - a is not a unit
production, since the lone symbol in the body is a terminal, rather than a
variable as is required for unit productions.

The technique suggested above — expand unit productions until] they disap-
pear — often works. However, it can fail if there is a cycle of unit productions,
such as A + B, B 3 C,and C > A. The technique that is guaranteed to work
involves first finding all those pairs of variables 4 and B such that 4 => B us-
ing a sequence of unit productions only. Note that it is possible for 4 > B to
be true even though no unit productions are involved. For instance, we might
have productions A > BC and C 7 e«.

Once we have determined all such pairs, we can replace any sequence of
derivation steps in which A > B) > B. > --- > B, => a by a production
that uses the nonunit production B, 4 a directly from A; that is, A> a. To
begin, here is the inductive construction of the pairs (A,B) such that AS B
using only unit productions. Cali such a pair a wnit pair.

BASIS: (4, A) is a unit pair for any variable A. That is, A > A by zero steps.

INDUCTION: Suppose we have determined that (A,B) is a unit pair, and
B - C is a production, where C is a variable. Then (A, C) is a unit pair.

Example 7.10: Consider the expression grammar of Example 5.27, which we
reproduced above. The basis gives us the unit pairs (£, £), (7,7), (F, F), and
(7,1). For the inductive step, we can make the following inferences:

1. (EZ, £) and the production £ - T gives us unit pair (£,T).
2. (E,T)} and the production T - F gives us unit pair (£, F).
3. (F,F) and the production F - J gives us unit pair (£, J).


--- Page 280 ---
264 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

4. (1,T) and the production T > F gives us unit pair (TF).
5. (T, F) and the production F' > I gives us unit pair (T, I).
6. (FF) and the production F + I gives us unit pair (F, J).

There are no more pairs that can be inferred, and in fact these ten pairs repre-
sent all the derivations that use nothing but unit productions. O

The pattern of development should by now be familiar, There is an easy
proof that our proposed algorithm does get all the pairs we want. We then use
the knowledge of those pairs to remove unit productions from a grammar and
show that the language of the two grammars is the same.

Theorem 7.11: The algoritlum above finds exactly the unit pairs for a CFG
G.

PROOF: In one direction, it is an easy induction on the order in which the pairs
are discovered, that if (A,B) is found to be a unit pair, then A >? B using

only unit productions. We leave this part of the proof to you.
In the other dircction, suppose that A > B using unit productions only.

We can show by induction on the length of the derivation that the pair (A, B)
will be found.

BASIS: Zero steps. Then 4 = B, and the pair (A, B) is added in the basis.

INDUCTION: Suppose 4 => B using n steps, for some n > 0, each step being
the application of a unit. production. Then the derivation looks like

ASC=3B

The derivation A > C takes n— 1 steps, so by the inductive hypothesis, we
discover the pair (A,C’). Then the inductive part of the algorithm combines
the pair (A, C) with the production C + B to infer the pair (A,B). 0

To eliminate unit productions, we proceed as follows. Given a CFG G =
(V,T, P, S$), construct CFG Gi = (V,T, Pi, $):

1. Find all the unit pairs of G.

2. For each unit pair (A,B), add to P, all the productions A > a, where
B — «is a nonunit production in P. Note that A = B is possible; in
that way, P,; contains all the nonunit productions in P.

Example 7.12: Let us continue with Example 7.10, which performed step (1)
of the construction above for the expression grammar of Example 5.27. Fig-
ure 7.1 summarizes step (2) of the algorithm, where we create the new set of
productions by using the first member of a pair as the head and ail the nonunit
bodies for the second member of the pair as the production bodies.

The final step is to eliminate the unit productions from the grammar of
Fig. 7.1. The resulting grammar:


--- Page 281 ---
7.1. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 265

Pair Productions

(E,E)|EOE+T

(E,T) | EoT*F

(E,F) | E(B)

(E,1) | E>o|b6|Ia| fb| f0| 1
(T,T) | ToT«F

(T,F) | T(E)

(7.1) | Tal b|ia| Ib) 10| 11
(FF) | F > (EB)

(F.1D) | F3a|6| ial Io| 70| 71
UI) | I>a|o|fa| Jo) 10) Ji

Figure 7.1: Grammar constructed by step (2) of the unit-production-elimination
algorithm

EBEOE+T|T+F\|(8)|a]b|le| Ib) 10/1
T3T«F|(E)la]|Ja| ib| 10} Ti
F+(E)|a|b| Ja] 16} £0{ 71
Taltb|Ia]Ib| 10| fi

has no unit productions, yet generates the same set of expressions as the gram-
mar of Fig. 5.19. DO

Theorem 7.13: If grammar G, is constructed from grammar G by the algo-
rithm described above for eliminating unit productions, then L(G) = L(G).

PROOF: We show that w is in L(G) if and only if w is in L(G).

(If) Suppose S > w. Since every production of G; is equivalent to a sequence

of zero or more “anit productions of G followed by a nonunit production of G,
we know that a > & implies a > 3. That is, every step of a derivation in Gy

can be replaced by one or more derivation steps in G. If we put these sequences
of steps together, we conclude that S > uh

(Only-if} Suppose now that w is in LC) Then by the equivalences in Sec-
tion 5.2, we know that w has a leftmost derivation, i-c., 5 = w. Whenever a

unit production is used in a leftmost derivation, the variable of the body be-
comes the leftmost variable, and so is iminediately replaced. Thus, the leftmost,
derivation in grammar G can be broken into a sequence of steps in which zero
or more unit productions are followed by a nonunit production. Note that any
nonunit production that is not preceded by a unit production is a “step” by
itself. Each of these steps can be performed by one production of G1, because
the construction of G, created exactly the productions that reflect zero or more
unit productions followed by a nonunit production. Thus, 3 2 w. O


--- Page 282 ---
266 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

We can now summarize the various simplifications described so far. We want.
to convert any CFG G into an equivalent CFG that has no useless symbols,
€-productions, or unit productions. Some care must be taken in the order of
application of the constructions. A safe order is:

1. Eliminate e-productions.
2. Eliminate unit productions.
3. Eliminate useless symbols.

You should notice that, just as in Section 7.1.1, where we had to order the
two steps properly or the result might have useless symbols, we must order the
three steps above as shown, or the result might still have some of the features
we thought we were eliminating.

Theorem 7.14: If Gis a CFG generating a language that contains at least one
string other than e, then there is another CFG G; such that L(Gi) = L(G)—{e},
and G, has no e-productiohs, unit productions, or useless symbols.

PROOF: Start by eliminating the e-productions by the method of Section 7.1.3.
If we then eliminate unit productions by the method of Section 7.1.4, we do
not introduce any ¢-productions, since the bodies of the new productions are
each identical to some body of an old production. Finally, we eliminate useless
symbols by the method of Section 7.1.1. As this transformation only eliminates
productions and symbols, never introducing a new production, the resulting
grammar will still be devoid of «-productions and unit productions. O

7.1.5 Chomsky Normal Form

We complete our study of grammatical simplifications by showing that every
nonempty CFL without ¢ has a grammar G in which all productions are in one
of two simple forms, either:

1. A- BC, where A, B, and C, are each variables, or
2. A— a, where A is a variable and a is a terminal.

Further, G has no useless symbols. Such a grammar is said to be in Chomsky
Normal Form, or CNF.1

To put a grammar in CNF, start with one that. satisfies the restrictions of
Theorem 7.14; that is, the grammar has no e-productions, unit productions,
or useless symbols. Every production of such a grammar is either of the form
A — a, which is already in a form allowed by CNF, or it has a body of length
2 or more. Our tasks are to:

'N. Chomsky is the linguist who first proposed context-free grammars as a way to de-
scribe natural languages, and who proved that every CFG could be converted to this form.
Interestingly, CNF does not appear to have important uses in natural linguistics, although
we shall see it has several other uses, such as an efficient test for membership of a string in a
context-free language (Section 7.4.4).


--- Page 283 ---
7.1. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 267

a) Arrange that all bodies of length 2 or more consist only of variables.

b) Break bodies of length 3 or more into a cascade of productions, each with
a body consisting of two variables.

The construction for (a) is as follows. For every terminal @ that appears in
a body of length 2 or more, create a new variable, say A. This variable has only
one production, A + a. Now, we use A in place of a everywhere a appears in
a body of length 2 or more. At this point, every production has a body that. is
either a single terminal or at least two variables and no terminals.

For step (b), we must break those productions A ~> B,Bo---B,, for k > 3,
into a group of productions with two variables in each body. We introduce
k — 2 new variables, C,,C2,...,C,—-2. The original production is replaced by
the & — 1 productions

A+ BiCy, Cy > BeCo,...,Ce-3 > Bu-2Cy—-2, Cyn 4+ Bei By

Example 7.15: Let us convert the grammar of Example 7.12 to CNF. For
part (a), notice that there are eight terminals, a, 5, 0,1, +, *, (, and ), each of
which appears in a body that is not a single terminal. Thus, we must introduce
eight new variables, corresponding to these terminals, and eight productions in
which the new variable is replaced by its terminal. Using the obvious initials
as the new variables, we introduce:

Avtvya Boab £230 O--O!1
P++ Mos Eat R>}

If we introduce these productions, and replace every terminal in a body that is
other than a single terminal by the corresponding variable, we get the grammar
shown in Fig. 7.2. .

EPT |TMP|LER|a\|b|IA|IB|IZ|I0
TMF |LER|a|6|fA|IB|1IZ|I0
LER|a|b|IA|IB|IZ|IO
a|b|IA|IB|£Z| Io

Moe VON WAS yD By
tLltltlisllisvidd
——~ He 1m OO 8

Figure 7.2: Making all bodies either a single terminal or several variables


--- Page 284 ---
268 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

Now, all productions are in Chomsky Normal Form except for those with
the bodies of length 3: EPT, TMF, and LER. Some of these bodies appear in
more than one production, but we can deal with each body once, introducing
one extra variable for each. For EPT, we introduce new variable C,, and
replace the one production, E > EPT, where it appears, by FE 4 EC, and
C1 3 PT.

For TAF we introduce new variable Cj. The two productions that, use this
body, # 36 TMF and T 4 TAF, are replaced by EF 4 TCy, T > TCs, and
Cy 4 ALF. Then, for LER we introduce new variable Cs and replace the three
productions that use it, EF 4 LER, T — LER, and F 4 LER, by E 3 LCs,
T > £C3, F + £C3, and C3 3 ER. The final grammar, which is in CNF, is
shown in Fig. 7.3. 0

BEB > EC,|TCy| £C3|a|b|fA|IB|IZ|f0
TF o> TCy,|£C;|a;,b| fA | 7B) IZ |I0
F - res ae hee Le

i + a|b|fA|IB|IZ|IO

Ao «4

B > ob

2 > 0

Oo 7 1

PO + +

Mos «

gb + [

Ro -+ }

C; — *PT

Cz -—- MF

C3; - ER

Figure 7.3: Making all bodies cither a single terminal or two variables

Theorem 7.16: If G is a CFG whose language contains at least one string
other than ¢, then there is a grammar G, in Chomsky Normal Form, such that

L(G4) = L(G) — fe}.

PROOF: By Theorem 7.14, we can find CFG Ge such that L(Gy) = L(G)—{e},
and such that G2 has no useless symbols, ¢-productions, or unit productions.
The construction that converts G2 to CNF grammar G, changes the produc-
tions In such a way that each production of G) can be simulated by one or
more productions of Gy. Conversely, the introduced variables of Gz each have
only one production, so they can only be used in the manner intended. More
formally, we prove that wis in £(G2) if and only if w is in L(G).


--- Page 285 ---
7.1. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 269

(Only-if) If w has a derivation in Ge, it is easy to replace each production
used, say A > X,X2---Xy, by a sequence of productions of G;. That is,
one step in the derivation in Gz becomes one or more steps in the derivation
of w using the productions of G1. First, if any X; is a terminal, we know
G, has a corresponding variable B; and a production B; > X;. Then, if
k > 2, G, has productions A > B,C), C,; - BoC, and so on, where 8; is
either the introduced variable for terminal X; or X; itself, if X; is a variable.
These productions simulate in G: one step of a derivation of Gp that uses
A— X\Xo---X,. We conclude that there is a derivation of w in G), so w is

(If) Suppose w is in L(G,). Then there is a parse tree in G,, with S at the
root and yield w. We convert this tree to a parse tree of Ge that also has root
& and yield w.

First, we “undo” part (b) of the CNF construction. That is, suppose there
is a node labeled A, with two children labeled B, and C, where Cj is one of the
variables introduced in part (b). Then this portion of the parse tree must look
like Fig. 7.4(a). That is, because these introduced variables each have only one
production, there is only one way that they can appear, and all the variables
introduced to handle the production A 3 B,B)--- By, must appear together,
as shown.

Any such cluster of nodes in the parse tree may be replaced by the pro-
duction that they represent. The parse-tree transformation is suggested by
Fig. 7.4(b).

The resulting parse tree is still not necessarily a parse tree of G2. The
reason is that step (a) in the CNF construction introduced other variables that
derive single terminals. However, we can identify these in the current parse tree
and replace a node labeled by such a variable A and its one child labeled a,
by a single node labeled a. Now, every interior node of the parse tree forms a
production of G2. Since w is the yield of a parse tree in Gz, we conclude that
wisin L(G2), O

7.1.6 Exercises for Section 7.1

* Bxercise 7.1.1: Find a grammar equivalent to

S > AB|CA
A -> «a

B -— BC|AB
C + aBl|é

with no useless symbols.


--- Page 286 ---


--- Page 287 ---
7.1. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 271

Greibach Normal Form

There is another interesting normal form for grammars that we shall not
prove. Every nonempty language without ¢ is L(G) for some grammar G
each of whose productions are of the form A —> ea, where a is a terminal
and « is a string of zero or more variables. Converting a grammar to
this form is complex, even if we simplify the task by, say, starting with a
Chomsky-Normal-Form grammar. Roughly, we expand the first variable
of each production, until we get a terminal. However, because there can
be cycles, where we never reach a terminal, it is necessary to “short-
circuit” the process, creating a production that introduces a terminal as
the first symbol of the body and has variables following it to generate all
the sequences of variables that might have been generated on the way to
generation of that terminal.

This form, called Greibach Normal Form, after Sheila Greibach, who
first gave a way to construct such grammars, has several intercsting con-
sequences. Since each use of a production introduces exactly one terminal
into a sentential form, a string of length nm has a derivation of exactly
n steps. Also, if we apply the PDA construction of Theorem 6.13 to
a Greibach-Normal-Form grammar, then we get a PDA with no e¢-rules,
thus showing that it is always possible to eliminate such transitions of a
PDA.

* Exercise 7.1.2: Begin with the grammar:

S 7 ASB|e«
A 7 @AS|a
B + SbS|A| bb

a) Eliminate ¢-productions.

b) Eliminate any unit productions in the resulting grammar.

c) Eliminate any useless symbols in the resulting grammar.

d) Put the resulting grammar into Chomsky normal form.
Exercise 7.1.3: Repeat Exercise 7.1.2 for the following grammar:

S - 040[1B1| BB

A - ¢€
Bo S|A
C 7+ Sle

Exercise 7.1.4: Repeat Exercise 7.1.2 for the following grammar:


--- Page 288 ---
*

272 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

5 + AAA|B
A > aA|B
Box ie

Exercise 7.1.5: Repeat Exercise 7.1.2 for the following grammar:

5S + aAa| bBb|e
A - Cla
Bowa Cla

C 7+ CDE |e
D -+ A|B|ab

Exercise 7.1.6: Design a CNF grammar for the set of strings of balanced
parentheses. You need not start from any particular non-CNF grammar.

Exercise 7.1.7: Suppose G is a CFG with p productions, and no production
body longer than n. Show that if A = e, then there is a derivation of « from

A of no more than (n? — 1)/(n — 1} steps. How close can you actually come to
this bound?

Exercise 7.1.8: Suppose we have a grammar G with nm productions, none of
them e-productions, and we we convert this grammar to CNF.

a) Show that the CNF grammar has at most O(n?) productions.

b) Show that it is possible for the CNF grammar to have a number of produc-
tions proportional to n*. Hiné: Consider the construction that eliminates
unit productions.

Exercise 7.1.9: Provide the inductive proofs needed to complete the following
theorems:

a) The part of Theorem 7.4 where we show that discovered symbols really
are generating.

b) Both directions of Theorem 7.6, where we show the correctness of the
algorithm in Section 7.1.2 for detecting the reachable symbols.

c) The part of Theorem 7.11 where we show that all pairs discovered really
are unit pairs.

Exercise 7.1.10: Is it possible to find, for every context-free language without
€, a grammar such that all its productions are either of the form 4 4 BCD
(i.e., a body consisting of three variables), or A > a (i.e., a body consisting of
a single terminal)? Give either a proof or a counterexample.

Exercise 7.1.11: In this exercise, we shall show that for every context-free lan-
guage £ containing at least one string other than e, there is a CFG in Greibach
normal form that generates £—{¢}. Recall that a Greibach normal form (GNF)
grammar is one where every production bedy starts with a terminal. The con-
struction will be done using a series of lemmas and constructions.


--- Page 289 ---
7.1. NORMAL FORMS FOR CONTEXT-FREE GRAMMARS 273

a) Suppose that a CFG G has a production A > aB8, and all the produc-

tions for B are B > y, | % | --- | Ya. Then if we replace A > aBJ by
all the productions we get by substituting some body of a B-production
for B, that is, A 4 ay 8 | oS | -:: | a@ynZ, the resulting grammar

generates the same language as G.

In what follows, assume that the grammar G for L is in Chomsky Normal Form,
and that the variables arc called A), Ao,..., Ap.

*! b)

—
oO
—

41d)

Show that, by repeatedly using the transformation of part (a), we can
convert G to an equivalent grammar in which every production body for
A; either starts with a terminal or starts with Aj, for some j > 2. In either
case, all symbols after the first in any production body are variables.

Suppose G, is the grammar that we get by performing step (b) on G.
Suppose that A; is any variable, and let A 4 Aim | --- | Aiam be all
the A;-productions that have a body beginning with A;. Let

be all the other 4;-productions. Note that cach 3; must start with either a
terminal or a variable with index higher than j. Introduce a new variable
B;, and replace the first group of m productions by

Ajo Bi |---| Bp Bi
B; 7 0 B; | ] | ote | On Bi | Orn,

Prove that the resulting grammar generates the same language as G' and
Gi.

Let Go be the grammar that results from step (c). Note that all the A,
productions have bodies that begin with either a terminal or an A; for
j >i. Also, all the B; productions have bodies that begin with either a
terminal or some A;. Prove that G2 has an equivalent gramunar in GNF.
Hint: First fix the productions for Ag, then 4,_1, and so on, down to
Aj, using part (a). Then fix the B; productions in any order, again using
part (a).

Exercise 7.1.12: Use the construction of Exercise 7.1.11 to convert the gram-

mar

5S —+ AA|O
A > SS|1

to GNF.


--- Page 290 ---
2r4 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

7.2 The Pumping Lemma for Context-Free
Languages

Now, we shall develop a tool for showing that certain languages are not context-
free. The theorem, called the “pumping lemma for context-free languages,” says
that in any sufficiently long string in a CFL, it is possible to find at most two
short, nearby substrings, that we can “pump” in tandem. That is, we may
repeat both of the strings 7 times, for any integer z, and the resulting string will
still be in the language.

We may contrast this theorem with the analogous pumping lemma for
regular languages, Theorem 4.1, which says we can always find one small
string to pump. The difference is seen when we consider a language like
£={0"1" | n > 1}. We can show it is not regular, by fixing n and pumping a
substring of 0’s, thus getting a string with more 0’s than 1’s. However, the CFL
pumping lemma. states only that we can find two small strings, so we might be
forced to use a string of 0’s and a string of 1’s, thus generating only strings in
£ when we “pump.” That outcome is fortunate, because Z is a CFL, and thus
we should not be able to use the CFL pumping lemma to construct. strings not
in L.

7.2.1 The Size of Parse Trees

Our first step in deriving a pumping lemma for CFL’s is to examine the shape
and size of parse trees. One of the uses of CNF is to turn parse trees into
binary trees. These trees have some convenient properties, one of which we
exploit here.

Theorem 7.17: Suppose we have a parse tree according to a Chomsky-Nor-
mal-Form grammar G = (V,T, P,S), and suppose that the yield of the tree is
a terminal string w. If the length of the longest path is n, then |w| < 2"-1.

PROOF: The proof is a simple induction on n.

BASIS: n = 1. Recall that the length of a path in a tree is the number of edges,
ie., one less than the number of nodes. Thus, a tree with a maximum path
length of 1 consists of only a root and one leaf labeled by a terminal. String w
is this terminal, so |w| = 1. Since 2"-+ = 2° = 1 in this case, we have proved
the basis.

INDUCTION: Suppose the longest path has length n, and n > 1. The root of
the tree uses a production, which must be of the form A > BC, since n > 1;
Le., we could not start the tree using a production with a terminal. No path
in the subtrees rooted at B and C can have length greater than n — 1, since
these paths exclude the edge from the root to its child labeled B or C. Thus,
by the inductive hypothesis, these two subtrees each have yields of length at
most 2"°~?. The yield of the entire tree is the concatenation of these two yields,


--- Page 291 ---
7.2. THE PUMPING LEMMA FOR CONTEXT-FREE LANGUAGES 275

and therefore has length at most 2°-? +2"-? = 2"—’. Thus, the inductive step
is proved. O

7.2.2 Statement of the Pumping Lemma

The pumping lemma for CFL’s is quite similar to the pumping lemma for
regular languages, but we break each string z in the CFL L into five parts, and
we pump the second and fourth, in tandem.

Theorem 7.18: (The pumping lemma for context-free languages) Let E be
a CFL. Then there exists a constant n such that if z is any string in L such
that |z| is at least n, then we can write z = uvway, subject to the following
conditions:

1. |vwa| <n. That is, the middie portion is not too long.

2. vz £€. Since v and z are the pieces to be “pumped,” this condition says
that at least one of the strings we pump must not be empty.

3. For all i > 0, uv'wety is in L. That is, the two strings v and x may be
“pumped” any number of times, including 0, and the resulting string will
still be a member of L.

PROOF: Our first step is to find a Chomsky-Normal-Form grammar G for L.
Technically, we cannot find such a grammar if L is the CFL @ or {e}. However,
if LF = @ then the statement of the theorem, which talks about a string z in £L
surely cannot be violated, since there is no such 2 in @. Also, the CNF grammar
G will actually generate L — {e}, but that is again not of importance, since we
shall surely pick n > 0, in which case z cannot be ¢ anyway.

Now, starting with a CNF grammar G = (V,T,P,S) such that L(G) =
EL — {e}, let G have m variables. Choose n = 2™_ Next, suppose that z in £ is
of length at least n. By Theorem 7.17, any parse tree whose longest path is of
length m or less must have a yield of length gm-1 = 7/2 or less. Such a parse
tree cannot have yield z, because z is too long. Thus, any parse tree with yield
z has a path of length at least m + 1.

Figure 7.5 suggests the longest path in the tree for z, where k is at least m
and the path is of length k-+1. Since & > m, there are at least m+ 1 occurrences
of variables Aj, .41,..., Ax on the path. As there are only m different variables
in V, at least two of the last m+ 1 variables on the path (that is, Asm
through A,, inclusive) must be the same variable. Suppose A; = Aj, where
k—-m<i<gshk.

Then it is possible to divide the tree as shown in Fig. 7.6. String w is the
yield of the subtree rooted at Aj. Strings v and « are the strings to the left. and
right, respectively, of w in the yield of the larger subtree rooted at A,;. Note
that, since there are no unit productions, v and x could not both be ¢, although
one could be. Finally, u and y are those portions of z that are to the left and
right, respectively, of the subtree rooted at Aj.


--- Page 292 ---
276 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

Figure 7.5: Every sufficiently long string in Z must have a long path in its parse
tree

If A; = A; = A, then we can construct new parse trees from the original
tree, as suggested in Fig. 7.7(a}. First, we may replace the subtree rooted
at A;, which has yield wwz, by the subtree rooted at A;, which has yield w.
The reason we can do so is that both of these trees have root labeled A. The
resulting tree is suggested in Fig. 7.7(b); it has yield wwy and corresponds to
the case i = 0 in the pattern of strings uv'wa'y.

Another option is suggested by Fig. 7.7(c). There, we have replaced the
subtree rooted at A; by the entire subtree rooted at A;. Again, the justification
is that we are substituting one tree with root labeled A for another tree with
the same root label. The yield of this tree is uv?wa2?y. Were we to then replace
the subtree of Fig. 7.7(c) with yield w by the larger subtree with yield vwz, we
would have a tree with yield uv?w2*y, and so on, for any exponent i. Thus,
there are parse trees in G for all strings of the form uv*s2'y, and we have
almost proved the pumping lemma.

The remaining detail is condition (1), which says that |uwa| <n. However,
we picked A; to be close to the bottom of the tree; that is, k -i < m. Thus,
the longest path in the subtree rooted at A; is no greater than m+1. By
Theorem 7.17, the subtree rooted at A; has a yield whose length is no greater
than 2% =n. O

7.2.3 Applications of the Pumping Lemma for CFL’s

Notice that, like the earlier pumping lemma for regular languages, we use the
CF] pumping lemma as an “adversary game, as follows.”

1. We pick a language £ that we want to show is not a CFL.


--- Page 293 ---
7.2. THE PUMPING LEMMA FOR CONTEXT-FREE LANGUAGES 277

Ss

Figure 7.6: Dividing the string w so it can be pumped

2. Our “adversary” gets to pick n, which we do not know, and we therefore
must plan for any possible 7.

3. We get to pick z, and may use n as a parameter when we do so.

4. Our adversary gets to break z into uuway, subject only to the constraints
that juwx| <n and vz # €.

_ We “win” the game, if we can, by picking i and showing that uv'waty is
not in L.

a1

We shall now see some examples of languages that we can prove, using the
pumping lemma, not to be context-free. Our first example shows that, while
context-free languages can match two groups of symbols for equality or inequal-
ity, they cannot match three such groups.

Example 7.19: Let L be the language {0%1"2" | > 1}. That is, 2 consists of
all strings in 0 1+2+ with an equal number of each symbol, e.g., 012, 001122,
and so on. Suppose L were context-free. Then there is an integer n given to us
by the pumping lemma.” Let us pick z = 071"2".

Suppose the “adversary” breaks z as z = uvwry, where juowa| <n and v
and z are not both e. Then we know that vwz cannot involve both 0’s and
2's, since the last 0 and the first 2 are separated by n + 1 positions. We shall
prove that L contains some string known not to be in LD, thus contradicting the
assumption that L is a CFL. The cases are as follows:

? Remember that this n is the constant provided by the pumping Jemma, and it has nothing
to do with the local variable n used in the definition of L itself.


--- Page 294 ---
278 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

5
UN .
iu v w x y
§
(b)
Ww
u ¥
§
UN °
v Ww x

Figure 7,7; Pumping strings v and « zero times and pumping them twice

1. vw has no 2’s. Then vax consists of only 0’s and 1’s, and has at least
one of these symbols. Then uwy, which would have to be in L by the
pumping lemma, has n 2’s, but has fewer than n 0’s or fewer than 7 1’s,
or both. It therefore does not belong in Z, and we conclude FE is not a
CFL in this case.

2. vw2 has no 0’s. Similarly, wwy has n 0's, but fewer 1’s or fewer 2’s. It
therefore is not in L.

Whichever case holds, we conclude that Z has a string we know not to be in L.
This contradiction allows us to conclude that our assumption was wrong; £ is
nota CFL. O


--- Page 295 ---
7.2. THE PUMPING LEMMA FOR CONTEXT-FREE LANGUAGES 279

Another thing that CFL’s cannot do is match two pairs of equal numbers
of symbols, provided that the pairs interleave. The idea is made precise in the
following example of a proof of non-context-freeness using the pumping lemma.

Example 7.20: Let L be the language {0°1/2°3’ |i > 1 and j > 1}. If L is
context-free, let n be the constant for 2, and pick z = 0"1"2"3". We may write
z= uvwey subject to the usual constraints |ywa| < n and vz # «. Then wwe
is either contained in the substring of one symbol, or it straddles two adjacent
symbols.

If vw consists of only one symbol, then wwy has n of three different symbols
and fewer than n of the fourth symbol. Thus, it cannot bein L. If vwz straddles
two symbols, say the 1’s and 2’s, then uwy is missing either some 1’s or some
2’s, or both. Suppose it is missing 1’s. As there are n 3’s, this string cannot
be in L. Similarly, if it is missing 2’s, then as it has n 0's, uwy cannot be in LD.
We have contradicted the assumption that L is a CFL and conclude that it is
not. 4

As a final example, we shall show that CFL's cannot match two strings
of arbitrary length, if the strings are chosen from an alphabet of more than
one symbol. An implication of this observation, incidentally, is that grammars
are not a suitable mechanism for enforcing certain “semantic” constraints in
programming languages, such as the common requirement that an identifier be
declared before use. In practice, another mechanism, such as a “symbol table”
is used to record declared identifiers, and we do not try to design a parser that,
by itself, checks for “definition prior to use.”

Example 7.21: Let L = {ww | w isin {0,1}*}. That is, £ consists of repeating
strings, such as €, 0101, 00100010, or 110110. If £ is context-free, then let n be
its pumping-lemma constant. Consider the string z = 071"0"1". This string is
0" 1" repeated, so z is in £.

Following the pattern of the previous examples, we can break z = uvwey,
such that |jvwa| <n and vz # «. We shail show that uwy is not in £, and thus
show ZL not to be a context-free language, by contradiction.

First, observe that, since juwa| <n, Juwy| > 3n. Thus, if uwy is some
repeating string, say tt, then t is of length at least 3n/2. There are several
cases to consider, depending where yvwz is within z.

1. Suppose vwz is within the first n 0’s. In particular, let vx consist of &
0’s, where k > 0. Then uwy begins with 0°-*1”. Since |juwy| = 4n — k,
we know that if uwwy = tt, then |t| = 2n —&/2. Thus, t does not end until
after the first block of 1’s; ie., ¢ ends in 0. But wwy ends in 1, and so it
cannot equal i.

2. Suppose vwa straddles the first block of 0’s and the first block of 1’s. It
may be that vz consists only of 0’s, if « = «. Then, the argument that
uwy is not of the form tt is the same as case (1). If vx has at least one


--- Page 296 ---
280 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

1, then we note that ¢, which is of length at least 3n/2, must end in 1”,
because uwy ends in 1”. However, there is no block of n 1’s except the
final block, so ¢t cannot repeat in uwy.

3. If vw is contained in the first block of 1’s, then the argument that uwy
is not in L is like the second part of case (2).

4. Suppose vwz straddles the first block of 1’s and the second block of 0's.
If vz actually has no 0’s, then the argument is the same as if vwa were
contained in the first block of 1’s. If va has at least one 0, then uwy starts
with a block of n 0’s, and so does t¢ if uwy = é. However, there is no
other block of n 0's in uwy for the second copy of ¢. We conclude in this
case too, that uwy is not in L.

5. In the other cases, where vwz is in the second half of z, the argument is
symmetric to the cases where vw is contained in the first half of z.

Thus, in no case is wwy in £, and we conclude that Z is not context-free. O

7.2.4 Exercises for Section 7.2

Exercise 7.2.1: Use the CFL pumping lemma to show each of these languages
not to be context-free:

* a) {ald’ck |i jf < kh.
b) {a"b"e! | i <n}.

c) {0" | pis a prime}. Hint: Adapt the same ideas used in Example 4.3,
which showed this language not to be regular.

*!d) {OV | j= 27}.
fe) {acl | n <i < Qn},

!f) {www | w is a string of 0’s and 1’s}. That is, the set of strings consisting
of some string w followed by the same string in reverse, and then the string
w again, such as 001100001.

! Exercise 7.2.2: When we try to apply the pumping lemma to a CFL, the
“adversary wins,” and we cannot complete the proof. Show what goes wrong
when we choose L to be one of the following languages:

a) {00,11}.
*b) {0°17 | 2 > 1}.

* c) The set of palindromes over alphabet {0,1}.


--- Page 297 ---
7.8. CLOSURE PROPERTIES OF CONTEXT-FREE LANGUAGES 281

Exercise 7.2.3: There is a stronger version of the CFL pumping lemma known
as Ogden’s lemma. It differs from the pumping lemma we proved by allowing
us to focus on any » “distinguished” positions of a string z and guaranteeing
that the strings to be pumped have between 1 and n distinguished positions.
The advantage of this ability is that a language may have strings consisting
of two parts, one of which can be pumped without producing strings not in
the language, while the other does produce strings outside the language when
pumped. Without being able to insist that the pumping take place in the latter
part, we cannot complete a proof of non-context-freeness. The formal statement
of Ogden’s lemma is: If L is a CFL, then there is a constant n, such that if z
is any string of length at least x in L, in which we select at least m positions to
be distinguished, then we can write z = wvway, such that:

1. vwe has at most n distinguished positions.
2. uz has at least one distinguished position.
3. For all i, uv'wa'y is in L.
Prove Ogden’s lemma. Hint: The proof is really the same as that of the pump-

ing lemma of Theorem 7.18 if we pretend that the nondistinguished positions
of z are not present as we select a long path in the parse tree for z.

Exercise 7.2.4: Use Ogden’s lemma (Exercise 7.2.3} to simplify the proof
in Example 7.21 that L = {ww | w is in {0,1}*} is not a CFL. Hint: With
z = 0"1"0"1", make the two middle blocks distinguished.

Exercise 7.2.5: Use Ogden’s lemma (Exercise 7.2.3) to show the following
languages are not CFL’s:

ta) {0'190* | 7 = max(é, k)}.

tb) {a%b"c! | i An}. Hint: If n is the constant for Ogden’s lemma, consider
the string z = a™b"c™.

7.3 Closure Properties of Context-Free
Languages

We shall now consider some of the operations on context-free languages that
are guaranteed to produce a CFL. Many of these closure properties will parallel
the theorems we had for regular languages in Section 4.2. However, there are
some differences.

First, we introduce an operation calied substitution, in which we replace each
symbol in the strings of one language by an entire language. This operation, a
generalization of the homomorphism that we studied in Section 4.2.3, is useful in
proving some other closure properties of CFL’s, such as the regular-expression
operations: union, concatenation, and closure. We show that CFL’s are closed


--- Page 298 ---
282 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

under homomorphisms and inverse homomorphisms. Unlike the regular lan-
guages, the CFL’s are not closed under intersection or difference. However, the
intersection or difference of a CFL and a regular language is always a CFL.

7.3.1 Substitutions

Let © be an alphabet, and suppose that for every symbol a in ©, we choose a
language £,. These chosen languages can be over any alphabets, net necessarily
E and not necessarily the same. This choice of languages defines a function s
(a substitution) on Z, and we shall refer to L, as s(a) for each symbol a.

If w = @)@)-'+ a, is a String in &*, then s(w) is the language of alf strings
Hy 2_Q-++ at, such that string x; is in the language s(a;), fori = 1,2,...,n. Put
another way, s(w) is the concatenation of the languages s(a,)s(a2)-+--s(an).
We can further extend the definition of s to apply to languages: s(Z) is the
union of s{w) for all strings w in ZL.

Example 7.22: Suppose s(0) = {a"b” | m > 1} and s(1) = {aa, bd}. That is,
s is a substitution on alphabet © = {0,1}. Language s(0) is the set of strings
with one or more a’s followed by an equal number of &’s, while s(1) is the finite
language consisting of the two strmgs aa and bb.

Let w= O01. Then s{w) is the concatenation of the languages s(0)s(1). To
be exact, s(w) consists of all strings of the forms a"b"aa and a”b"t?, where
nei.

Now, suppose L = £(0*), that is, the set of all strings of 0’s. Then s(Z) =
(s(0))". This language is the set of all strings of the form

am pm a? p?2 tan gtk pie

for some k > 0 and any sequence of choices of positive integers ny, ne,...,74-
It includes strings such as ¢, aabdbaaebib, and abaabbabab. O

Theorem 7.23: If £ is a context-free language over alphabet X, and 5 is a
substitution on © such that s(@) is a CFL for each @ in X, then s(£) is a CFL.

PROOF: The essential idea is that we may take a CFG for £ and replace each
terminal a by the start symbol of a CFG for language s(a). The result is a
single CFG that generates s(Z). However, there are a few details that must be
gotten right to make this idea work.

More formally, start with grammars for each of the relevant languages, say
G = (V,U,P,S) for L and Gy = (Va,Ta, Pa, 5.) for each a@ in E. Since we
can choose any names we wish for variables, let, us make sure that the sets of
variables are disjoint; that is, there is no symbol A that is in two or more of
V and any of the V,’s. The purpose of this choice of names is to make sure
that when we combine the productions of the various grammars into one set
of productions, we cannot get accidental mixing of the productions from two
grammars and thus have derivations that do not resemble the derivations in
any of the given grammars.

We construct a new grammar G' = {V"',7", P’,S) for s(Z), as follows:


--- Page 299 ---
7.3. CLOSURE PROPERTIES OF CONTEXT-FREE LANGUAGES — 283

e V’ is the union of V and all the Y,’s for @ in &.
e T' is the union of all the T,’s for a in E.
e P' consists of:

1. Ail productions in any F,, for @ in &.

2. The productions of P, but with each terminal a in their bodies re-
placed by S, everywhere @ occurs.

Thus, all parse trees in grammar G" start out like parse trees in G, but instead
of generating a yield in £*, there is a frontier in the tree where all nodes have
labels that are S, for some a in ©. Then, dangling from each such node is a
parse tree of Ga, whose yield is a terminal string that is in the language s(a).
The typical parse tree is suggested in Fig. 7.8.

5

x) X x;

Figure 7.8: A parse tree in G’ begins with a parse tree in G and finishes with
many parse trees, each in one of the grammars G,

Now, we must prove that this construction works, in the sense that G’
generates the language s(L). Formally:

e A string w is in L(G’) if and only if w is in s(Z).

(If) Suppose w is in s(Z). Then there is some string x = @,@2--+@n in E, and
strings z; in s(a;) for i = 1,2,...,n, such that w = 2)f2---Zn- Then the
portion of G’ that comes from the productions of G with S, substituted for
each a will generate a string that looks like x, but with S, in place of each a.
This string is S,,5,:--Sa,- This part of the derivation of w is suggested by
the upper triangle in Fig. 7.8.

Since the productions of each G', are also productions of G', the derivation
of z; from Sq, is also a derivation in G’. The parse trees for these derivations
are suggested by the lower triangles in Fig. 7.8. Since the yield of this parse
tree of G’ is 2,22 °-+2_ = w, we conclude that w is in L(G’).


--- Page 300 ---
284 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

(Only-if) Now suppose w is in L(G’). We claim that the parse tree for w
must look like the tree of Fig. 7.8. The reason is that the variables of each
of the grammars G and G, for ¢ in © are disjoint. Thus, the top of the tree,
starting from variable S, must use only productions of G until some symbol Sy
is derived, and below that 5S, only productions of grammar G, may be used.
As a result, whenever w has a parse tree 7’, we can identify a string a)a2°-- ay
in L(G), and strings z; in language s(a;), such that

tl. w= 2,22'+'Zy, and

2. The string $,,Sa.°--Sq, is the yield of a tree that is formed from T by
deleting some subtrees (as suggested by Fig. 7.8).

But the string 2)22---2, is in s(Z), since it is formed by substituting strings
x; for each of the a;’s. Thus, we conclude w is in a(£). O

7.3.2 Applications of the Substitution Theorem

There are several familiar closure properties, which we studied for regular lan-
guages, that we can show for CFL’s using Theorem 7.23. We shall list them all
in one theorem.

Theorem 7.24: The context-free languages are closed under the following
operations:

1. Union.

2. Concatenation.

3. Closure (*), and positive closure (+).
4, Homomorphism.

PROOF: Each requires only that we set up the proper substitution. The proofs
below each involve substitution of context-free languages into other context-free
languages, and therefore produce CFL's by Theorem 7.23.

1. Union: Let £, and Ly be CFL’s. Then £, U Ly is the language s(Z),
where L is the language {1,2}, and s is the substitution defined by (1) =
Ey and 5(2) = Ey.

2. Concatenation: Again let £; and Ly be CFL’s. Then Ly Lo is the language
s(L), where Z is the language {12}, and s is the same substitution as in
case (1).

3. Clesure and positive closure: If Ly is a CFL, L is the language {1}*, and
s is the substitution s(1) = £1, then LT = s(L). Similarly, if L is instead
the language {1}*, then LY = s(L).


--- Page 301 ---
7.3. CLOSURE PROPERTIES OF CONTEXT-FREE LANGUAGES — 285

4, Suppose L is a CFL over alphabet 5, and his a homomorphism on x. Let
s be the substitution that replaces each symbol a in © by the language
consisting of the one string that is h(a). That is, s(@) = {h{a)}}, for all a
in 5. Then A(L) = s(L).

QO

7.3.3 Reversal

The CFL’s are also closed under reversal. We cannot use the substitution
theorem, but there is a simple construction using grammars.

Theorem 7.25: If L is a CFL, then so is L*.

PROOF: Let L = L(G) for some CFL G = (V,T,P,5). Construct Gt =
(V,T,P#,S), where P* is the “reverse” of each production in P. That is, if
A a is a production of G, then A > a® is a production of G*. It is an easy
induction on the lengths of derivations in G and G# to show that L(G Ry = pA,
Essentially, all the sententiai forms of GF are reverses of sentential forms of G,
and vice-versa. We leave the formal proof as an exercise. O

7.3.4 Intersection With a Regular Language

The CFL’s are not closed under intersection. Here is a simple example that
proves they are not.

Example 7.26: We learned in Example 7.19 that the language
L= {0"1"2" [n> 1}

is not a context-free language. However, the following two languages @re con-
text-free:

Ly = {0"1"2' |n >1,4>1}

Ly = {0°1"2" |n > 1,1 > 1}

A grammar for Ly is:

S73 AB
A-+0A1|01
B-+2B\|2

In this grammar, A generates all strings of the form 0" 1”, and B generates all
strings of 2’s. A grammar for DL» is:

S— AB
A+0A|0
B- 1B2{ 12


--- Page 302 ---
286 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

It works similarly, but with A generating any string of 0’s, and B generating
matching strings of 1’s and 2’s.

However, L = £, M Ly. To see why, observe that £, requires that there be
the same number of 0’s and 1’s, while La requires the numbers of 1’s and 2’s
to be equal. A string in both languages must have equal numbers of all three
symbols and thus be in L.

If the CFL’s were closed under intersection, then we could prove the false
statement that Z is context-free. We conclude by contradiction that the CFL’s
are not closed under intersection. O

On the other hand, there is a weaker claim we can make about intersection.
The context-free languages are closed under the operation of “intersection with
a regular language.” The formal statement and proof is in the next theorem.

Theorem 7.27: If Z is a CFL and F is a regular language, then LM FR is a
CFL.

PROOF: This proof requires the pushdown-automaton representation of CFL’s,
as well as the finite-automaton representation of regular languages, and gener-
alizes the proof of Theorem 4.8, where we ran two finite automata “in parallel”
to get the intersection of their languages. Here, we run a finite automaton “in
parallel” with a PDA, and the result is another PDA, as suggested in Fig. 7.9.

Input Accept/

reject

Figure 7.9: A PDA and a FA can run in parallel to create a new PDA

Formally, let
P= (Qp,%, lr, dp, gp, Za, Fp)


--- Page 303 ---
7.3. CLOSURE PROPERTIES OF CONTEXT-FREE LANGUAGES — 287

be a PDA that accepts £ by final state, and let
A = (Qa,¥,9a, 9A, Fa)
be a DFA for R. Construct PDA
P! = (Qp x Qa, 5,7, 4, (ap,44), Za, Fp x Fa)
where 5((q,p),@,X) is defined to be the set of all pairs ((7,s),y} such that:

las é4(p, a), and

2. Pair (r,) is in dp(g,a, X).
That is, for each move of PDA P, we can make the same move im PDA FP’, and
in addition, we carry along the state of the DFA A in a second component of
the state of P’. Note that a may be a symbol of £, or a = ¢. In the former
case, 6(p,a) = é4(p), while if a = ¢, then 4(p,a)} = p; ie., A does not change
state while P makes moves on ¢ input.

It is an easy induction on the numbers of moves made by the PDA’s that
(gp, w, Zag) (g,€,7) if and only if (ar, 4a), w, Zo) Ce ((4,P), 67); where

p= (pa, w). We leave these inductions as exercises. Since (q, p) is an accepting
state of P' if and only if q is an accepting state of P, and p is an accepting state
of A, we conclude that P’ accepts w if and only if both P and A do; i.e., w is
mink. oO

Example 7.28: In Fig. 6.6 we designed a PDA called F to accept by final
state the set of strings of i’s and e’s that represent minimal violations of the
rule regarding how if’s and else’s may appear in C programs. Call this language
L. The PDA F was defined by

Pr = ({p,9,7}, fie}, {Z, Xo}, dr, p, Xo, {r})

where d¢ consists of the rules:

1. 6¢(p,¢, Xo) = {(a, ZXo)}-

2. dr(g,t, 2) = {(q,22Z)}.

3. Sr(g,e, Z) = {(¢, €}}.

4. dr(9,6,X0) = {(,6,0)}-

Now, let us introduce a finite automaton

A= ({s,t}, {i,e}, 64,8, {8,€})

that accepts the strings in the language of i*e*, that is, all strings of 7’s followed
by e’s. Call this language &. Transition function é, is given by the rules:


--- Page 304 ---
288 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

a) d4(s,i) = 8.
b) éd4(s,e) = t.
c) é4(é,¢) = t.

Strictly speaking, A is not a DFA, as assumed in Theorem 7.27, because it is
missing a dead state for the case that we see input 7 when in state t. However,
the same construction works even for an NFA, since the PDA that we construct
is allowed to be nondeterministic. In this case, the constructed PDA is actually
deterministic, although it will “die” on certain sequences of input.

We shall construct a PDA

P = ({p,q,r} x {s,t}, {¢,e},{Z, Xo}, 6, (p, 5), Xo, (r} x {8,t})

The transitions of 6 are listed below and indexed by the rule of PDA F (a
number from 1 to 4) and the rule of DFA A (a letter a, b, or c) that gives rise
to the rule. In the case that the PDA F makes an ¢-transition, there is no rule
of A used. Note that we construct these rules in a “lazy” way, starting with the
state of P that is the start states of F and A, and constructing rules for other
states only if we discover that P can enter that pair of states.

2a: 5((q,8),7,Z) = {((¢, 8), ZZ) }.
3b: 5((a,8),e,Z) = {((at),€)}.

4: 5((q,8),¢, Xo) = {((r,8),€}}. Note: one can prove that this rule is never
exercised. The reason is that it is impossible to pop the stack without
seeing an e, and as soon as P sees an e the second component of its state
becomes t.

3c: 6{(¢,t),e,2Z) = {((¢,2), €)}-
4: 5{(g,t),€, Xo) = {((r, 2), €)}-

The language EO R is the set of strings with some number of i’s followed by
one more e, that is, {i"e"t! | n > 0}. This set is exactly those if-else violations
that consist of a block of if’s followed by a block of else’s. The language is
evidently a CFL, generated by the grammar with productions 5 - iSe | e.

Note that the PDA P accepts this language £9 R. After pushing Z onto
the stack, it pushes more Z’s onto the stack in response to inputs 7, staying in
state (g,s). As soon as it sees and e, it goes to state (g,¢) and starts popping
the stack. It dies if it sees an é until Xo is exposed on the stack. At that point,
it spontaneously transitions to state (r,#) and accepts. 0


--- Page 305 ---
7.3. CLOSURE PROPERTIES OF CONTEXT-FREE LANGUAGES — 289

Since we know that the CFL’s are not closed under intersection, but are
closed under intersection with a regular language, we also know about the set-
difference and complementation operations on CFL’s. We summarize these
properties in one theorem.

Theorem 7.29: The following are true about a CFL’s Z, Ly, and Lo, and a
regular language R.

1. L— # is a context-free language.
9. F is not necessarily a context-free language.
3. L, — Le is not necessarily context-free.

PROOF: For (1), note that LE - R= LR. If Ris regular, so is R regular by
Theorem 4.5. Then L — R is a CFL by Theorem 7.27.
For (2), suppose that E is always context-free when L is. Then since

IyNLg=f, UL,

and the CFL’s are closed under union, it would follow that the CFL’s are closed
under intersection. However, we know they are not from Example 7.26.

Lastly, let us prove (3). We know * is a CFL for every alphabet &; de-
signing a grammar or PDA for this regular language is easy. Thus, if £; — £3
were always a CFL when £; and Ly are, it would follow that &* — DL was always
a CFL when L is. However, £* - L is L when we pick the proper alphabet
¥. Thus, we would contradict (2) and we have proved by contradiction that
LE, — £2 is not necessarilya CFL.

7.3.5 Inverse Homomorphism

Let us review from Section 4.2.4 the operation called “inverse homomorphism.”
If h is a homomorphism, and L is any language, then Ah~*(£) is the set of
strings w such that A(w) is in L. The proof that regular languages are closed
under inverse homomorphism was suggested in Fig. 4.6. There, we showed how
to design a finite automaton that processes its input symbols a by applying a
homomorphism h to it, and simulating another finite automaton on the sequence
of inputs A(a).

We can prove this closure property of CF'L’s in much the same way, by using
PDA’s instead of finite automata. However, there is one problem that we face
with PDA’s that did not arise when we were dealing with finite automata. The
action of a finite automaton on a sequence of inputs is a state transition, and
thus looks, as far as the constructed automaton is concerned, just like a move
that a finite automaton might make on a single input symbol.

When the automaton is a PDA, in contrast, a sequence of moves might not
took like a move on one input symbol. In particular, in n moves, the PDA can
pop n symbols off its stack, while one move can only pop one symbol. Thus,


--- Page 306 ---
290 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

the construction for PDA’s that is analogous to Fig. 4.6 is somewhat more
complex; it is sketched in Fig. 7.10. The key additional idea is that after input
a is read, h{a) is placed in a “buffer.” The symbols of h(a) are used one at
a time, and fed to the PDA being simulated. Only when the buffer is empty
does the constructed PDA read another of its input symbols and apply the
homomorphism to it. We shall formalize this construction in the next theorem.

Figure 7.10: Constructing a PDA to accept the inverse homomorphism of what
a given PDA accepts

Theorem 7.30: Let L be a CFL and A a homomorphism. Then A7~1(L) is a
CFL.

PROOF: Suppose fh applies to symbols of alphabet © and produces strings in
T*. We also assume that L is a language over alphabet T. As suggested above,
we start with a PDA P = (Q,T,T,6,99, 20, F) that accepts L by final state.
We construct a new PDA

P' = (Q’,=,6', (g0,€); Zo, F x fe}) (7.1)

where:
1. Q! is the set of pairs (g,z) such that:

(a) gis a state in Q, and

(b) 2 is a suffix (not necessarily proper) of some string h(a) for some
input symbol @ in EB.


--- Page 307 ---
7.3. CLOSURE PROPERTIES OF CONTEXT-FREE LANGUAGES — 291

That is, the first component of the state of P’ is the state of P, and the
second component is the buffer. We assume that the buffer will period-
ically be loaded with a string A{a), and then allowed to shrink from the
front, as we use its symbols to feed the simulated PDA P. Note that since
> is finite, and f(a) is finite for all @, there are only a finite number of
states for P’.

2. é' is defined by the following rules:

(a) 4’((9,€),a,X) = {((a, h(a)),X)} for all symbols a in ©, all states
g in Q, and stack symbols X in T. Note that a cannot be ¢€ here.
When the buffer is empty, P’ can consume its next input symbol a
and place h(a) in the buffer.

(b) If 6(g,,X} contains (p,-y), where 6 is in T or 6 = €, then
5'((q, br), €, X)

contains ((p,z),y). That is, P’ always has the option of simulating
a move of P, using the front of its buffer. If b is a symbol in T, then
the buffer must not be empty, but if b = «, then the buffer can be
empty.

3. Note that, as defined in (7.1), the start state of P’ is (go, €); i-e., P’ starts
in the start state of P with an empty buffer.

4. Likewise, the accepting states of P’, as per (7.1), are those states (¢q, €)
such that g is an accepting state of P.

The following statement characterizes the relationship between P’ and P:
« (qo, h(w), Zo) F (p, €, 7) if and only if ((¢0, €},w, Zo) F ((p,€),67)-

The proofs in both directions are inductions on the number of moves made by
the two automata. In the “if” portion, one needs to observe that once the buffer
of P' is nonempty, it cannot read another input symbol and must simulate P,
until the buffer has become empty (although when the buffer is empty, it may
still simulate P). We leave further details as an oxercise.

Once we accept this relationship between P’ and P, we note that P accepts
h(w) if and only if P’ accepts w, because of the way the accepting states of P’
are defined. Thus, L(P’) = h-{Z(P)). O

7.3.6 Exercises for Section 7.3

Exercise 7.3.1: Show that the CFL’s are closed under the following opera-
tions:

* a) init, defined in Exercise 4.2.6(c). Hiné: Start with a CNF grammar for
the language L.


--- Page 308 ---
292 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES
*!b) The operation L/a, defined in Exercise 4.2.2. Hint: Again, start with a
CNF grammar for L.

'lc) cycle, defined in Exercise 4.2.11. Hint: Try a PDA-based construction.

Exercise 7.3.2: Consider the following two languages:

Ly = {a™b?"c™ | n,m > 0}
Lo = fab™2" | n,m > 0}

a) Show that each of these languages is context-free by giving grammars for
each.

!b) Is £, N Ly a CFL? Justify your answer.

'f Exercise 7.3.3: Show that the CFL’s are not closed under the following op-
erations:

* a) min, as defined in Exercise 4.2.6(a).
b) maz, as defined in Exercise 4.2.6(b).
c) helf, as defined in Exercise 4.2.8.

d) alt, as defined in Exercise 4.2.7.

Exercise 7.3.4: The shuffle of two strings w and z is the set of all strings that
one can get by interleaving the positions of w and x in any way. More precisely,
shuffie(w, x) is the set of strings z such that

1. Each position of z can be assigned to w or z, but not both.
2. The positions of z assigned to w form w when read from left to right.
3. The positions of z assigned to x form x when read from left to right.

For example, if w = 01 and z = 110, then shuffle(01, 110) is the set of strings
{01110, 01101, 10110, 10101, 11010, 11001}. To illustrate the necessary reason-
ing, the third string, 10110, is justified by assigning the second and fifth posi-
tions to 01 and positions one, three, and four to 110. The first string, 01110
has three justifications. Assign the first position and either the second, third,
or fourth to 01, and the other three to 110. We can also define the shuffle of
languages, shuffle(L,,£) to the the union over all pairs of strings, w from £1
and « from Le, of shuffle(w, x).

a) What is shuffle(00, 111)?
*b) What is shuffe(L,, Lo) if L; = £(0*) and Ly = {0"1" | n > O}.

*!c) Show that if £) and L2 are both regular languages, then so is


--- Page 309 ---
*T

7.4, DECISION PROPERTIES OF CFL'S 293

shuffte(Z1, £3)

Hint: Start with DFA’s for £, and Lo.

1d) Show that if L is a CFL and R is a regular language, then shuffle(L, R)
isa CFL. Hint: start with a PDA for £ and a DFA for RR.

tle) Give a counterexample to show that if Z; and ZL, are both CFL’s, then
shuffle(L,, Lz) need not be a CFL.

Exercise 7.3.5: A string y is said to be a permutation of the string x if the
symbols of y can be reordered to make z. For instance, the permutations
of string = 011 are 110, 101, and 011. If LE is a language, then perm(L)
is the set of strings that are permutations of strings in Z. For example, if
EL = {0"1" | n > O}, then perm(L) is the set of strings with equal numbers of
0’s and 1’s.

a) Give an cxample of a regular language Z over alphabet {0,1} such that
perm(L) is not regular. Justify your answer. Hint: Try to find a regular
language whose permutations are all strings with an equal number of 0’s
and 1’s.

b) Give an example of a regular language L over alphabet {0, 1,2} such that
perm(L) is not context-free.

c) Prove that for every regular language L over a two-symbol alphabet,
perm(L) is context-free.

Exercise 7.3.6: Give the formal proof of Theorem 7.25: that the CFL’s are
closed under reversal.

Exercise 7.3.7: Complete the proof of Theorem 7.27 by showing that

7.4 Decision Properties of CFL’s

Now, let us consider what kinds of questions we can answer about context-free
languages. In analogy with Section 4.3 about decision properties of the regular
languages, our starting point for a question is always some representation of a
CFL — either a grammar or a PDA. Since we know from Section 6.3 that we
can convert between grammars and PDA’s, we may assume we are given either
representation of a CFL, whichever is more convenient.

We shall discover that very little can be decided about a CFL; the major
tests we are able to make are whether the language is empty and whether a given


--- Page 310 ---
294 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

string is in the language. We thus close the section with a brief discussion of the
kinds of problems that we shall later show (in Chapter 9) are “undecidable,”
ie., they have no algorithm. We begin this section with some observations
about the complexity of converting between the grammar and PDA notations
for a language. These calculations enter into any question of how efficiently we
can decide a property of CFL’s with a given representation.

7.4.1 Complexity of Converting Among CFG’s and PDA’s

Before proceeding to the algorithms for deciding questions about CFL’s, let
us consider the complexity of converting from one representation to another.
The running time of the conversion is a component of the cost of the decision
algorithm whenever the language is given in a form other than the one for which
the algorithm is designed.

In what follows, we shall let n be the length of the entire representation of
a PDA or CFG. Using this parameter as the representation of the size of the
grammar Or automaton is “coarse,” in the sense that some algorithms have a
running time that could be described more precisely in terms of more specific
parameters, such as the number of variables of a grammar or the sum of the
lengths of the stack strings that appear in the transition function of a PDA.

However, the total-length measure is sufficient to distinguish the most im-
portant issues: is an algorithm linear in the length (i.e., does it take little more
time than it takes to read its input), is it exponential in the length (i.e., you can
perform the conversion only for rather small examples), or is it some nonlinear
polynomial (i.e., you can run the algorithm, even for large examples, but the
time is often quite significant).

There are several conversions we have seen so far that are linear in the size
of the input. Since they take linear time, the representation that they produce
as output is not only produced quickly, but it is of size comparable to the input
size. These conversions are:

1. Converting a CFG to a PDA, by the algorithm of Theorem 6.13.

2. Converting a PDA that accepts by final state to a PDA that accepts by
empty stack, using the construction of Theorem 6.11.

3. Converting a PDA that accepts by empty stack to a PDA that accepts
by final state, using the construction of Theorem 6.9.

On the other hand, the running time of the conversion from a PDA to a
grammar (Theorem 6.14) is much more complex. First, note that n, the total
length of the input, is surely an upper bound on the number of states and
stack symbols, so there cannot be more than n® variables of the form [pXq]
constructed for the grammar. However, the running time of the conversion can
be exponential, if there is a transition of the PDA that puts a large number of
symbols on the stack. Note that one rule could place almost n symbols on the
stack.


--- Page 311 ---
7.4. DECISION PROPERTIES OF CFL’S 295

If we review the construction of grammar productions from a rule like
“5(q,a,X) contains (ro, %41¥2--:¥4),” we note that it gives rise to a collec-
tion of productions of the form [¢X rx] > [ro¥iril[ri¥2re] ---{re—1¥e7%] for all
lists of states r1,72,...,7%- As & could be close to n, and there could be close
to n states, the total number of productions grows as n”. We cannot carry out
such a construction for reasonably sized PDA’s if the PDA has even one long
stack string to write.

Fortunately, this worst case never has to occur. As was suggested by Ex-
ercise 6.2.8, we can break the pushing of a long string of stack symbols into a
sequence of at most n steps that each pushes one symbol. That is, if d(¢,a,X)
contains (19, ¥i Yo ---¥4), we may introduce new states po, p3,...,Px-1- Then,
we replace (ro, Y1Yo---¥s) in 6(q,0,X) by (ps1, ¥e-1 Ye), and introduce the
new transitions

6(pe-1, ¥e-1) = {(Pe-2, Ye-2¥e-1)}, S(pe—2; Ye—-2) = ((pe—3, Yaa ¥u-2)}

and so on, down to 6(pe, ¥2) = {(ro, ¥1 ¥2)}.

Now, no transition has more than two stack symbols. We have added at
most n new states, and the total length of all the transition rules of 6 has grown
by at most a constant factor; ie., it is still O(n). There are O(n) transition
rules, and each generates O(n?) productions, since there are only two states
that need to be chosen in the productions that come from each rule. Thus, the
constructed grammar has length O(n*) and can be constructed in cubic time.
We summarize this informal analysis in the theorem below.

Theorem 7.31: There is an O(n*) algorithm that takes a PDA P whose
representation has length n and produces a CFG of length at most O(n). This
CFG generates the same language as P accepts by empty stack. Optionally, we
can cause G to generate the language that P accepts by finai state. O

7.4.22 Running Time of Conversion to Chomsky Normal
Form

As decision algorithms may depend on first putting a CFG into Chomsky Nor-
mal Form, we should also look at the running time of the various algorithms
that we used to convert an arbitrary grammar to a CNF grammar. Most. of the
steps preserve, up to a constant factor, the length of the grammar’s description;
that is, starting with a grammar of length n they produce another grammar of
length O(n). The good news is summarized in the following list of observations:

1. Using the proper algorithm (See Section 7.4.3), detecting the reachable
and generating symbols of a grammar can be done in O(n) time. Elimi-
nating the resulting useless symbols takes O(n) time and does not increase
the size of the grammar.

2. Constructing the unit pairs and eliminating unit productions, as in Sec-
tion 7.1.4, takes O(n?) time and the resulting grammar has length O(n”).


--- Page 312 ---
296 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

3. The replacement of terminals by variables in production bodies, as in
Section 7.1.5 (Chomsky Normal Form), takes O(n} time and results in a
grammar whose length is O(n}.

4, The breaking of production bodies of length 3 or more into bodies of
length 2, as carried out in Section 7.1.4 also takes O(n) time and results
in a grammar of length O(n).

The bad news concerns the construction of Section 7.1.3, where we eliminate
e-productions. If we have a production body of length k, we could construct
from that one production 2* — 1 productions for the new grammar. Since k
could be proportional to n, this part of the construction could take O(2") time
and result in a grammar whose length is O(2").

To avoid this exponential blowup, we need only to bound the length of
production bodies. The trick of Section 7.1.5 can be applied to any production
bedy, not just. to one without terminals. Thus, we recommend, as a preliminary
step before eliminating e-productions, the breaking of all long production bodies
into a sequence of productions with bodies of length 2. This step takes O(n)
time and grows the grammar only lincarly. The construction of Section 7.1.3,
to eliminate e-productions, will work on bodies of length at most 2 in such a
way that the running time is O(n) and the resulting grammar has length O(n).

With this modification to the overall CNF construction, the only step that
ig not linear is the elimination of unit productions. As that step is O(n”), we
conclude the following:

Theorem 7.32: Given a grammar G of length n, we can find an equivalent
Chomsky-Normal-Form grammar for G in time O(n”); the resulting grammar
has length O(n?). O

7.4.3 Testing Emptiness of CFL’s

We have already seen the algorithm for testing whether a CFL £ is empty.
Given a granuar G for the language £, use the algorithm of Section 7.1.2 to
decide whether the start symbot S of G is generating, i.e., whether S derives
at least one string. E is empty if and only if 5 is not generating.

Because of the importance of this testi, we shall consider in detail how much
time it takes to find all the generating symbols of a grammar G. Suppose
the length of G is n. Then there could be on the order of n variables, and
each pass of the inductive discovery of generating variables could take O(n)
time to examine all the productions of G. If only one new generating variable
is discovered on each pass, then there could be O(n) passes. Thus, a naive
implementation of the generating-symbols test is O(n”).

However, there is a more careful algorithm that sets up a data structure in
advance to make our discovery of generating symbols take O(n) time only. The
data structure, suggested in Fig. 7.11, starts with an array indexed by the vari-
ables, as shown on the left, which tells whether or not we have established that


--- Page 313 ---
7.4, DECISION PROPERTIES OF CFL’S 297

the variable is generating. In Fig. 7.11, the array suggests that we have discov-
ered B is generating, but we do not know whether or not A is generating. At the
end of the algorithm, each question mark will become “no,” since any variable
not discovered by the algorithm to be generating is in fact nongenerating.

Generating?

Figure 7.11: Data structure for the linear-time emptiness test

The productions are preprocessed by setting up several kinds of useful links.
First, for each variable there is a chain of all the positions in which that vari-
able appears. For instance, the chain for variable B is suggested by the solid
lines. For each production, there is a count of the number of positions holding
variables whose ability to generate a terminal string has not yet been taken into
account. The dashed lines suggest links from the productions to their counts.
The counts shown in Fig. 7.11 suggests that we have not yet taken any of the
variables into account, even though we just established that 2 is generating.

Suppose that we have discovered that 8 is generating. We go down the list
of positions of the bodies holding B. For each such position, we decrement the
count for that production by 1; there is now one fewer position we need to find
generating in order to conclude that the variable at the head is also generating.

If a count reaches 0, then we know the head variable is generating. A
link, suggested by the dotted lines, gets us to the variable, and we may put
that variable on a queue of generating variables whose consequences need to be
explored (as we just. did for variable 8B). This queue is not shown.

We must argue that this algorithm takes O(n) time. The important. points
are as follows:

* Since there are at most n variables in a grammar of size n, creation and
initialization of the array takes O(n) time.

e There are at most m productions, and their total length is at most n, so
initialization of the links and counts suggested in Fig. 7.11 can be donc
in O(n) time.

e When we discover a production has count 0 (i.e., all positions of its body
are generating), the work involved can be put into two categorics:


--- Page 314 ---
298 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

Other Uses for the Linear Emptiness Test

The same data structure and accounting trick that we used in Section 7.4.3
to test whether a variable is generating can be used to make some of the
other tests of Section 7.1 linear-time. Two important examples are:

1. Which symbols are reachable?

2. Which symbols are nullable?

1. Work done for that production: discovering the count is 0, finding
which variable, say .4, is at the head, checking whether it is already
known to be generating, and putting it on the queue if not. All these
steps are O{1) for each production, and so at most O(n) work of this
type is done in total.

2. Work done when visiting the positions of the production bodies that
have the head variable A. This work is proportional to the aumber
of positions with 4. Therefore, the aggregate amount of work done
processing all generating symbols is proportional to the sum of the
lengths of the production bodies, and that is O(n).

We conclude that. the total work done by this algorithm is O(n).

7.4.4 Testing Membership in a CFL

We can also decide membership of a string w in a CFL £. There are several
inefficient ways to make the test; they take time that is exponential in |zw},
assuming a grammar or PDA for the language L is given and its size is treated
as a constant, independent of w. For instance, start by converting whatever
representation of ZL we arc given into a CNF grammar for L. As the parse trees
of a Chomsky-Normal-Form grammar are binary trees, if w is of length n then
there will be exactly 2n — 1 nodes labeled by variables in the tree (that result
has an easy, inductive proof, which we leave to you). The number of possible
trees and node-labelings is thus “ouly” exponertial in 1, 80 in principle we can
list thei all and check to see if any of them yields w.

There is a much more cficient technique based on the idea of “dynamic
programming,” which may also be known to you as a “table-filling algorithm”
or “tabulation.” This algorithm, known as the CYK Algorithm,’ starts with a
CONF grammar G = (V,T, P,S) for a language £. The input to the algorithm is
a string w = a,a9°+-a@py in T*. In O(n) time, the algorithm constructs a table

3Tt ig named after three people, eack of whom independently discovered essentially the
same idea: J. Cacke, D. Younger, and T, Kasami.


--- Page 315 ---
7.4. DECISION PROPERTIES OF CFL'S 299

that tells whether w is in £. Note that when computing this running time,
the grammar itself is considered fixed, and its size contributes only a constant
factor to the running time, which is measured in terms of the length of the
string w whose membership in ZL is being tested.

In the CYK algorithm, we construct a triangular table, as suggested in
Fig. 7.12. The horizontal axis corresponds to the positions of the string w =
G9 +**@n, which we have supposed has length 5. The table entry X;; is the
set of variables A such that A > aiai41°°-@;. Note in particular, that we are
interested in whether § is in the set X,,, because that is the same as saying
S = w, ie, w is in L.

4 4&4 & Y% 4%

Figure 7.12: The table constructed by the CYK algorithm

To fill the table, we work row-by-row, upwards. Notice that each row cor-
responds to one length of substrings; the bottom row is for strings of length 1,
the second-from-bottom row for strings of length 2, and so on, until the top row
corresponds to the one substring of length n, which is w itself. It takes O(n)
time to compute any one entry of the table, by a method we shall discuss next.
Since there are n(n + 1)/2 table entries, the whole table-construction process
takes O(n") time. Here is the algorithm for computing the X;;’s:

BASIS: We compute the first row as follows. Since the string beginning and
ending at position i is just the terminal a;, and the grammar is in CNF, the
only way to derive the string a; is to use a production of the form A — qj.
Thus, X;; is the set of variables A such that A - a; is a production of G.

INDUCTION: Suppose we want to compute X,;, which is in row j —i+1, and
we have computed all the X’s in the rows below. That is, we know about all
strings shorter than a;a;41 ---@;, and in particular we know about all proper
prefixes and proper suffixes of that string. As j — i > 0 may be assumed (since
the case 2 = 7 is the basis), we know that any derivation A => A¢Qj41 -** as Thust


--- Page 316 ---
300 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

start out with some step A = BC. Then, B derives some prefix of ajaj41 -+ + a;,
say B > @i4i41 °°: Gy, for some k < 7, Also, C' must then derive the remainder
of ajajoy--: Qj, that is, C > Apri dep2 + aj.

We conclude that in order for A to be in .X;;, we must find variables B and
C, and integer & such that:

Lisk<j.

2. Bis in Xiz.

3. Cisin Xp41 3.

4. A+ BC is a production of G.

Finding such variables A requires us to compare at most n pairs of previously
computed sets: (Xs,Xi41,3), (Xei41,Xi42,j), and so on, until (X;3-1, Xj).
The pattern, in which we go up the column below Xj; at the same time we go
down the diagonal, is suggested by Fig. 7.13.

Figure 7.13: Computation of Xj; requires matching the column below with the
diagonal to the right

Theorem 7.33: The algorithm described above correctly computes Xj; for all
i and j; thus w is in L(G) if and only if S is in X,,. Moreover, the running
time of the algorithm is O(n).

PROOF: The reason the algorithm finds the correct sets of variables was ex-
plained as we introduced the basis and inductive parts of the algorithm. For the
running time, note that there are O(n”) entries to compute, and each involves
comparing and computing with » pairs of entries. It is important to remember
that, although there can be many variables in each set X;;, the grammar G is
fixed and the number of its variables does not depend on n, the length of the
string w whose membership is being tested. Thus, the time to compare two
entries Xi, and X,41,;, and find variables to go into X;; is O(1). As there are
at most n such pairs for each X;;, the total work is O(n*). O


--- Page 317 ---
7.4. DECISION PROPERTIES OF CFL’S 301

Example 7.34: The following are the productions of a CNF grammar G:

S —+ AB|BC
A — BAle
B+ CC|b
C + ABla

We shall test for membership in £(G) the string baaba. Figure 7.14 shows the
table filled in for this string.

{S.A,C}

- (SA,G

. {B} {BY

{SA} {BE (SCE {S.A}

®@ {4G fAqG { {AQ

b a a b a
Figure 7.14: The table for string baaba constructed by the CYK algorithm

To construct the first (lowest) row, we use the basis rule. We have only to
consider which variables have a production body a (those variables are A and
C) and which variables have body 6 (only B does). Thus, above those positions
holding a we see the entry {A,C}, and above the positions holding 6 we sec
{B}. That is,X1, = X4q = {B}, and Xo) = Xg3 = Nsg5 = {A,C}.

In the second row we see the values of X12, X23, X34, and X45. For instance,
let us see how X12 is computed. There is only one way to break the string from
positions 1 to 2, which is ba, into two nonempty substrings. The first must be
position | and the second must be position 2. In order for a variable to generate
ba, it must have a body whose first variable is in X), = {8} (i.c., it generates
the 6) and whose second variable is in X22 = {A,C} (ie., it generates the a).
This body can only be BA or BC. If we inspect the grammar, we find that the
productions A 4 BA and S > BC are the only ones with these bodies. Thus,
the two heads, A and S, constitute X12.

For a more complex example, consider the computation of Y24. We can
break the string aab that occupies positions 2 through 4 by ending the first
string after position 2 or position 3. That is, we may choose k = 2 or k=3 in
the definition of X24. Thus, we must consider ali bodies in X22X34 U Xo3 X44.
This set of strings is {A, C}{5,C} U {B}{B} = {AS, AC, C'S,CC, BB}. Of the


--- Page 318 ---
302 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

five strings in this set, only CC is a body, and its head is B. Thus, Xo, = {B}.
D

7.4.5 Preview of Undecidable CFL Problems

In the next chapters we shall develop a remarkable theory that lets us prove
formally that there are problems we cannot solve by any algorithm that can
rum on a computer. We shall use it to show that a number of simple-to-state
questions about grammars and CFL’s have no algorithm; they are called “un-
decidable problems.” For now, we shall have to content ourselves with a list
of the most significant undecidable questions about context-free grammars and
languages. The following are undecidable:

1. Is a given CFG G ambiguous?

2. Is a given CFL inherently ambiguous?

3. Is the intersection of two CFL’s empty?

4. Are two CFL’s the same?

5. Is a given CFL equal to £*, where © is the alphabet of this language?

Notice that the flavor of question (1), about ambiguity, is somewhat different
from the others, in that it is a question about a grammar, not a language. All
the other questions assume that the language is represented by a grammar or
PDA, but the question is about the language(s) defined by the grammar or
PDA. For instance, in contrast to question (1), the second question asks, given
a grammar G (or a PDA, for that matter), does there exist some equivalent
grammar G" that is unambiguous. If G is itself unambiguous, then the answer
is surely “yes,” but if G is ambiguous, there could still be some other grammar
G’ for the same language that is unambiguous, as we learned about expression
grammars in Example 5.27.

7.4.6 Exercises for Section 7.4

Exercise 7.4.1: Give algorithms to decide the following:

* a) Is L(G) finite, for a given CFG G? Hint: Use the pumping lemma.
!b) Does L(G) contain at least 100 strings, for a given CFG G?

ic) Given a CFG G and one of its variables A, is there any sentential form
in which A is the first symbol. Note: Remember that it is possible for A
to appear first in the middle of some sentential form but then for all the
symbols to its left to derive e.

Exercise 7.4.2: Use the technique described in Section 7.4.3 to develop linear-
time algorithms for the following questions about CFG’s:


--- Page 319 ---
7.5. SUMMARY OF CHAPTER 7 303

a)
b)

Which symbols appear in some sentential form?

Which symbols are nullable (derive ¢)?

Exercise 7.4.3: Using the grammar G of Example 7.34, use the CYK algo-
rithm to determine whether each of the following strings is in L(G):

* a)
b)
¢)

ababa.
baaad.

aabab.

* Exercise 7.4.4: Show that in any CNF grammar, all parse trees for strings of
length n have 2n —1 interior nodes (i.e., 2n — 1 nodes with variables for labels).

! Exercise 7.4.5: Modify the CYK algorithm so can report the number of dis-
tinct parse trees for the given input, rather than just reporting membership in
the language.

7.5

+

Summary of Chapter 7

Eliminating Useless Symbols: A variable can be eliminated from a CFG
unless it derives some string of terminals and also appears in at least
one string derived from the start symbol. To correctly eliminate such
useless symbols, we must first test whether a variable derives a terminal
string, and eliminate those that do not, along with all their productions.
Only then do we eliminate variables that are not derivable from the start
symbol.

Eliminating e- and Unit-productions: Given a CFG, we can find another
CFG that generates the same language, except for string ¢, yet has no
productions (those with body ¢€) or unit productions (those with a single
variable as the body).

Chomsky Nermal Form: Given a CFG that derives at least one nonempty
string, we can find another CFG that generates the same language, except
for e, and is in Chomsky Normal Form: there are no useless symbols, and
every production body consists of either two variables or one terminal.

The Pumping Lemma: In any CFL, it is possible to find, in any sufficiently
long string of the language, a short substring such that the two ends of
that substring can be “pumped” in tandem; i.e., each can be repeated any
desired number of times. The strings being pumped are not both e. This
lemma, and a more powerful version called Ogden’s lemma mentioned in
Exercise 7.2.3, allow us to prove many languages not to be context-free.


--- Page 320 ---
304 CHAPTER 7. PROPERTIES OF CONTEXT-FREE LANGUAGES

+ Operations That Preserve Contest-Free Languages: The CFL’s are closed
under substitution, union, concatenation, closure (star), reversal, and in-
verse homomorphisms. CFL's are not closed under intersection or com-
plementation, but the intersection of a CFL and a regular language is
always a CFL.

+ Testing Emptiness of a CFL: Given a CFG, there is an algorithm to tell
whether it generates any strings at all. A careful implementation allows
this test. to be conducted in time that is proportional to the size of the
grammar itself.

* Testing Membership in a CFL: The Cocke-Younger-Kasami algorithm
tells whether a given string is in a given context-free language. For a
fixed CFL, this test takes time O{n*), if m is the length of the string
being tested.

7.6 References for Chapter 7

Chomsky normal form comes from [2]. Greibach normal form is from [4], al-
though the construction outlined in Exercise 7.1.11 is due to M. C. Paull.

Many of the fundamental properties of context-free languages come from [1].
These ideas include the pumping lemma, basic closure properties, and tests for
simple questions such as emptiness and finiteness of a CFL. In addition [6] is
the source for the nonclosure under intersection and complementation, and [3]
provides additional closure results, including closure of the CFL’s under inverse
homomorphism. Ogden’s lemma comes from [5].

The CYK algorithm has three known independent sources. J. Cocke’s work
was circulated privately and never published. T. Kasami’s rendition of essen-
tially the same algorithm appeared only in an internal US-Air-Force memoran-
dum. However, the work of D. Younger was published conventionally (7].

1. Y. Bar-Hillel, M. Perles, and E. Shamir, “On formal properties of simple
phrase-structure grammars,” 2. Phonetik. Sprachwiss. Kommunikations-
fersch. 14 (1961), pp. 143-172.

2. N. Chomsky, “On certain formal properties of grammars,” Information
and Control 2:2 (1959), pp. 137-167.

3. 5. Ginsburg and G. Rose, “Operations which preserve definability in lan-
guages,” J. ACM 10:2 (1963), pp. 175-195.

4. §. A. Greibach, “A new normal-form theorem for context-free phrase
structure grammars,” J. ACM 12:1 (1965), pp. 42-52.

Dd. W. Ogden, “A helpful result for proving inherent ambiguity,” Mathemat-
ical Systems Theory 2:3 (1969), pp. 31-42.


--- Page 321 ---
7.6. REFERENCES FOR CHAPTER 7 305

6. S$. Scheinberg, “Note on the boolean properties of context-free languages,”
Information and Control 3:4 (1960), pp. 372-375.

7. D.H. Younger, “Recognition and parsing of context-free languages in time
n?.* Information and Control 10:2 (1967), pp. 189-208.


--- Page 322 ---


--- Page 323 ---
Chapter 8

Introduction to Turing
Machines

In this chapter we change our direction significantly. Until now, we have been
interested primarily in simple classes of languages and the ways that they can
be used for relatively constrained problems, such as analyzing protocols, search-
ing text, or parsing programs. Now, we shall start looking at the question of
what languages can be defined by any computational device whatsoever. This
question is tantamount to the question of what computers can do, since recog-
nizing the strings in a language is a formal way of expressing any problem, and
solving a problem is a reasonable surrogate for what it is that computers do.

We begin with an informal argument, using an assumed knowledge of C
programming, to show that there are specific problems we cannot solve using
a computer. These problems are called “undecidable.” We then introduce a
venerable formalism for computers, called the Turing machine. While a Turing
machine looks nothing like a PC, and would be grossly inefficient should some
startup company decide to manufacture and sell them, the Turing machine long
has been recognized as an accurate model for what any physical computing
device is capable of doing.

In Chapter 9, we use the Turing machine to develop a theory of “undecid-
able” problems, that is, problems that no computer can solve. We show that
a number of problems that are easy to express are in fact undecidable. An ex-
ample is telling whether a given grammar is ambiguous, and we shall see many
others.

8.1 Problems That Computers Cannot Solve
The purpose of this section is to provide an informal, C-programming-based

introduction to the proof of a specific problem that computers cannot solve.
The particular problem we discuss is whether the first thing a C program prints

307


--- Page 324 ---
308 CHAPTER & INTRODUCTION TO TURING MACHINES

is hello, world. Although we might imagine that simulation of the program
would allow us to tell what the program does, we must in reality contend with
programs that take an unimaginably long time before making any output at
all. This problem — not knowing when, if ever, something will occur — is the
ultimate cause of our inability to tell what a program does. However, proving
formally that there is no program to do a stated task is quite tricky, and we
need to develop some formal mechanics. In this section, we give the intuition
behind the formal proofs.

8.1.1 Programs that Print “Hello, World”

In Fig. 8.1 is the first C program met by students who read Kernighan and
Ritchie’s classic book.’ It is rather easy to discover that this program prints
hello, world and terminates. This program is so transparent that it has
become a common practice to introduce languages by showing how to write a
program to print hello, world in those languages.

main{)
{

printf ("hello, world\n");
}

Figure 8.1: Kernighan and Ritchie’s hello-world program

However, there are other programs that also print hello, world; yet the
fact that they do so is far from obvious. Figure 8.2 shows another program that
might print hello, world. It takes an input n, and looks for positive integer
solutions to the equation 2” +y” = 2”. If it finds one, it prints hello, world.
If it never finds integers z, y, and z to satisfy the equation, then it continues
searching forever, and never prints hello, world.

To understand what this program does, first observe that exp is an auxiliary
function to compute exponentials. The main program needs to search through
triples (x, y, z) in an order such that we are sure we get to every triple of positive
integers eventually. To organize the search properly, we use a fourth variable,
total, that starts at 3 and, in the while-loop, is increased one unit at a time,
eventually reaching any finite integer. Inside the while-loop, we divide total
into three positive integers x, y, and z, by first allowing x to range from I to
total-2, and within that for-loop allowing y to range from 1 up to one less
than what « has not already taken from total. What remains, which must be
between 1 and total-2, is given to z.

In the innermost loop, the triple (x,y, z) is tested to see if x7? +y” = 2”. If
so, the program prints hello, world, and if not, it prints nothing.

1B. W. Kernighan and D. M. Ritchie, The C Programming Language, 1978, Prentice-Hall,
Englewood Cliffs, NJ.


--- Page 325 ---
8.1. PROBLEMS THAT COMPUTERS CANNOT SOLVE 309

int exp(int i, n)
/* computes i to the power n +/

{
int ans, j;
ans = 1;
for (j=1; j<=n; j++) ans *= 1;
return (ans) ;
is
Main ()
{
int n, total, x, y, 23
scanf("4d", &n);
total = 3;
while (1) {
for (x=1; x<=total-2; x++)
for (y=1; y<=total-x-1; yr+) {
2 = total - x7 ¥3
if (exp(x,n) + exp(y,n) == exp(z,n))
printf("hello, world\n");
}
total++;:
}
}

Figure 8.2: Fermat’s last theorem expressed as a hello-world program

If the value of x that the program reads is 2, then it will eventually find
combinations of integers such as total = 12, x = 3, y = 4, and z = 5, for which
g” + y” =z”. Thus, for input 2, the program does print hello, world.

However, for any integer n > 2, the program will never find a triple of
positive integers to satisfy x” + y” = 2", and thus will fail to print hello,
world. Interestingly, until a few years ago, it was not known whether or not this
program would print hello, world for some large integer n. The claim that it
would not, i.e., that there are no integer solutions to the equation x” +y" = 2”
if n > 2, was made by Fermat 300 years ago, but no proof was found until quite
recently. This statement is often referred to as “Fermat's last theorem.”

Let us define the hello-world problem to be: determine whether a given C
program, with a given input, prints hello, world as the first 12 characters
that it prints. In what follows, we often use, as a shorthand, the statement
about a program that it prints hello, world to mean that it prints hello,
world as the first 12 characters that it prints.

It seems likely that, if it takes mathematicians 300 years to resolve a question


--- Page 326 ---
310 CHAPTER 8 INTRODUCTION TO TURING MACHINES

Why Undecidable Problems Must Exist

While it is tricky to prove that a specific problem, such as the “hello-
world problem” discussed here, must be undecidable, it is quite easy to
see why almost all problems must be undecidable by any system that
involves programming. Recall that a “problem” is really membership of a
string in a language. The number of different languages over any alphabet.
of more than one symbol is not countable. That is, there is no way to
assign integers to the languages such that every language has an integer,
and every integer is assigned to one language.

On the other hand programs, being finite strings over a finite alphabet
(typically a subset of the ASCII alphabet), are countable. That is, we can
order them by length, and for programs of the same length, order them
lexicographically. Thus, we can speak of the first program, the second
program, and in general, the ith program for any integer 7.

As aresult, we know there are infinitely fewer programs than there are
problems. If we picked a language at random, almost certainly it would be
an undecidable problem. The only reason that most problems appear to be
decidable is that we rarely are interested in random problems. Rather, we
tend to look at fairly simple, well-structured problems, and indeed these
are often decidable. However, even among the problems we are interested
in and can state clearly and succinctly, we find many that are undecidable;
the hello-world problem is a case in point.

about a single, 22-line program, then the general problem of telling whether a
given program, on a given input, prints hello, world must be hard indeed.
In fact, any of the problems that mathematicians have not yet been able to
resolve can be turned into a question of the form “does this program, with this
input, print hello, world? Thus, it would be remarkable indeed if we could
write a program that could examine any program P and input J for P, and tell
whether P, run with J as its input, would print hello, world. We shall prove
that no such program exists.

8.1.2 The Hypothetical “Hello, World” Tester

The proof of impossibility of making the hello-world test is a proof by contra-
diction. That is, we assume there is a program, call it H, that takes as input
a program P and an input J, and tells whether P with input J prints hello,
world. Figure 8.3 is a representation of what H does. In particular, the only
output H makes is either to print the three characters yes or to print the two
characters no. It always does one or the other.

Ifa problem has an algorithm like H, that always tells correctly whether an
instance of the problem has answer “yes” or “no,” then the problem is said to


--- Page 327 ---
81. PROBLEMS THAT COMPUTERS CANNOT SOLVE 311

fi Hello-world
tester yes
P H no

Figure 8.3: A hypothetical program H that is a hello-world detector

be “decidable.” Otherwise, the problem is “undecidable.” Our goal is to prove
that H doesn’t exist; i.e., the hello-world problem is undecidable.

In order to prove that statement by contradiction, we are going to make
several changes to H, eventually constructing a related program called Hz that
we show does not exist. Since the changes to H are simple transformations that
can be done to any C program, the only questionable statement is the existence
of H, so it is that assumption we have contradicted.

To simplify our discussion, we shall make a few assumptions about C pro-
grams. These assumptions make H’s job easier, not harder, so if we can show
a “hello-world tester” for these restricted programs does not exist, then surely
there is no such tester that could work for a broader class of programs. Our
assumptions are:

1. All output is character-based, e.g., we are not using a graphics package
or any other facility to make output that is not in the form of characters.

2. All character-based output is performed using printf, rather than put-
char() or another character-based output function.

We now assume that the program H exists. Our first modification is to
change the output no, which is the response that H makes when its input
program FP does not print hello, world as its first output in response to mput
I. As soon as H prints “n,” we know it will eventually follow with the “o.”?
Thus, we can modify any printf statement in H that prints “n” to instead
print hello, world. Another printf statement that prints an “o” but not
the “n” is omitted. As a result, the new program, which we call H,, behaves
like H, except it prints hello, world exactly when H would print no. Hj is
suggested by Fig. 8.4.

Our next transformation on the program is a bit trickier; it is essentially
the insight that allowed Alan Turing to prove his undecidability result about
Turing machines. Since we are really interested in programs that take other
programs as input and tell something about them, we shall restrict Hj so it:

a) Takes only input P, not P and J.

? Most likely, the program would put no in one printf, but it could print the “n” in one
printf and the “o” in another.


--- Page 328 ---
312 CHAPTER 8. INTRODUCTION TO TURING MACHINES

yes

hello, world

Figure 8.4: A, behaves like H, but it says hello, world instead of no
b) Asks what P would do if its input were its own code, i.e., what would H;
do on inputs P as program and P as input I as well?

The modifications we must perform on H, to produce the program Ho sug-
gested in Fig. 8.5 are as follows:

1. He first reads the entire input P and stores it in an array A, which it
“malloc’s” for the purpose.?

2. Hy then simulates H,, but whenever H, would read input from P or 7,
Hy reads from the stored copy in A. To keep track of how much of FP and
i Hy has read, Hz can maintain two cursors that mark positions in A.

yes
P H, K
hello, world

Figure 8.5: Hz behaves like H,, but uses its input P as both P and f

We are now ready to prove H» cannot exist. Thus, H, does not exist, and
likewise, H does not exist. The heart of the argument is to envision what Hy.
does when given itself as input. This situation is suggested in Fig. 8.6. Recall
that Ho, given any program P as input, makes output yes if P prints hello,
world when given itself as input. Also, Hy prints hello, world if P, given
itself as input, does not print hello, world as its first output.

Suppose that the H2 represented by the box in Fig. 8.6 makes the output
yes. Then the Mo in the box is saying about its input He that Ho, given itself

*The UNIX mailoc system function allocates a block of memory of a size specified in
the call to malloc. This function is used when the amount of storage needed cannot be
determined until the program is run, a8 would be the case if an input of arbitrary length were
read. ‘T'ypically, malloc would be called several times, as more and more input is read and
progressively more space is needed.


--- Page 329 ---
8.1. PROBLEMS THAT COMPUTERS CANNOT SOLVE 313

yes

hello, world

Figure 8.6: What does Hz do when given itself as input?

as input, prints hello, world as its first output. But we just supposed that
the first output H2 makes in this situation is yes rather than hello, world.

Thus, it appears that in Fig. 8.6 the output of the box is hello, world,
since it must be one or the other. But if Hy, given itself as input, prints hello,
world first, then the output of the box in Fig. 8.6 must be yes. Whichever
output we suppose Hy, makes, we can argue that it makes the other output.

This situation is paradoxical, and we conclude that A cannot exist. As a
result, we have contradicted the assumption that H exists. That is, we have
proved that no program H can tell whether or not a given program P with
input J prints hello, world as its first output.

8.1.3 Reducing One Problem to Another

Now, we have one problem — does a given program with given input print
hello, worldas the first thing it prints? — that we know no computer prograz
can solve. A problem that cannot be solved by computer is called undecidable.
We shall give the formal definition of “undecidable” in Section 9.3, but for the
moment, let us use the term informally. Suppose we want to determine whether
or not some other problem is solvable by a computer. We can try to write a
program to solve it, but if we cannot figure out how to do so, then we might
try a proof that there is no such program.

Perhaps we could prove this new problem undecidable by a technique similar
to what we did for the hello-world problem: assume there is a program to solve
it and develop a paradoxical program that must do two contradictory things,
like the program Hz. However, once we have one problem that we know is
undecidable, we no longer have to prove the existence of a paradoxical situation.
It is sufficient to show that if we could solve the new problem, then we could use
that solution to solve a problem we already know is undecidable. The strategy
is suggested in Fig. 8.7; the technique is called the reduction of P, to Po.

Suppose that we know problem P, is undecidable, and P, is a new problem
that we would like to prove is undecidable as well. We suppose that there is a
program represented in Fig. 8.7 by the diamond labeled “decide”; this program
prints yes or no, depending on whether its input instance of problem P, is or
is not in the language of that problem.?

4Recall that a problem is really a language. When we talked of the problem of deciding


--- Page 330 ---
314 CHAPTER 8. INTRODUCTION TO TURING MACHINES

P Construct , Decide yes
instance instance
ne

Figure 8.7: If we could solve problem P2, then we could use its solution to solve
problem P,

In order to make a proof that problem P» is undecidable, we have to invent a
construction, represented by the square box in Fig. 8.7, that converts instances
of P, to instances of that have the same answer. That is, any string in the
language P, is converted to some string in the language J», and any string over
the alphabet of P, that is not in the language P, is converted to a string that
is not in the language Ps. Once we have this construction, we can solve P, as
follows:

1. Given an instance of P,, that is, given a string w that may or may not be
in the language /, apply the construction algorithm to produce a string
z.

2. Test whether x is in FP), and give the same answer about w and P,.

If w is in P,, then z is in P, so this algorithm says yes. If w is not in P,,
then a is not in P2, and the algorithm says no. Either way, it says the truth
about w. Since we assumed that no algorithm to decide membership of a string
in P, exists, we have a proof by contradiction that the hypothesized decision
algorithm for P, does not exist; i.e., P. is undecidable.

Example 8.1: Let us use this methodology to show that the question “does
program Q, given input y, ever call function foo” is undecidable. Note that Q
may not have a function foo, in which case the problem is easy, but the hard
cases are when @ has a function foo but may or may not reach a call to foo with
input y. Since we only know one undecidable problem, the role of P, in Fig. 8.7
will be played by the hello-world problem. P» will be the calls-foo problem just
mentioned. We suppose there is a program that solves the calls-foo problem.
Our job is to design an algorithm that converts the hello-world problem into
the calls-foo problem.

That is, given program @ and its input y, we must construct a program R
and an input z such that R, with input z, calls foo if and only if Q@ with input
y prints hello, world. The construction is not hard:
whether a given program and input results in helle, world as the first output, we were really

talking about strings consisting of a C source program followed by whatever input file(s) the
program reads. This set of strings ia a language over the alphabet of ASCII characters.


--- Page 331 ---
8.1. PROBLEMS THAT COMPUTERS CANNOT SOLVE 315

Can a Computer Really Do All That?

If we examine a program such as Fig. 8.2, we might ask whether it really
searches for counterexamples to Fermat's last theorem. After all, integers
are only 32 bits Jong in the typical computer, and if the smallest counterex-
ample involved integers in the billions, there would be an overflow error
before the solution was found. In fact, one could argue that a computer
with 128 megabytes of main memory and a 30 gigabyte disk, has “only”
9620128000000 states, and is thus a finite automaton.

However, treating computers as finite automata {or treating brains
as finite automata, which is where the FA idea originated), is unproduc-
tive. The number of states involved is so large, and the limits so unclear,
that you don’t draw any useful conclusions. In fact, there is every Teason
to believe that, if we wanted to, we could expand the set of states of a
computer arbitrarily.

For instance, we can represent integers as linked lists of digits, of
arbitrary length. If we run out of memory, the program can print a request
for a human to dismount its disk, store it, and replace it by an empty disk.
As time goes on, the computer could print requests to swap among as many
disks as the computer needs. This program would be far more complex
than that of Fig. 8.2, but not beyond our capabilities to write. Similar
tricks would allow any other program to avoid finite limitations on the size
of memory or on the size of integers or other data items.

1. If Q has a function called foo, rename it and all calls to that function.
Clearly the new program Q, does exactly what @ does.

9. Add to Q, a function foo. This function does nothing, and is not called.
The resulting program is Q2.

3. Modify Qs to remember the first 12 characters that it prints, storing them
in a global array A. Let the resulting program be Qs.

4. Modify Q3 so that whenever it executes any output statement, it then
checks in the array A to see if it has written 12 characters or more, and
if so, whether helio, world are the first 12 characters. In that case, call
the new function foo that was added in item (2). The resulting program
is R, and input z is the same as y.

Suppose that Q with input y prints hello, world as its first output. Then
R as constructed will call foo. However, if Q with input y does not print
helio, world as its first output, then FR will never call foo. If we can decide
whether R with input z calls foo, then we also know whether @ with input y
(remember y = z) prints helle, world. Since we know that no algorithm to


--- Page 332 ---
316 CHAPTER 8. INTRODUCTION TO TURING MACHINES

The Direction of a Reduction Is Important

It is a common mistake to try to prove a problem P, undecidable by
reducing P, to some known undecidable problem P,; i.e., showing the
statement “if P, is decidable, then P, is decidable.” That statement,
although surely true, is useless, since its hypothesis “P, is decidable” is

false.

The only way to prove a new problem / to be undecidable is to
reduce a known undecidable problem P,; to Pz. That way, we prove the
statement “if Po is decidable, then P, is decidable.” The contrapositive of
that statement is “if P, is undecidable, then P, is undecidable.” Since we
know that P, undecidable, we can deduce that PF. is undecidable.

decide the hello-world problem exists, and all four steps of the construction of
R from @ could be carried out by a program that edited the code of programs,
our assumption that there was a calls-foo tester is wrong. No such program
exists, and the calls-foo problem is undecidable. O

8.1.4 Exercises for Section 8.1

Exercise 8.1.1: Give reductions from the hello-world problem to each of the
problems below. Use the informal style of this section for describing plausi-
ble program transformations, and do not worry about the real limits such as
maximum file size or memory size that real computers impose.

*! a) Given a program and an input, does the program eventually halt; ie.,
does the program not loop forever on the input?

b) Given a program and an input, does the program ever produce any out-
put?

'c) Given two programs and an input, do the programs produce the same
output for the given input?

8.2 The Turing Machine

The purpose of the theory of undecidable problems is not only to establish the
existence of such problems — an intellectually exciting idea in its own right —
but to provide guidance to programmers about what they might or might not be
able to accomplish through programming. The theory also has great pragmatic
impact when we discuss, as we shall in Chapter 10, problems that although
decidable, require large amounts of time to solve them. These problems, called
“intractable problems,” tend to present greater difficulty to the programmer


--- Page 333 ---
8.2. THE TURING MACHINE 317

and system designer than do the undccidable problems. The reason is that,
while undecidable problems are usually quite obviously so, and their solutions
are rarely attempted in practice, the intractable problems are faced every day.
Moreover, they often yield to small modifications in the requirements or to
heuristic solutions. Thus, the designer is faced quite frequently with having to
decide whether or not a problem is in the intractable class, and what to do
about it, if so.

We need tools that will allow us to prove everyday questions undecidable or
intractable. The technology introduced in Section 8.1 is useful for questions that
deal with programs, but it does not translate easily to problems in unrelated
domains. For example, we would have great difficulty reducing the hello-world
problem to the question of whether a grammar is ambiguous.

As a result, we need to rebuild our theory of undecidability, based not on
programs in C or another language, but based on a very simple model of a com-
puter, called the Turing machine. This device is essentially a finite automaton
that has a single tape of infinite length on which it may read and write data.
One advantage of the Turing machine over programs as representation of what
can be computed is that the Turing machine is sufficiently simple that we can
represent its configuration precisely, using a simple notation much like the ID’s
of a PDA. In comparison, while C programs have a state, involving all the vari-
ables in whatever sequence of function calls have been made, the notation for
describing these states is far too complex to allow us to make understandable,
formal proofs.

Using the Turing machine notation, we shall prove undecidable certain prob-
lems that appear unrelated to programming. For instance, we shail show in
Section 9.4 that “Post’s Correspondence Problem,” a simple question involving
two lists of strings, is undecidable, and this problem makes it easy to show
questions about grammars, such as ambiguity, to be undecidable. Likewise,
when we introduce intractable problems we shall find that certain questions,
seemingly having little to do with computation (e.g., satisfiability of boolean
formulas), are intractable.

8.2.1 The Quest to Decide All Mathematical Questions

At the turn of the 20th century, the mathematician D. Hilbert asked whether
it was possible to find an algorithm for determining the truth or falsehood of
any mathematical proposition. In particular, he asked if there was a way to
determine whether any formula in the first-order predicate calculus, applied
to integers, was true. Since the first-order predicate calculus of integers is
sufficiently powerful to express statements like “this grammar is ambiguous,”
or “this program prints hello, world,” had Hilbert been successful, these
problems would have algorithms that we now know do not exist.

However, in 1931, K. Gédel published his famous incompleteness theorem.
He constructed a formula in the predicate calculus applied to integers, which
asserted that the formula itself could be neither proved nor disproved within


--- Page 334 ---
318 CHAPTER 8& INTRODUCTION TO TURING MACHINES

the predicate calculus. G6del’s technique resembles the construction of the
seli-contradictory program 2 in Section 8.1.2, but deals with functions on the
integers, rather than with C programs.

The predicate calculus was not the only notion that mathematicians had for
“any possible computation.” In fact predicate calculus, being declarative rather
than computational, had to compete with a variety of notations, including the
“partial-recursive functions,” a rather programming-language-like notation, and
other similar notations. In 1936, A. M. Turing proposed the Turing machine
as a model of “any possible computation.” This model is computer-like, rather
than program-like, even though true electronic, or even clectromechanical com-
puters were several years in the future (and Turing himself was involved in the
construction of such a machine during World War II).

Interestingly, all the serious proposals for a madel of computation have the
same power; that is, they compute the same functions or recognize the same
languages. The unprovable assumption that any gencral way to compute will
allow us to compute only the partial-recursive functions (or equivalently, what
Turing machines or modern-day computers can compute) is known as Church's
hypothesis (after the logician A. Church) or the Church-Turing thesis.

8.2.2. Notation for the Turing Machine

We may visualize a Turing machine as in Fig. 8.8. The machine consists of
a fintte control, which can be in any of a finite set of states. There is a tape
divided into squares or cells; each cell can hold any one of a finite number of
symbols.

Finite
control

Figure 8.8: A Turing machine

Initially, the input, which is a finite-length string of symbols chosen from the
input alphabet, is placed on the tape. All other tape cells, extending infinitely
to the left and right, initially hold a special symbol called the blank. The blank
is a tape symbol, but not an input symbol, and there may be other tape symbols
besides the input symbols and the blank, as well.

There is a tape head that is always positioned at one of the tape cells. The
Turing machine is said to be scanning that cell. Initially, the tape head is at


--- Page 335 ---
8.2.

THE TURING MACHINE 319

the leftmost cell that holds the input.
A move of the Turing machine is a function of the state of the finite control
and the tape symbol scanned. In one move, the Turing machine will:

1.

Change state. The next state optionally may be the same as the current
state.

. Write a tape symbol in the cell scanned. This tape symbol replaces what-

ever symbol was in that cell. Optionally, the symbol written may be the
same as the symbol currently there.

. Move the tape head left or right. In our formalism we require a move,

and do not allow the head to remain stationary. This restriction does
not constrain what a Turing machine can compute, since any sequence of
moves with a stationary head could be condensed, along with the next
tape-head move, into a single state change, a new tape symbol, and a
move left or right.

The formal notation we shall use for a Turing machine (TM) is similar to
that used for finite automata or PDA’s. We describe a TM by the 7-tuple

whose components have the following meanings:

Q:

fo:

P:

The finite set of states of the finite control.

=: The finite set of input symbols.
r:
6

The complete set of tape symbols; & is always a subset of [.

: The transition function. The arguments of 6(q,X) are a state q and a

tape symbol X, The value of 6(q, X}, if it is defined, is a triple (p, Y, D),
where:
1. pis the next state, in Q.

2. Y is the symbol, in I’, written in the cell being scanned, replacing
whatever symbol was there.

3. Dis a direction, either L or R, standing for “left” or “right,” respec-
tively, and telling us the direction in which the head moves.

The start state, a member of Q, in which the finite control is found initially.

: The blank symbol. This symbol is in T but not in 4; 1e., it is not an input

symbol. The blank appears initially in all but the finite number of initial
cells that hold input symbols.

The set of final or accepting states, a subset of Q.


--- Page 336 ---
320 CHAPTER & INTRODUCTION TO TURING MACHINES

8.2.3 Instantaneous Descriptions for Turing Machines

In order to describe formally what a Turing machine does, we need to develop
a notation for configurations or instantaneous descriptions (ID's), like the no-
tation we developed for PDA’s. Since a TM, in principle, has an infinitely long
tape, we might imagine that it is impossible to describe the configurations of a
TM succinctly. However, after any finite number of moves, the TM can have
visited only a finite number of cells, even though the number of cells visited can
eventually grow beyond any finite limit. Thus, in every ID, there is an infinite
prefix and an infinite suffix of cells that have never been visited. These cells
must all hold either blanks or one of the finite number of input symbols. We
thus show in an ID only the cells between the leftmost and the rightmost non-
blanks. Under special conditions, when the head is scanning one of the leading
or trailing blanks, a finite number of blanks to the left or right of the nonblank
portion of the tape must also be included in the ID.

In addition to representing the tape, we must represent the finite control and
the tape-head position. To do so, we embed the state in the tape, and place it
immediately to the left of the cell scanned. To disambiguate the tape-plus-state
string, we have to make sure that we do not use as a state any symbol that
is also a tape symbol. However, it is easy to change the names of the states
so they have nothing in common with the tape symbols, since the operation of
the TM does not depend on what the states are called. Thus, we shall use the
string X1X--+ Aj1@X;iXii1°+:X,y to represent an ID in which

l. g is the state of the Turing machine.
2. The tape head is scanning the ith symbol from the left.

3. XX2---X, is the portion of the tape between the leftmost and the right-
most nonblank. As an exception, if the head is to the left of the leftmost
nonblank or to the right of the rightmost nonblank, then some prefix or
suffix of X, X2---X,y will be blank, and i will be 1 or n, respectively.

We describe moves of a Turing machine M = (Q,,T,6,go,B,F) by the F

M
notation that was used for PDA’s. When the TM /M is understood, we shall
use just F to reflect moves. As usual, F , or just F , will be used to indicate

zero, one, or more moves of the TM M.
Suppose d(g¢,.X;) = (p, Y, L); i.e., the next move is leftward. Then

Ky XQ +> X19 Xe Xj41 +++ Xn "i NXg +++ Xy-epXi_-1Y Xign + Xp

Notice how this move reflects the change to state p and the fact that the tape
head is now positioned at cell i— 1. There are two important exceptions:

1. If¢ =1, then M moves to the blank to the left of X,. In that case,
gQX1X2°--Xn  PBYX2---Xn


--- Page 337 ---
8.2. THE TURING MACHINE 321

2. Ifi =n and Y = B, then the symbol B written over X, joins the infinite
sequence of trailing blanks and does not appear in the next ID. Thus,

XXo-°*Xn-19Xn Ye XX --+ Xy_apXy—-1

Now, suppose 4(g, X;) = (p, Y, R); i.e., the next move is rightward. Then
Ky Xa-++ Xy-1g9XiXign ++ Xn i XX 0+ Ki VY PXi41 + Xn

Here, the move reflects the fact that the head has moved to cell i+ 1. Again
there are two important exceptions:

1. If i =n, then the i + 1st cell holds a blank, and that cell was not part of
the previous ID. Thus, we instead have

XX . ++ Xn-1gAn Ay Xe “* -Xy-1Y pB

2. Ifi=1 and Y = B, then the symbol B written over X] joins the infinite
sequence of leading blanks and does not appear in the next ID. Thus,

QX1X9+--Xn bk pX2---Xn

Example 8.2: Let us design a Turing machine and see how it behaves on a
typical input. The TM we construct will accept the language {0"1" | n > 1}.
Initially, it is given a finite sequence of 0’s and 1’s on its tape, preceded and
followed by an infinity of blanks. Alternately, the TM will change a 0 to an X
and then a 1 to a Y, until all 0’s and 1’s have been matched.

In more detail, starting at the left end of the input, it repeatedly changes
a Oto an X and moves to the right over whatever 0’s and Y’s it sees, until it
comes to a 1. It changes the 1 to a Y, and moves left, over Y’s and 0’s, until
it finds an X. At that point, it looks for a 0 immediately to the right, and if it
finds one, changes it to X and repeats the process, changing a matching 1 to a
¥Y.

If the nonblank input is not in 0“ 1*, then the TM will eventually fail to have
a next move and will die without accepting. However, if it finishes changing all
the 0’s to X’s on the same round it changes the last 1 to a Y, then it has found
its input to be of the form 0"1" and accepts. The formal specification of the
TM A is

where é is given by the table in Fig. 8.9.

As M performs its computation, the portion of the tape, where A/’s tape
head has visited, will always be a sequence of symbols described by the regular
expression X*0*Y*1t". That is, there will be some 0’s that have been changed


--- Page 338 ---
322 CHAPTER 8 INTRODUCTION TO TURING MACHINES

State 6 1 XxX Y B

(qi, X, R) (a3, ¥, R)
3 ~ ~~ ~ (qs, Y, Rt} (qa, B, R)
Ys - - - - -

Figure 8.9: A Turing machine to accept {0"1" | n> 1}

to X’s, followed by some 0’s that have not yet been changed to X’s. Then there
are some 1's that were changed to Y’s, and 1’s that have not yet been changed
to Y’s. There may or may not be some 0’s and 1’s following.

State gg is the initial state, and AM also enters state gg every time it returns
to the leftmost remaining 0. If M is in state gg and scanning a 0, the rule in
the upper-left corner of Fig. 8.9 tells it to go to state g,, change the 0 to an X,
and move right. Once in state q,, Mf keeps moving right over all 0’s and Y’s
that it finds on the tape, remaining in state q,. If Mf sees an X or a B, it dies.
However, if Af sees a 1 when in state ¢1, it changes that 1 to a Y, enters state
gz, and starts moving left.

In state ge, M moves left over 0’s and Y’s, remaining in state q.. When
M reaches the rightmost A, which marks the right end of the block of 0’s that
have already been changed to X, M returns to state gj and moves right. There
are two cases:

1. If A¢ now sees a 0, then it repeats the matching cycle we have just de-
scribed.

2. If Af sees a Y, then it has changed all the 0’s to X’s. If all the I’s have
been changed to Y's, then the input was of the form 0"1", and M should
accept. Thus, 4 enters state gg, and starts moving right, over Y’s. If
the first symbol other than a Y that Af sees is a blank, then indeed there
were an equal number of 0’s and 1's, so Af enters state g4 and accepts.
On the other hand, if M encounters another 1, then there are too many
1’s, so M dies without accepting. If it encounters a 0, then the input was
of the wrong form, and M also dies.

Here is an example of an accepting computation by @. Its input is 0011.
Initially, AZ is in state go, scanning the first 0, 1.e., M4’s initial ID is ggQ021.
The entire sequence of moves of M is:

qo00L1 Xq,011 + XOq 11+ Xq20V1F g@X0Y1F
Xqo0VIE XXMYIEXXY¥qlbXXq@¥YY bt XeXYV +
XXqoVY¥ + XXY¥qY | XXVY¥qBLXXYYBaB


--- Page 339 ---
8.2. THE TURING MACHINE 323

For another example, consider what M does on the input 0010, which is not
in the language accepted.

go0010 F Xq1010F X0q 10+ XqOVOb q2XOYOr
Xq0YOK XXqYOR XXY¥q0t XXV0UB

The behavior of M on 0010 resembles the behavior on 0011, until inID X XY qi0
M scans the final 0 for the first time. M must move right, staying in state q,,
which takes it to the ID ¥ XY0q,B. However, in state gq; M has no move on
tape symbol B; thus M dies and does not accept its input. Do

8.2.4 Transition Diagrams for Turing Machines

We can represent, the transitions of a Turing machine pictorially, much as we
did for the PDA. A transition diagram consists of a set of nodes corresponding
to the states of the TM. An arc from state q to state p is labeled by one or
more items of the form X/YD, where X and Y are tape symbols, and D is
a direction, either L or R. That is, whenever 6(g,.X) = (p, Y, D), we find the
label X/Y D on the arc from g to p. However, in our diagrams, the direction D
is represented pictorially by ¢ for “left” and > for “right.”

As for other kinds of transition diagrams, we represent the start state by the
word “Start” and an arrow entering that state. Accepting states are indicated
by double circles. Thus, the only information about the TM one cannot read
directly from the diagram is the symbol used for the blank. We shall assume
that symbol is B unless we state otherwise.

Example 8.3: Figure 8.10 shows the transition diagram for the Turing ma-
chine of Example 8.2, whose transition function was given in Fig. 89. O

Example 8.4: While today we find it most convenient to think of Turing ma-
chines as recognizers of languages, or equivalently, solvers of problems, Turing’s
original view of his machine was as a computer of integer-valued functions. In
his scheme, integers were represented in unary, as blocks of a single character,
and the machine computed by changing the lengths of the blocks or by con-
structing new blocks elsewhere on the tape. In this simple example, we shall
show how a Turing machine might compute the function ~, which is called
monus or proper subtraction and is defined by m + n = max(m — 7,0). That
is, mtnism—nifm>nand 0ifm<n.
A TM that performs this operation is specified by

Note that, since this TM is not used to accept inputs, we have omitted the
seventh component, which is the set of accepting states. M will start with a
tape consisting of 010" surrounded by blanks. Af halts with 0'"*" on its tape,
surrounded by blanks.


--- Page 340 ---
324 CHAPTER 8 INTRODUCTION TO TURING MACHINES

yi y~>
0/ O> Y/Y<

Y/ Y=

Figure 8.10: Transition diagram for a TM that accepts strings of the form 0"1”

AM repeatedly finds its leftmost remaining 0 and replaces it by a blank. It
then searches right, looking for a 1. After finding a 1, it continues right, until it
comes to a 0, which it replaces by a 1. M then returns left, seeking the leftmost
0, which it identifies when it first meets a blank and then moves one ceil to the
right. The repetition ends if either:

1. Searching right for a 0, Mf encounters a blank. Then the 7 0's in 010"
have all been changed to 1’s, and n+ 1 of the m 0’s have been changed
to B. M replaces the n+ 1 1’s by one 0 and n B’s, leaving m — nr 0’s on
the tape. Since m > n in this case, m-—n = mtn.

2. Beginning the cycle, M cannot find a 0 to change to a blank, because the
first m 0’s already have been changed to B. Then n > m, som +n= 0.
M replaces all remaining 1’s and 0’s by B and ends with a completely
blank tape.

Figure 8.11 gives the rules of the transition function 6, and we have also
represented d as a transition diagram in Fig. 8.12. The following is a summary
of the role played by each of the seven states:

go: This state begins the cycle, and also breaks the cycle when appropriate.
If M is scanning a 0, the cycle must repeat. The 0 is replaced by B, the
head moves right, and state g, is entered. On the other hand, if M is


--- Page 341 ---
8.2. THE TURING MACHINE 325

State 0 1 B

al (1,9, R) (qe, 1, R) —
G2 (qs, 1,L) (42, 1, R) (qa, B, L)
gs | (9,8,R) (95,8,R) (96,8, R)
6 — - _

0; B> 0/0
i/ Bo 1/8

Figure 8.12: Transition diagram for the TM of Example 8.4


--- Page 342 ---
326 CHAPTER & INTRODUCTION TO TURING MACHINES

scanning 1, then all possible matches between the two groups of 0’s on
the tape have been made, and M goes to state gs to make the tape blank.

qi: In this state, Af searches right, through the initial block of 0’s, looking
for the leftmost 1. When found, M goes to state qz.

ga: AZ moves right, skipping over 1’s, until it finds a 0. It changes that 0 to
a1, turns leftward, and enters state g;. However, it is also possible that
there are no more 0's left after the block of 1’s. In that case, M4 in state
qo cucounters a blank. We have case (1) described above, where n 0’s in
the second block of 0’s have been used to cancel n of the m 0’s in the first
block, and the subtraction is complete. M enters state g4, whose purpose
is to convert the 1’s on the tape to blanks.

q3: Af moves left, skipping over 0’s and 1's, until it finds a blank. When it
finds B, it moves right and returns to state go, beginning the cycle again.

qa: Herc, the subtraction is complete, but one unmatched 0 in the first block
was Incorrectly changed to a B. Ad therefore moves left, changing 1's to
B’s, until it encourters a B on the tape. It changes that B back to 0, and
enters state gg. wherein Af halts.

gs: State gs is entered from gg when it is found that all 0’s in the first block
have been changed to B. In this case, described in (2) above, the result,
of the proper subtraction is 0. Mf changes all remaining 0’s and 1’s to B
and enters state gg.

qs: The sole purpose of this state is to allow M to halt when it has finished
its task. If the subtraction had been a subroutine of some more complex
function, then gg would initiate the next step of that larger computation.

Oo

8.2.5 The Language of a Turing Machine

We have intuitively suggested the way that a Turing machine accepts a lan-
guage. The input string is placed on the tape, and the tape head begins at the
leftmost input symbol. If the TM eventually enters an accepting state, then
the input is accepted, and otherwise not.

More formally, let 4 = (Q,5,T,6,go,B,F) be a Turing machine. Then
L(M) is the set of strings w in ©* such that gow F apf for some state pin F
and any tape strings a and &. This definition was assumed when we discussed
the Turing machine of Example 8.2, which accepts strings of the form 0”1*.

The set of languages we can accept using a Turing machine is often called
the recursively enumerable languages or RE languages, The term “recursively
enumerable” comes from computational formalisms that predate the Turing
machine but that define the same class of languages or arithmetic functions.
We discuss the origins of the term as an aside (box) in Section 9.2.1.


--- Page 343 ---
8.2. THE TURING MACHINE 327

Notational Conventions for Turing Machines

The symbols we normally use for Turing machines resemble those for the
other kinds of automata we have seen.

1. Lower-case letters at the beginning of the alphabet stand for input
symbols.

. Capital letters, typically near the end of the alphabet, are used for
tape symbols that may or may not be input symbols. However, B is
generally used for the blank symbol.

_ Lower-case letters near the end of the alphabet are strings of input
symbois.

. Greek letters are strings of tape symbols.

. Letters such as g, p, and nearby letters are states.

8.2.6 Turing Machines and Halting

There is another notion of “acceptance” that is commonly used for Turing
machines: acceptance by halting. We say a TM halts if it enters a state q,
scanning a tape symbol X, and there is no move in this situation; i-e., 6(g, X)
is undefined.

Example 8.5: The Turing machine M of Example 8.4 was not designed to
accept a language; rather we viewed it as computing an arithmetic function.
Note, however, that A¢ halts on all strings of 0’s and 1’s, since no matter what
string M finds on its tape, it will eventually cancel its second group of 0’s, if it
can find such a group, against its first group of 0’s, and thus must reach state
gg and halt. O

We can always assume that a TM halts if it accepts. That is, without
changing the language accepted, we can make 6 (q¢,X) undefined whenever q is
an accepting state. In general, without otherwise stating so:

e We assume that a TM always halts when it is in an accepting state.

Unfortunately, it is not always possible to require that a TM halts even
if it does not accept. Those languages with Turing machines that do halt
eventually, regardless of whether or not they accept, are called recursive, and
we shall consider their important properties starting in Section 9.2.1. Turing
machines that always halt, regardless of whether or not they accept, are a good
model of an “algorithm.” ff an algorithm to solve a given problem exists, then


--- Page 344 ---
*

—

328 CHAPTER & INTRODUCTION TO TURING MACHINES

we say the problem is “decidable,” so TM’s that always halt figure importantly
into decidability theory in Chapter 9.

8.2.7 Exercises for Section 8.2

Exercise 8.2.1: Show the ID’s of the Turing machine of Fig. 8.9 if the input
tape contains:

* a) 00.

b) 000111.

¢) OO111.
Exercise 8.2.2: Design Turing machines for the following languages:
* a) The set of strings with an equal number of 0’s and 1’s.

b) {a™b"e" | n > 1}.

c) {ww | w is any string of 0's and 1’s}.

Exercise 8.2.3: Design a Turing machine that takes as input a number N and
adds 1 to it in binary. To be precise, the tape initially contains a $ followed by
N in binary. The tape head is initially scanning the $ in state go. Your TM
should halt with N+1, in binary, on its tape, scanning the leftmost symbol of
N +1, in state gy. You may destroy the $ in creating N + 1, if necessary. For
instance, gg$10011F $¢710100, and qo$11111F qs100000.

a) Give the transitions of your Turing machine, and explain the purpose of
each state,

b} Show the sequence of ID’s of your TM when given input $111.

Exercise 8.2.4: In this exercise we explore the equivalence between function
computation and language recognition for Turing machines. For simplicity, we
shall consider only functions from nonnegative integers to nonnegative integers,
but the ideas of this problem apply to any computable functions. Here are the
two central definitions:

¢ Define the graph of a function f to be the set of all strings of the form
[x, f(z)], where z is a nonnegative integer in binary, and f(x) is the value
of function f with argument z, also written in binary.

« A Turing machine is said to compute function f if, started with any non-
negative integer z on its tape, in binary, it halts (in any state) with f(x),
in binary, on its tape.

Answer the following, with informal, but clear constructions.


--- Page 345 ---
83. PROGRAMMING TECHNIQUES FOR TURING MACHINES = 329
a) Show how, given a TM that computes f, you can construct a TM that
accepts the graph of f as a language.

b) Show how, given a TM that accepts the graph of f, you can construct a
TM that computes f.

c) A function is said to be partial if it may be undefined for some arguments.
If we extend the ideas of this exercise to partial functions, then we do not
require that the TM computing f halts if its input z is one of the integers
for which f(z) is not defined. Do your constructions for parts (a) and (b)
work if the function f is partial? If not, explain how you could modify
the construction to make it work.

Exercise 8.2.5: Consider the Turig machine

Informally but clearly describe the language Z(M) if 6 consists of the following
sets of rules:

(q+, B, FR).
(az, B, ).

8.3 Programming Techniques for Turing
Machines

Our goal is to give you a sense of how a Turing machine can be used to compute
in a manner not unlike that of a conventional computer. Eventually, we want
to convince you that a TM is exactly as powerful as a conventional computer.
In particular, we shall learn that the Turing machine can perform the sort of
calculations on other Turing machines that we saw performed in Section 8.1.2 by
a program that examined other programs. This “introspective” ability of both
Turing machines and computer programs is what enables us to prove problems
undecidable.

To make the ability of a TM clearer, we shall present a number of examples
of how we might think of the tape and finite control of the Turing machine.
None of these tricks extend the basic model of the TM; they are only notational
conveniences. Later, we shall use them to simulate extended Turing-machine
models that have additional features — for instance, more than one tape — by
the basic TM model.


--- Page 346 ---
330 CHAPTER 8. INTRODUCTION TO TURING MACHINES

8.3.1 Storage in the State

We can use the finite control not only to represent a position in the “program”
of the Turing machine, but to hold a finite amount of data. Figure 8.13 suggests
this technique (as well as another idea: multiple tracks). There, we see the finite
control consisting of not only a “control” state g, but three data elements A,
B, and C. The technique requires no extension to the TM model; we merely
think of the state as a tuple. In the case of Fig. 8.13, we should think of the
state as [g, A, B,C]. Regarding states this way allows us to describe transitions
in & more systematic way, often making the strategy behind the TM program
more transparent.

State

Storage

Track |
Track 2
Track 3

Figure 8.13: A Turing machine viewed as having finite-control storage and
multiple tracks

Example 8.6: We shall design a TM

M = (Q, {0,1}, {0,1, B}, 6, [¢o, B), {la., BY)

that remembers in its finite control the first. symbol (0 or 1) that it sees, and
checks that it does not appear elsewhere on its input. Thus, Af accepts the
language 01*+10". Accepting regular languages such as this one does not stress
the ability of Turing machines, but it will serve as a simple demonstration.

The set of states Q is {qo,q1} x {0,1, B}. That is, the states may be thought
of as pairs with two components:

a) A control portion, gq or m, that remembers what the TM is doing. Con-
trol state go indicates that M has not yet read its first symbol, while H
indicates that it Aas read the symbol, and is checking that it does not
appear elsewhere, by moving right and hoping to reach a blank cell.

b) A data portion, which remembers the first symbol seen, which must be 0
or 1. The symbol B in this component means that no symbol has been
Tead.


--- Page 347 ---
8.3. PROGRAMMING TECHNIQUES FOR TURING MACHINES 331

The transition function 6 of Af is as follows:

1. 6([go, B], a) = ((m,4],4, 2) for a = 0 or a = 1. Initially, go is the control
state, and the data portion of the state is B. The symbol scanned is copied
into the second component of the state, and M moves right, entering
control state g, as it does so.

2. &([a1,4],@) = ((g1, a],@, R) where @ is the “complement” of a, that is, 0 if
a=1and 1 if a= 0. In state gq, M skips over each symbol 0 or 1 that
is different from the one it has stored in its state, and continues moving
right.

3. 4([a1,¢],B) = ((1,B],B, R) for a = 0 or a = 1. If M reaches the first
blank, it enters the accepting state [q, 3].

Notice that M has no definition for é([g1,a],a) for a = 0 ora = 1. Thus, if
M encounters a second occurrence of the symbol it stored initially in its finite
control, it halts without having entered the accepting state. O

8.3.2 Multiple Tracks

Another useful “trick” is to think of the tape of a Turing machine as composed
of several tracks. Each track can hold one symbol, and the tape alphabet of the
TM consists of tuples, with one component for each “track.” Thus, for instance,
the cell scanned by the tape head in Fig. 8.13 contains the symbol [X, Y, Z].
Like the technique of storage in the finite control, using multiple tracks does
not extend what the Turing machine can do. It is simply a way to view tape
symbols and to imagine that they have a useful structure.

Example 8.7: A common use of multiple tracks is to treat one track as holding
the data and a second track as holding a mark. We can check off each symbol
as we “use” it, or we can keep track of a small number of positions within the
data by marking only those positions. Examples 8.2 and 8.4 were two instances
of this technique, but in neither example did we think explicitly of the tape as
if it were composed of tracks. In the present example, we shall use a second
track explicitly to recognize the non-context-free language

Lwew = {wew | w is in (0+ 1)*}
The Turing machine we shall design is:
M = (Q,%,T,6,(m, B],[B, 8), {l99, B]})
where:

Q: The set of states is {g1,92,-.-,o} x {0, 1,8}, that is, pairs consisting
of a control state gj and a data component: 0, 1, or blank. We again
use the technique of storage in the finite control, as we allow the state to
remember an input symbol 0 or 1.


--- Page 348 ---
332

CHAPTER 8 INTRODUCTION TO TURING MACHINES

: The set of tape symbols is {B,*} x {0,1,¢,B}. The first component, or

track, can be either blank or “checked,” represented by the symbols B
and *, respectively. We use the * to check off symbols of the first. and
second groups of 0’s and 1’s, eventually confirming that the string to the
left of the center marker ¢ is the same as the string to its right. The
second component of the tape symbol is what we think of as the tape
symbol itself. ‘That is, we may think of the symbol [B,X] as if it were
the tape symbol X, for X = 0,1,¢, B.

: The input symbols are [B,0] and [B, 1], which, as just mentioned, we

identify with 0 and 1, respectively.

: The transition function d is defined by the following rules, in which a and

& each may stand for either 0 or 1.

1. d({q., B),[B,a]) = ([¢2,4], [+, a], 2). In the initial state, M picks up
the symbol ¢ (which can be either 0 or 1), stores it in its finite control,
goes to control state g2, “checks off” the symbol it just scanned, and
moves right. Notice that by changing the first component of the tape
symbol from B to *, tt performs the check-off,

2. 6([go, a), (B,e}) = (a2, ¢],[B, 4], 2). AM moves right, looking for the
symbol c. Remember that @ and 6 can each be either 0 or 1, inde-
pendently, but cannot be e.

3. 6([¢2,a],[B,¢]) = ([¢3, a], [B, cl, R). When M finds the ¢, it continues
to move right, but changes to control state q3.

4. 5([¢a, a], [*,4]) = ([g3,e], [*, 6), R). In state gg, M continues past all
checked symbols.

5. 6((gs, 4}, (B,a]) = {[¢a, 8], [*,a],L). If the first unchecked symbol
that M finds is the sarne as the symbol in its finite control, it checks
this symbol, because it has matched the corresponding symbol from
the first block of 0’s and 1’s. M goes to control state q4, dropping
the symbol from its finite control, and starts moving left.

6. 6({¢4, B], [*,a]) = ((g4, B], [xa], £). Mf moves left over checked sym-
bols.

7. 6([¢4, B),[B,¢]} = ((¢s, B],[B,¢], £). When M encounters the sym-
bol ¢, it switches to state g, and continues left. In state g5, M4 must
make a decision, depending on whether or not the symbol immedi-
ately to the left of the c is checked or unchecked. If checked, then we
have already considered the entire first block of 0’s and 1’s — those
to the left of the c. We must make sure that all the 0’s and 1’s to the
right, of the ¢ are also checked, and accept if no unchecked symbols
remain to the right of the e. If the symbol immediately to the left
of the c is unchecked, we find the leftmost unchecked symbol, pick it
up, and start the cycle that began in state q.


--- Page 349 ---
8.3, PROGRAMMING TECHNIQUES FOR TURING MACHINES 333

8. 4([gs, B],(B,a]) = ([¢e,.8],[B,a],£). This branch covers the case
where the symbol to the left of c is unchecked. Af gocs to state qg
and continues left, looking for a checked symbol.

9. d([ge, B].(B,a]) = ([as, B],(B8,e],£). As long as symbols are un-
checked, Af remains in state gg and proceeds left.

10. 3((g6, BJ, [*,@]) = ([q,B),[*,a], 2). When the checked symbol is
found, Af enters state g, and moves right to pick up the first un-
checked symbol.

11. 8([¢s, 8], {*,e]) = ({a7, 8], [x, a], R). Now, let us pick up the branch
from state gs, where we have just moved left from the c and find a
checked symbol. We start moving right again, entering state g;.

12. 6((97,B],[B,¢]) = ([¢s, B), (B, cl, R). In state g7 we shall surely sec
the ¢. We enter state gg as we do so, and proceed right.

13. &{[¢s. 8], [*,@]) = ([¢s, 8], [*,@],.R). Af moves right in state gg, skip-
ping over any checked 0's or 1’s that it finds.

14. 6((gs, B],[B, B]) = ([¢9, 8], [(B, 8], 2). If M reaches a blank cell is
state gg without encountering any unchecked 0 or 1, then Af accepts.
If Af first finds an unchecked 0 or 1, then the blocks before and after
the ¢ do not match, and M halts without accepting.

D

8.3.3 Subroutines

As with programs in general, it helps to think of Turing machines as built from
a collection of interacting components, or “subroutines.” A Turing-machine
subroutine is a set of states that perform some useful process. This sct of states
includes a start state and another state that temporarily has no moves, and
that serves as the “return” state to pass control to whatever other set of states
called the subroutine. The “call” of a subroutine occurs whenever there is a
transition to its initial state. Since the TM has no mechanism for remembering
a “return address,” that is, a state to go to after it finishes. should our design
of a TM call for one subroutine to be called from several states, we can make
copies of the subroutine, using a new set of states for each copy. The “calls”
are made to the start states of different copies of the subroutine, and each copy
“returns” to a different state.

Example 8.8: We shall design a TM to implement the function “multiplica-
tion.” That is, our TM will start with 0710"1 on its tape, and will end with
0™*" on the tape. An outline of the strategy is:

1. The tape will, in general, have one nonblank string of the form 010"10*"
for some &.


--- Page 350 ---
—

334 CHAPTER 8 INTRODUCTION TO TURING MACHINES

2. In one basic step, we change a 0 in the first group to B and add n 0's to
the last group, giving us a string of the form 0'—110"104+))",

3. As a result, we copy the group of n 0's to the end m times, once each
time we change a 0 in the first group to B. When the first group of 0’s is
completely changed to blanks, there will be mn 0’s in the last group.

4, The final step is to change the leading 10"1 to blanks, and we are done.

The heart of this algorithm is a subroutine, which we call Copy. This sub-
routine implements step (2) above, copying the block of n 0’s to the end.
More precisely, Copy converts an ID of the form 0~*1g,0"10%-")" to ID
o™-*1¢,0"10*", Figure 8.14 shows the transitions of subroutine Copy. This
subroutine marks the first 0 with an X, moves right in state gq until it finds a
blank, copies the 0 there, and moves left in state gz to find the marker X. It
repeats this cycle until in state q, it finds a 1 instead of a 0. At that point, it
uses state q4 to change the X’s back to 0’s, and ends in state gs.

The complete multiplication Turing machine starts in state gg. The first
thing it does is go, in several steps, from ID g90’"10" to ID O—'1g,0"1. The
transitions needed are shown in the portion of Fig. 8.15 to the left of the sub-
routine call; these transitions involve states go and gg only.

Then, to the right of the subroutine call in Fig. 8.15 we see states g7 through
giz. The purpose of states q7, gg, and gg is to take control after Copy has just
copied a block of n 0’s, and is in ID 0" —*ig,0"108". Eventually, these states
bring us to state gg0™—*10"10'". At that point, the cycle starts again, and
Copy is called to copy the block of n 0’s again.

As an exception, in state gg the TM may find that all m 0’s have been
changed to blanks (i.e., k =m). In that case, a transition to state gio occurs.
This state, with the help of state q,;, changes the leading 10"1 to blanks and
enters the halting state gj2.. At this point, the TM is in ID g,.0™", and its job
isdone. DO

8.3.4 Exercises for Section 8.3

Exercise 8.3.1: Redesign your Turing machines from Exercise 8.2.2 to take
advantage of the programming techniques discussed in Section 8.3.

Exercise 8.3.2: A common operation in Turing-machine programs involves
“shifting over.” Ideally, we would like to create an extra cell at the curret
head position, in which we could store some character. However, we cannot
edit the tape in this way. Rather, we need to move the contents of each of the
cells to the right of the current head position one cell right, and then find our
way back to the current head position. Show how to perform this operation.
Hint: Leave a special symbol to mark the position to which the head must
return.


--- Page 351 ---
8.3. PROGRAMMING TECHNIQUES FOR TURING MACHINES

1/1 = l/l =
0/0 > 0/0 ~<-

Start O/X => Bi0~<

xX/0 <=
Figure 8.14: The subroutine Copy

Start Bi B->

BS

335

0/0-=-

Figure 8.15: The complete multiplication program uses the subroutine Copy


--- Page 352 ---
336 CHAPTER 8. INTRODUCTION TO TURING MACHINES

* Exercise 8.3.3: Design a subroutine to move a TM head from its current
position to the right, skipping over all 0’s, until reaching a 1 or a blank. If the
current position does not hold 0, then the TM should halt. You may assume
that there are no tape symbols other than 0, 1, and B (blank). Then, use this
subroutine to design a TM that accepts all strings of 0’s and 1’s that do not
have two 1’s in a row.

8.4 Extensions to the Basic Turing Machine

In this section we shall see certain computer models that are related to Turing
machines and have the same language-recognizing power as the basic model of
a TM with which we have been working. One of these, the multitape Turing
machine, is important because it is much easier to see how a multitape TM can
simulate real computers (or other kinds of Turing machines), compared with
the single-tape model we have been studying. Yet the extra tapes add no power
to the model, as far as the ability to accept languages is concerned.

We then consider the nondeterministic Turing machine, an extension of the
basic model that is allowed to make any of a finite set of choices of move in
a given situation. This extension also makes “programming” Turing machines
easier in some circumstances, but adds no language-defining power to the basic
model.

8.4.1 Multitape Turing Machines

A multitape TM is as suggested by Fig. 8.16. The device has a finite control
(state), and some finite number of tapes. Each tape is divided into cells, and
each cell cant hold any symbol of the finite tape alphabet. As in the single-tape
TM, the set of tape symbols includes a blank, and has a subset called the input
symbols, of which the blank is not a member. The set of states includes an
initial state and some accepting states. Initially:

1. The input, a finite sequence of input symbols, is placed on the first tape.
2. All other cells of all the tapes hold the blank.

3. The finite control is in the initial state.

4. The head of the first tape is at the left end of the input.

5. All other tape heads are at some arbitrary cell. Since tapes other than
the first tape are completely blank, it does not matter where the head is
placed initially; all cells of these tapes “look” the same.

A move of the multitape TM depends on the state and the symbol scanned
by each of the tape heads. In one move, the multitape TM does the foliowing:


--- Page 353 ---
8.4. EXTENSIONS TO THE BASIC TURING MACHINE 337

Figure 8.16: A multitape Turing machine

1. The control enters a new state, which could be the same as the previous
state.

2. On each tape, a new tape symbol is written on the cell scanned. Any of
these symbols may be the same as the symbol previously there.

3. Each of the tape heads makes a move, which can be either left, right, or
stationary. The heads move independently, so different heads may move
in different directions, and some may not move at all.

We shall not give the formal notation of transition rules, whose form is
a straightforward generalization of the notation for the one-tape TM, except
that directions are now indicated by a choice of E, R, or S. For the one-
tape machine, we did not allow the head to remain stationary, so the S option
was not present. You should be able to imagine an appropriate notation for
instantaneous descriptions of the configuration of a multitape TM; we shall not
give this notation formally. Multitape Turing machines, like one-tape TM’s,
accept by entering an accepting state.

8.4.2 Equivalence of One-Tape and Multitape TM’s

Recall that the recursively enumerable languages are defined to be those ac-
cepted by a one-tape TM. Surely, multitape TM’s accept all the recursively
enumerable languages, since a one-tape TM is a multitape TM. However, are
there languages that are not recursively enumerable, yet are accepted by mul-
titape TM’s? The answer is “no,” and we prove this fact by showing how to
simulate a multitape TM by a one-tape TM.


--- Page 354 ---
338 CHAPTER 8 INTRODUCTION TO TURING MACHINES

Theorem 8.9: Every language accepted by a multitape TM is recursively
enumerable.

PROOF: The proof is suggested by Fig. 8.17. Suppose language Z is accepted
by a &-tape TM M. We simulate Af with a one-tape TM N whose tape we
think of as having 2k tracks. Half these tracks hold the tapes of M, and the
other half of the tracks each hold only a single marker that indicates where the
head for the corresponding tape of M is currently located. Figure 8.17 assumes
& = 2. The second and fourth tracks hold the contents of the first and second
tapes of M, track 1 holds the position of the head of tape 1, and track 3 holds
the position of the second tape head.

Figure 8.17; Simulation of a two-tape Turing machine by a one-tape Turing
machine

To simulate a move of Af, N’s head must visit the k head markers. So that
N not get lost, it must remember how many head markers are to its left at all
times; that count is stored as a component of N’s finite control. After visiting
each head marker and storing the scanned symbol in a component of its finite
control, N knows what tape symbols are being scanned by each of A4’s heads.
N also knows the state of M, which it stores in N’s own finite control. Thus,
N knows what move Af will make.

N now revisits each of the head markers on its tape, changes the symbol
in the track representing the corresponding tapes of M, and moves the head
markers left or right, if necessary. Finally, N changes the state of M as recorded
in its own finite control. At this point, NW has simulated one move of AM.

We select as N’s accepting states all those states that record M’s state as
one of the accepting states of Mf. Thus, whenever the simulated M accepts, N
also accepts, and N does not accept otherwise. O


--- Page 355 ---
8.4. EXTENSIONS TO THE BASIC TURING MACHINE 339

A Reminder About Finiteness

A common fallacy is to confuse a value that is finite at any time with a set
of values that is finite. The many-tapes-to-one construction may help us
appreciate the difference. In that construction, we used tracks on the tape
to record the positions of the tape heads. Why could we not store these
positions as integers in the finite control? Carelessly, one could argue that
after n moves, the TM can have tape head positions that must be within
n positions of original head positions, and so the head only has to store

integers up to 7.

The problem is that, while the positions are finite at any time, the
complete set of positions possible at any time is infinite. If the state is
to represent any head position, then there must be a data component of
the state that has any integer as value. This component forces the set of
states to be infinite, even if only a finite number of them can be used at
any finite time. The definition of a Turing machine requires that the set
of states be finite. Thus, it is not permissible to store a tape-head position
in the finite control.

8.4.3 Running Time and the Many-Tapes-to-One
Construction

Let us now introduce a concept that will become quite important later: the
“time complexity” or “running time” of a Turing machine. We say the running
time of TM M on input w is the number of steps that Af makes before halting.
If M doesn’t halt on w, then the running time of M on w is infinite. The téme
complexity of TM M is the function T(n) that is the maximum, over all inputs
w of length n, of the running time of Mf on w. For Turing machines that do
not halt on all inputs, T'(m)} may be infinite for some or even all n. However, we
shall pay special attention to TM’s that do halt on all inputs, and in particular,
those that have a polynomial time complexity T(n); Section 10.1 initiates this
study.

The construction of Theorem 8.9 seems clumsy. In fact, the constructed one-
tape TM may take much more running time than the multitape TM. However,
the amounts of time taken by the two Turing machines are commensurate in
a weak sense: the one-tape TM takes time that is no more than the square of
the time taken by the other. While “squaring” is not a very strong guarantee,
it does preserve polynomial running time. We shall see in Chapter 10 that:

a) The difference between polynomial time and higher growth rates in run-
ning time is really the divide between what we can solve by computer and
what is in practice not solvable.

b) Despite extensive research, the running time needed to solve many prob-


--- Page 356 ---
340 CHAPTER 8 INTRODUCTION TO TURING MACHINES

lems has not been resolved closer than to within some polynomial. Thus,
the question of whether we are using a one-tape or multitape TM to solve
the problem is not crucial when we examine the running time needed to
solve a particular problem.

The argument that the running times of the one-tape and multitape TM’s are
within a square of each other is as follows.

Theorem 8.10: The time taken by the one-tape TM WN of Theorem 8.9 to
simulate n moves of the k-tape TM M is O{n*),

PROOF: After m moves of M, the tape head markers cannot have separated by
more than 27 cells. Thus, if N starts at the leftmost marker, it has to move
no more than 2n cells right, to find all the head markers. Jt can then make
an excursion leftward, changing the contents of the simulated tapes of M, and
moving head markers left or right as needed. Doing so requires no more than
2n moves left, plus at. most 2k moves to reverse direction and write a marker
in the cell to the right (in the case that a tape head of M moves right).

Thus, the number of moves by V needed to simulate one of the first nm moves
is no more than 4n + 2k. Since & is a constant, independent of the number of
moves simulated, this number of moves is O(n). To simulate n moves requires
no more than vr times this amount, or O(n”). O

8.4.4 Nondeterministic Turing Machines

A nondeterministic Turing machine (NJ'M) differs from the deterministic vari-
ety we have been studying by having a transition function 6 such that for each
state g and tape symbol X, 6(q,X) is a set of triples

{(m,¥%4,D4), (go, ¥2, Do), hey (ge. Yk, Du)}

where & is any finite integer. The NTM can choose, at each step, any of the
triples to be the next move. It cannot, however, pick a state from one, a tape
symbol from another, and the direction from yet another.

The language accepted by an NTM M is defined in the expected manner,
in analogy with the other nondeterministic devices, such as NFA’s and PDA’s,
that we have studied. That is, M accepts an input w if there is any sequence of
choices of move that leads from the initial ID with w as input, to an ID with an
accepting state. The existence of other choices that do not lead to an accepting
state is irrelevant, as it is for the NFA or PDA,

The NTM’s accept no languages not accepted by a deterministic TM (or
DTM if we need to emphasize that it is deterministic). The proof involves
showing that for every NTM My, we can construct a DTM Mp that explores
the ID’s that My can reach by any sequence of its choices. If Mf{p finds one
that has an accepting state, then A¢p enters an accepting state of its own. Mp
must be systematic, putting new ID’s on a queue, rather than a stack, so that
after some finite time Mp has simulated all sequences of up to k moves of My,
fork =1,2,....


--- Page 357 ---
8.4. EXTENSIONS TO THE BASIC TURING MACHINE 341

Theorem 8.11: If Afy is a nondeterministic Turing machine, then there is a
deterministic Turing machine Mp such that L(My) = L(Mp).

PROOF: Mp will be designed as a multitape TM, sketched in Fig. 8.18. The
first tape of Mp holds a sequence of ID's of My, including the state of My.
One ID of My is marked as the “current” ID, whose successor ID’s are in the
process of being discovered. In Fig. 8.18, the third ID is marked by an z along
with the inter-ID separator, which is the +. All ID’s to the left of the current
one have been explored and can be ignored subsequently.

Finite
control

Queue

of ID’s IDL * D2 * ID3 *ID4 *

Scratch
tape

Figure 8.18: Simulation of an NTM by a DTM

To process the current ID, Afp does the following:

1.

Mp examines the state and scanned symbol of the current ID. Built into
the finite control of Mp is the knowledge of what choices of move My
has for each state and symbol. If the state in the current ID is accepting,
then Mp accepts and simulates My no further.

. However, if the state is not accepting, and the state-symbol combination

has & moves, then Mp uses its second tape to copy the ID and then make
k copies of that ID at the end of the sequence of ID’s on tape 1.

. Mp modifies each of those k ID’s according to a different one of the *

choices of move that My has from its current ID.

. Mp returns to the marked, current ID, erases the mark, and moves the

mark to the next ID to the right. The cycle then repeats with step (1).

It should be clear that the simulation is accurate, in the sense that Mp wiil
only accept if it finds that AJy can enter an accepting ID. However, we need
to confirm that if My enters an accepting ID after a sequence of n of its own
moves, then Mp will eventually make that ID the current ID and will accept.


--- Page 358 ---
—

342 CHAPTER 8. INTRODUCTION TO TURING MACHINES

Suppose that m is the maximum number of choices My has in any configu-
ration. Then there ts one initial ID of My, at most m ID's that My can reach
after one move, at. most m* ID’s My can reach after two moves, and so on.
Thus, after n moves, My can reach at most 1+m-+m?+---4+m" ID’s. This
number is at most nm” ID's.

‘The order in which Mp explores ID’s of My is “breadth first”; that is, it
explores all ID’s reachable by 0 moves (i.e., the initial ID), then all ID’s reach-
able by one move, then those reachable by two moves, and so on. In particular,
Mp will make current, and consider the successors of, all ID’s reachable by up
to n moves before considering any ID’s that are only reachable by more than n
moves.

As a consequence, the accepting ID of A¢y will be considered by Mp among
the first nm” ID’s that it considers. We only care that Afp considers this ID
in some finite time, and this bound is sufficient to assure us that the accepting
ID is considered eventually. Thus, if My accepts, then so does Mp. Since we
already observed that if Mp accepts it does so only because My accepts, we
conclude that L(Mn) =L(Mp). O

Notice that the constructed deterministic TM may take exponentially more
time than the nondeterministic TM. It is unknown whether or not this expo-
nential slowdown is necessary. In fact, Chapter 10 is devoted to this question
and the consequences of someone discovering a better way to simulate N'TM’s
deterministically.

8.4.5 Exercises for Section 8.4

Exercise 8.4.1: Informally but clearly describe multitape Turing machines
that accept each of the languages of Exercise 8.2.2. Try to make each of your
Turing machines run in time proportional to the input length.

Exercise 8.4.2; Here is the transition function of a nondeterministic TM M =

é| 0 1 B

q2

Show the ID’s reachable from the initial ID if the input is:
* a) Ol.
b) 011.

Exercise 8.4.3: Informally but clearly describe nondeterministic Turing ma-
chines — multitape if you like — that accept the following languages. Try to


--- Page 359 ---
—_

*%

8.4, EXTENSIONS TO THE BASIC TURING MACHINE 343

take advantage of nondeterminism to avoid iteration and save time in the non-
deterministic sense. That is, prefer to have your NTM branch a lot, while each
branch is short.

* a) The language of all strings of 0’s and 1’s that have some string of length
100 that repeats, not necessarily consecutively. Formally, this language is
the set of strings of 0’s and 1’s of the form wayzz, where || = 100, and
w, y, and 2 are of arbitrary length.

b) The language of all strings of the form wi#tw2#-:-#wn, for any n, such
that each w; is a string of 0’s and 1’s, and for some j, wj is the integer j
in binary.

c) The language of all strings of the sare form as (b), but for at least two
values of j, we have w; equal to j in binary.

Exercise 8.4.4: Consider the nondeterministic Turing machine

Informally but clearly describe the language L({AZ) if 6 consists of the following
{(go,1, R)}; 6(41,B) = {(ay, B, R)}.

Exercise 8.4.5: Consider a nondeterministic TM whose tape is infinite in
both directions. At some time, the tape is completely blank, except for one
cell, which holds the symbol $. The head is currently at some blank cell, and
the state is g.

a) Write transitions that will enable the NTM to enter state p, scanning the
5.

!b)} Suppose the TM were deterministic instead. How would you enable it to
find the $ and enter state p?

Exercise 8.4.6: Design the following 2-tape TM to accept the language of all
strings of 0's and 1’s with an equal number of each. The first tape contains the
input, and is scanned from left to right. The second tape is used to store the
excess of 0’s over 1’s, or vice-versa, in the part of the input seen so far. Specify
the states, transitions, and the intuitive purpose of each state.

Exercise 8.4.7: In this exercises, we shall implement a stack using a special
3-tape TM.

1. The first tape will be used only to hold and read the input. The input
alphabet consists of the symbol t, which we shall interpret as “pop the
stack,” and the symbols a and 6, which are interpreted as “push an @
(respectively 6) onto the stack.”


--- Page 360 ---
344 CHAPTER 8 INTRODUCTION TO TURING MACHINES

2. The second tape is used to store the stack.

3. The third tape is the output tape. Every time a symbol is popped from
the stack, it must be written on the output tape, following all previously
written symbols.

The Turing machine is required to start with an empty stack and implement the
sequence of push and pop operations, as specified on the input, reading from
left to right. If the input causes the TM to try to pop and empty stack, then it
must halt in a special error state q,. If the entire input leaves the stack empty
at the end, then the input is accepted by going to the final state qs. Describe
the transition function of the TM informally but clearly. Also, give a summary
of the purpose of each state you use.

Exercise 8.4.8: In Fig. 8.17 we saw an example of the general simulation of
a k-tape TM by a one-tape TM.

* a) Suppose this technique is used to simulate a 5-tape TM that had a tape
alphabet of seven symbols. How many tape symbols would the one-tape
TM have?

*'b) An alternative way to simulate & tapes by one is to use a (k + 1)st track
to hold the head positions of all & tapes, while the first & tracks simulate
the & tapes in the obvious manner. Note that in the (& + 1)st track, we
must be careful to distinguish among the tape heads and to allow for the
possibility that two or more heads are at the same cell. Does this method
reduce the number of tape symbols needed for the one-tape TM?

c) Another way to simulate & tapes by 1 is to avoid storing the head positions
altogether. Rather, a (k + 1)st track is used only to mark one cell of the
tape. At all times, each simulated tape is positioned on its track so the
head is at the marked cell. If the k-tape TM moves the head of tape i, then
the simulating one-tape TM slides the entire nonblank contents of the ith
track one cell in the opposite direction, so the marked cell continues to
hold the cell scanned by the ith tape head of the k-tape TM. Does this
method help reduce the number of tape symbols of the one-tape TM?
Does it have any drawbacks compared with the other methods discussed?

Exercise 8.4.9: A k-head Turing machine has k heads reading cells of one
tape. A move of this TM depends on the state and on the symbol scanned
by each head. In one move, the TM can change state, write a new symbol
on the cell scanned by each head, and can move each head left, right, or keep
it stationary. Since several heads may be scanning the same cell, we assume
the heads are numbered 1 through k, and the symbol written by the highest
numbered head scanning a given cell is the one that actually gets written there.
Prove that the languages accepted by &-head Turing machines are the same as
those accepted by ordinary TM’s.


--- Page 361 ---
8.5. RESTRICTED TURING MACHINES 345

1! Exercise 8.4.10: A two-dimensional Turing machine has the usual finite-state
control but a tape that ig a two-dimensional grid of cells, infinite in all directions.
The input is placed on one row of the grid, with the head at the left end of the
input and the control in the start state, as usual. Acceptance is by entering a
final state, also as usual. Prove that the languages accepted by two-dimensional
Turing machines are the same as those accepted by ordinary TM’s.

8.5 Restricted Turing Machines

We have seen seeming generalizations of the Turing machine that do not add any
language-recognizing power. Now, we shall consider some examples of apparent
restrictions on the TM that also give exactly the same language-recognizing
power. Our first restriction is minor but useful in a number of constructions
to be seen later: we replace the TM tape that is infinite in both directions by
a tape that is infinite only to the right. We also forbid this restricted TM to
print a blank as the replacement tape symbol. The value of these restrictions
is that we can assume ID’s consist of only nonblank symbols, and that they
always begin at the left end of the input.

We then explore certain kinds of multitape Turing machines that are gen-
eralized pushdown automata. First, we restrict the tapes of the TM to behave
like stacks. Then, we further restrict the tapes to be “counters,” that is, they
can only represent one integer, and the TM can only distinguish a count of 0
from any nonzero count. The impact of this discussion is that there are several
very simple kinds of automata that have the full power of any computer. More-
over, undecidable problems about Turing machines, which we see in Chapter 9,
apply as well to these simple machines.

8.5.1 Turing Machines With Semi-infinite Tapes

While we have allowed the tape head of a Turing machine to move either left
or right from its initial position, it is only necessary that the TM’s head be
allowed to move within the positions at and to the right of the initial head
position. In fact, we can assume the tape is semi-infinite, that is, there are no
cells to the left of the initial head position. In the next theorem, we shall give a
construction that shows a TM with a semi-infinite tape can simulate one whose
tape is, like our original TM model, infinite in both directions.

The trick behind the construction is to use two tracks on the semi-infinite
tape. The upper track represents the cells of the original TM that are at or to
the right of the initial head position. The lower track represents the positions
left of the initial position, but in reverse order. The exact arrangement is
suggested in Fig. 8.19. The upper track represents cells Xp, X1,--. , where Xo
is the initial position of the head; X,, Xz, and so on, are the cells to its right.
Cells X_1,X_2, and so on, represent cells to the left of the initial position.
Notice the * on the leftmost cell’s bottom track. This symbol serves as an


--- Page 362 ---
346 CHAPTER 8 INTRODUCTION TO TURING MACHINES

endmarker and prevents the head of the semi-infinite TM from accidentally
falling off the left end of the tape.

Figure 8.19: A semi-infinite tape can simulate a two-way infinite tape

We shall make one more restriction to our Turing machine: it never writes a
blank. This simple restriction, coupled with the restriction that the tape is only
semi-infinite means that the tape is at all times a prefix of nonblank symbols
followed by an infinity of blanks. Further, the sequence of nonblanks always
begins at the initial tape position. We shall see in Theorem 9.19, and again in
Theorem 10.9, how useful it is to assume ID’s have this form.

Theorem 8,12: Every language accepted by a TM Mb is also accepted by a
TM M4 with the following restrictions:

1. M;,’s head never moves left of its initial position.
2. My never writes a blank.

PROOF: Condition (2) is quite easy. Create a new tape symbol B’ that func-
tions as a blank, but is not the blank B. That is:

a) If Mz has a rule 62(¢,X) = (p,B,D), change this rule to ée(q,X) =
(p, BY, D).

b) Then, let d2(g, B’) be the same as 62(¢, B), for every state g.
Condition (1) requires more effort. Let
Mz = (Q2, ©, 2, 52, 42, B, Fa)
be the TM Mo as modified above, so it never writes the blank B. Construct
My = (Q1,5 x {B},T:, 61,00, [B, Bl, A)
where:

Qi: The states of M, are {90,91} U (Qo x {U, L}). That is, the states of My
are the initial state go another state q,, and all the states of Me with a
second data component that is either U or L (upper or lower). The second
component tells us whether the upper or lower track, as in Fig. 8.19 is
being scanned by Adj. Put another way, U means the head of Mo is at
or to the right of its initial position, and Z means it is to the left of that
position.


--- Page 363 ---
8.5. RESTRICTED TURING MACHINES 347

I: The tape symbols of Af; are all pairs of symbols from T2, that is, P2 x Pe.
The input symbols of M; are those pairs with an input symbol of M2 in
the first component and a blank in the second component, that is, pairs
of the form [a, B], where a is in E. The blank of Mf, has blanks in both
components. Additionally, for every symbol X in Tz, there is a pair [X, +]
inT,. Here, * is a new symbol, not in I'2, and serves to mark the left end
of M,’s tape.

6: The transitions of My are as follows:

1. 6; (40, (2, B)) = (a1, [a, +), 2), for any a in L. The first move of M,
puts the * marker in the lower track of the leftmost cell. The state
becomes q1, and the head moves right, because it cannot move left
or remain stationary.

2. 6:(91,[X, B]) = ({g2, UI, [X, B], £), for any X in T2. In state qi, Ah
establishes the initial conditions of A¢g, by returning the head to its
initial position and changing the state to [go, U], i.e., the initial state
of M>, with attention focused on the upper track of M).

3. If d9(@, X) = (p,Y, D), then for every Z in Pe:
where D is the direction opposite D, that is, Lif D = Rand Rif
D= UL. If M, is not at its leftmost cell, then it simulates Mz on
the appropriate track — the upper track if the second component of
state is U and the lower track if the second component is L, Note,
however, that when working on the lower track, MM. moves in the
direction opposite that of Mz. That choice makes sense, because the
left half of Mo’s tape has been folded, in reverse, along the lower
track of M@)’s tape.

4, If 6o(q,X) = (p, Y, BR), then
51([¢, £), [X, #]) = 41([9, UI (X, *]) = (pO), LY, +), 2)

This rule covers one case of how the left endmarker * is handled. If
Mz moves right from its initial position, then regardless of whether
it had previously been to the left or the right of that position (as
reflected in the fact that the second component of Afj’s state could
be L or U), Af, must move right and focus on the upper track. That
is, My will next be at the position represented by Xj in Fig. 8.19.

5. If do(q, X) = (p, Y, £), then

This rule is similar to the previous, but covers the case where Mg
moves left from its initial position. Ad; must move right from its


--- Page 364 ---
348 CHAPTER 8 INTRODUCTION TO TURING MACHINES

endmarker, but now focuses on the lower track, i.e., the cell indicated
by X_, in Fig. 8.19.

F\: The accepting states F, are those states in F) x {U, L}, that is all states
of Md) whose first component is an accepting state of Mo. The attention
of M, may be focused on either the upper or lower track at the time it
accepts.

The proof of the theorem is now essentially complete. We may observe by
induction on the number of moves made by Afy that Af, will mimic the ID of
Mp on its own tape, if you take the lower track, reverse it, and follow it by the
upper track. Also, we note that Mj, enters one of its accepting states exactly
when Af, does. Thus, £(A4,;) = E{Me). O

8.5.2 Multistack Machines

We now consider several computing models that are based on generalizations
of the pushdown automaton. First, we consider what happens when we give
the PDA several stacks. We already know, from Example 8.7, that a Turing
machine can accept languages that are not accepted by any PDA with one
stack. It turns out that if we give the PDA two stacks, then it can accept any
language that a TM can accept.

We shall then consider a class of machines called “counter machines.” These
machines have only the ability to store a finite number of integers (“counters”),
and to make different moves depending on which if any of the counters are
currently 0. The counter machine can only add or subtract. one from the counter,
and cannot tell two different nonzero counts from each other. In effect, a counter
is like a stack on which we can place only two symbols: a bottom-of-stack marker
that appears only at the bottom, and one other symbol that may be pushed
and popped from the stack.

Finite
Input _ | state

control

Accept/reject

Figure 8.20: A machine with three stacks


--- Page 365 ---
8.5. RESTRICTED TURING MACHINES 349

We shall not give a formal treatment of the multistack machine, but the
idea is suggested by Fig. 8.20. A k-stack machine is a deterministic PDA with
k stacks. It obtains its input, like the PDA does, from an input source, rather
than having the input placed on a tape or stack, as the TM does. The multistack
machine has a finite control, which is in one of a finite set of states. It has a
finite stack alphabet, which it uses for all its stacks. A move of the multistack
machine is based on:

1. The state of the finite control.

2. The input symbol read, which is chosen from the finite input alphabet.
Alternatively, the multistack machine can make a move using € input, but
to make the machine deterministic, there cannot be a choice of an e-move
or a non-¢-move in any situation.

3. The top stack symbol on each of its stacks.
In one move, the multistack machine can:

a) Change to a new state.

b) Replace the top symbol of each stack with a string of zero or more stack
symbols. There can be (and usually is) a different replacement string for
each stack.

Thus, a typical transition rule for a k-stack machine looks like:
5(q, a, Al, 42;-- Xk) = (P. V1; V2) - us Yh)

The interpretation of this rule is that in state g, with X; on top of the ith stack,

for i = 1,2,...,%, the machine may consume a (either an input symbol or €)
from its input, go to state p, and replace X; on top of the th stack by string
;, for each 4 = 1,2,...,&. The multistack machine accepts by entering a final
state.

We add one capability that simplifies input processing by this deterministic
machine: we assume there is a special symbol $, called the endmarker, that
appears only at the end of the input and is not part of that input. The presence
of the endmarker allows us to know when we have consumed all the available
input. We shall see in the next theorern how the endmarker makes it easy for the
multistack machine to simulate a Turing machine. Notice that the conventional
TM needs no special endmarker, because the first blank serves to mark the end
of the input.

Theorem 8.13: If a language L is accepted by a Turing machine, then Lis
accepted by a two-stack machine.


--- Page 366 ---
350 CHAPTER 8 INTRODUCTION TO TURING MACHINES

PROOF: The essential idea is that two stacks can simulate one Turing-machine
tape, with one stack holding what is to the left of the head and the other stack
holding what is to the right of the head, except for the infinite strings of blanks
beyond the leftmost and rightmost nonblanks. In more detail, let L be L(M)
for some (one-tape) TM M. Our two-stack machine S$ will do the following:

1. & begins with a bottom-of-stack marker on each stack. This marker can
be the start symbol for the stacks, and must not appear elsewhere on the
stacks, In what follows, we shall say that a “stack is empty” when it
contains only the bottom-of-stack marker.

2. Suppose that wS is on the input of S. S$ copies w onto its first stack,
ceasing to copy when it reads the endmarker on the input.

3. S pops each symbol in turn from its first stack and pushes it onto its
second stack. Now, the first stack is empty, and the second stack holds
w, with the left end of w at the top.

4. S enters the (simulated) start state of M@. It has an empty first stack,
representing the fact that Af has nothing but blanks to the left of the cell
scanned by its tape head. S has a second stack holding w, representing
the fact that w appears at and to the right of the cell scanned by M’s
head.

5. .S simulates a move of M as follows.

(a) S knows the state of M, say ¢, because S simulates the state of Af
in its own finite control.

(b) 3 knows the symbol X scanned by M’s tape head; it is the top
of S’s second stack. As an exception, if the second stack has only
the bottom-of-stack marker, then Af has just moved to a blank; $
interprets the symbol scanned by M as the blank.

(c) Thus, 5 knows the next move of M.

(d} The next state of M is recorded in a component of S's finite control,
in place of the previous state.

(ec) If Af replaces X by Y and moves right, then S pushes Y onto its
first stack, representing the fact that Y is now to the left of Ad’s
head, X is popped off the second stack of S. However, there are two
exceptions:

i. Ifthe second stack has only a bottom-of-stack marker (and there-
fore, X is the blank), then the second stack is not changed; M
has moved to yet another blank further to the right.

ii. IfY is blank, and the first stack is empty, then that stack remains
empty. The reason is that there are still only blanks to the left
of Af’s head.


--- Page 367 ---
8.5. RESTRICTED TURING MACHINES 351

(f) If M replaces X by Y and moves left, 5 pops the top of the first
stack, say Z, then replaces X by ZY on the second stack. This
change reflects the fact that what used to be one position left of the
head is now at the head. As an exception, if Z is the bottom-of-stack
marker, then MM must push BY onto the second stack and not pop
the first stack.

6. S$ accepts if the new state of M is accepting. Otherwise, 5 simulates
another move of M in the same way.

im

8.5.3 Counter Machines

A counter machine may be thought of in one of two ways:

1. The counter machine has the same structure as the multistack machine
(Fig. 8.20), but in place of each stack is a counter. Counters hold any
nonnegative integer, but we can only distinguish between zero and nonzero
counters. That is, the move of the counter machine depends on its state,
input symbol, and which, if any, of the counters are zero. In one move,
the counter machine can:

{a) Change state.

(b) Add or subtract 1 from any of its counters, independently. However,
a counter is not allowed to become negative, so it cannot subtract 1
from a counter that is currently 0.

2. A counter machine may also be regarded as a restricted multistack ma-
chine. The restrictions are as follows:

(a) There are only two stack symbols, which we shall refer to as Zp (the
bottom-of-stack marker), and X.

(b) 2p is initially on each stack.
(c) We may replace Zp only by a string of the form X *Zo, for some i > 0.

(d) We may replace X only by X* for some i > 0. That is, Zo) appears
only on the bottom of each stack, and all other stack symbols, if any,
are X.

We shall use definition (1) for counter machines, but the two definitions clearly
define machines of equivalent power. The reason is that stack X #Zy can be
identified with the count i. In definition (2), we can tell count 0 from other
counts, because for count 0 we see Zp on top of the stack, and otherwise we see
X. However, we cannot distinguish two positive counts, since both have X on
top of the stack.


--- Page 368 ---
352 CHAPTER 8. INTRODUCTION TO TURING MACHINES

8.5.4 The Power of Counter Machines

There are a few observations about the languages accepted by counter machines
that are obvious but worth stating:

e Every language accepted by a counter machine is recursively enumerable.
The reason is that a counter machine is a special case of a stack machine,
and a stack machine is a special case of a multitape Turing machine, which
accepts only recursively enumerable languages by Theorem 8.9.

Every language accepted by a one-counter machine is a CFL. Note that
a coutiter, in point-of-view (2), is a stack, so a one-counter machine is a
special case of a one-stack machine, i.e., a PDA. In fact, the languages
of one-counter machines are accepted by deterministic PDA’s, although
the proof is surprisingly complex. The difficulty in the proof stems from
the fact that the multistack and counter machines have an endmarker $
at the end of their input. A nondeterministic PDA can guess that it has
seen the last input symbol and is about to see the $; thus it is clear that a
nondeterministic PDA without the endmarker can simulate a DPDA with
the endmarker. However, the hard proof, which we shall not attack, is
to show that a DPDA without the endmarker can simulate a DPDA with
the endmarker.

The surprising result about counter machines is that two counters are enough to
simulate a Turing machine and therefore to accept every recursively enumerable
language. It is this result we address now, first showing that three counters are
enough, and then simulating three counters by two counters.

Theorem 8,14: Every recursively enumerable language is accepted by a three-
counter machine.

PROOF: Begin with Theorem 8.13, which says that every recursively enumer-
able language is accepted by a two-stack machine. We then need to show how
to simulate a stack with counters. Suppose there are r — 1 tape symbols used
by the stack machine. We may identify the symbols with the digits 1 through
r—1, and think of a stack X|X2---X, as an integer in base r. That is, this
stack (whose top is at the left end, as usual) is represented by the integer
Xar® + Xp rt? te + Xor + XM.

We use two counters to hold the integers that represent each of the two
stacks. The third counter is used to adjust the other two counters. In particular,
we need the third counter when we either divide or multiply a count by r.

The operations on a stack can be broken into three kinds: pop the top
symbol, change the top symbol, and push a symbol onto the stack. A move of
the two-stack machine may involve several of these operations; in particular,
replacing the top stack symbol X by a string of symbols must be broken down
into replacing X and then pushing additional symbols onto the stack. We
perform these operations on a stack that is represented by a count 7, as follows.


--- Page 369 ---
8.5. RESTRICTED TURING MACHINES 353

Note that it is possible to use the finite control of the multistack machine to do
each of the operations that requires counting up to r or less.

1. To pop the stack, we must replace i by 7/r, throwing away any remainder,
which is X,. Starting with the third counter at 0, we repeatedly reduce
the count ¢ by r, and increase the third counter by 1. When the counter
that originally held ¢ reaches 0, we stop. Then, we repeatedly increase the
original counter by 1 and decrease the third counter by 1, until the third
counter becomes 0 again. At this time, the counter that used to hold 7
holds i/r.

2. To change X to Y on the top of a stack that is represented by count ¢,
we increment or decrement i by a smal] amount, surely no more than r.
If ¥Y > X, as digits, increment i by Y — X; if Y < X then decrement i by
xX-Y.

3. To push X onto a stack that initially holds 7, we need to replace ¢ by
ir +X. We first multiply by r. To do so, repeatedly decrement the count
i by 1 and increase the third counter (which starts from 0, as always), by
r. When the original counter becomes 0, we have ¢r on the third counter.
Copy the third counter to the original counter and make the third counter
0 again, as we did in item (1). Finally, we increment the original counter
by X.

To complete the construction, we must initialize the counters to simulate the
stacks in their initial condition: holding only the start symbol of the two-stack
machine. This step is accomplished by incrementing the two counters involved
to some small integer, whichever integer from 1 to r—1 corresponds to the start
symbol. O

Theorem 8.15: Every recursively enumerable language is accepted by a two-
counter machine.

PROOF: With the previous theorem, we only have to show how to simulate
three counters with two counters. The idea is to represent the three counters,
say i, 7, and k, by a single integer. The integer we choose is m = 2°3/5*. Onc
counter will hold this number, while the other is used to help multiply or divide
m. by one of the first three primes: 2, 3, and 5. To simulate the three-counter
inachine, we need to perform the following operations:

1. Increment i, j, and/or k. To increment i by 1, we multiply m by 2.
We already saw in the proof of Theorem 8.14 how to multiply a count
by any constant r, using a second counter. Likewise, we increment j by
multiplying m by 3, and we increment k by multiplying m by 5.

2. Tell which, if any, of 7, j, and & are 0. To tell if 7 = 0, we must determine
whether m is divisible by 2. Copy m into the second counter, using the
state of the counter machine to remember whether we have decremented


--- Page 370 ---
354 CHAPTER 8 INTRODUCTION TO TURING MACHINES

Choice of Constants in the 3-to-2 Counter
Construction

Notice how important it is in the proof of Theorem 8.15 2, 3, and 5 are

distinct primes. If we had chosen, say m = 2'3/4*, then m = 12 could
represent either i = 0, 7 = 1, and k = 1, or it could represent 7 = 2, 7 = 1,
and & = 0. Thus, we could not tell whether 7 or k was 0, and thus could
not simulate the 3-counter machine reliably.

m an even or Odd number of times. If we have decremented m an odd
number of times when it becomes 0, then 7 = 0. We then restore m
by copying the second counter to the first. Similarly, we test if 7 = 0
by determining whether m is divisible by 3, and we test if k = 0 by
determining whether m is divisible by 5.

3. Decrement 7, 7, and/or k. To do so, we divide m by 2, 3, or 5, respec-
tively. The proof of Theorem 8.14 tells us how to perform the division by
any constant, using an extra counter. Since the 3-counter machine cannot
decrease a count below 0, it is an error, and the simulating 2-counter ma-
chine halts without accepting, if m is not evenly divisible by the constant
by which we are dividing.

8.5.5 Exercises for Section 8.5

Exercise 8.5.1: Informally but clearly describe counter machines that accept
the following languages. In each case, use as few counters as possible, but not
more than two counters.

*a) {071" | n >m > 1}.
b) {071" | 1<m<n}.
*1c) {atbc* |i =f or i =k}.
Nd) {ate* |i=j ori =k or j =k}.

it Exercise 8.5.2: The purpose of this exercise is to show that a one-stack ma-
chine with an endmarker on the input has no more power than a deterministic
PDA. £$ is the concatenation of language Z with the language containing only
the one string $; that is, L$ is the set of all strings w$ such that w isin £. Show
that if L$ is a language accepted by a DPDA, where $ is the endmarker symbol,
not appearing in any string of LZ, then £ is also accepted by some DPDA. Hint:


--- Page 371 ---
8.6. TURING MACHINES AND COMPUTERS 355

This question is really one of showing that the DPDA languages are closed un-
der the operation Z/a defined in Exercise 4.2.2. You must modify the DPDA
P for L$ by replacing each of its stack symbols X by ali possible pairs (*, 3),
where S is a set of states. If P has stack X)X--:Xn, then the constructed
DPDA for L has stack (X1,.91)(X2, 52) -+-(Xn,Sn), where each S; is the set of
states g such that P, started in ID (g,@, X;Xi41+-* Xp) will accept.

8.6 Turing Machines and Computers

Now, let us compare the Turing machine and the common sort of computer
that we use daily. While these models appear rather different, they can accept
exactly the same languages — the recursively enumerable languages. Since
the notion of “a common computer” is not well defined mathematically, the
arguments in this section are necessarily informal. We must appeal to your
intuition about what computers can do, especially when the numbers involved
exceed normal limits that are built into the architecture of these machines (c.g.,
32-bit address spaces). The claims of this section can be divided into two parts:

1. A computer can simulate a Turing machine.

2. A Turing machine can simulate a computer, and can do so in an amount
of time that is at most some polynomial in the number of steps taken by
the computer.

8.6.1 Simulating a Turing Machine by Computer

Let us first examine how a computer can simulate a Turing machine. Given
a particular TM M, we must write a program that acts like Mf. One aspect
of M is its finite control. Since there are only a finite number of states and a
finite number of transition rules, our program can encode states as character
strings and use a table of transitions, which it looks up to determine each move.
Likewise, the tape symbols can be encoded as character strings of a fixed length,
since there are only a finite number of tape symbols.

A serious question arises when we consider how our program is to simulate
the Turing-machine tape. This tape can grow infinitely long, but the computer's
memory — main memory, disk, and other storage devices — are finite. Can we
simulate an infinite tape with a fixed amount of memory?

If there is no opportunity to replace storage devices, then in fact we cannot;
a computer would then be a finite automaton, and the only languages it could
accept would we regular. However, common computers have swappable storage
devices, perhaps a “Zip” disk, for example. In fact, the typical hard disk is
removable and can be replaced by an empty, but otherwise identical disk.

Since there is no obvious limit on how many disks we could use, let us assume
that as many disks as the computer needs is available. We can thus arrange
that the disks are placed in two stacks, as suggested by Fig. 8.21. One stack


--- Page 372 ---
356 CHAPTER 8 INTRODUCTION TO TURING MACHINES

holds the data in cells of the Turing-machine tape that are located significantly
to the left of the tape head, and the other stack holds data significantly to the
right of the tape head. The further down the stacks, the further away from the
tape head the data is.

Processor i
Tape to Tape to
left of right of
the head the head

Figure 8.21: Simulating a Turing machine with a common computer

If the tape head of the TM moves sufficiently far to the left that it reaches
cells that are not represented by the disk currently mounted in the computer,
then it prints a message “swap left.” The currently mounted disk is removed
by a human operator and placed on the top of the right stack. The disk on top
of the left stack is mounted in the computer, and computation resumes.

Similarly, if the TM’s tape head reaches cells so far to the right that these
cells are not represented by the mounted disk, then a “swap right” message is
printed. The human operator moves the currently mounted disk to the top of
the left stack, and mounts the disk on top of the right stack in the computer.
If either stack is empty when the computer asks that a disk from that stack
be mounted, then the TM has entered an all-blank region of the tape. In that
case, the human operator must go to the store and buy a fresh disk to mount.

8.6.2 Simulating a Computer by a Turing Machine

We also need to consider the opposite comparison: are there things a common
computer can do that a Turing machine cannot. An important subordinate
question is whether the computer can do certain things much faster than a
‘Turing machine. In this section, we argue that a TM can simulate a computer,
and in Section 8.6.3 we argue that the simulation can be done sufficiently fast
that “only” a polynomial separates the running times of the computer and TM


--- Page 373 ---
8.6. TURING MACHINES AND COMPUTERS 357

The Problem of Very Large Tape Alphabets

The argument of Section 8.6.1 becomes questionable if the number of tape
symbols is so large that the code for one tape symbol doesn’t fit on a disk.
There would have to be very many tape symbols indeed, since a 30 gigabyte
disk, for instance, can represent any of 2740000000000 symbols. Likewise, the
number of states could be so large that we could not represent the state
using the entire disk.

One resolution of this problem begins by limiting the number of tape
symbols a TM uses. We can always encode an arbitrary tape alphabet in
binary. Thus, any TM M can be simulated by another TM M’ that uses
only tape symbols 0, 1, and B. However, M' needs many states, since to
simulate a move of M, the TM M’ must scan its tape and remember, in its
finite control, all the bits that tell it what symbol M is scanning. In this
manner, we are left with very large state sets, and the PC that simulates
M' may have to mount and dismount several disks when deciding what
the state of Mf’ is and what the next move of M' should be. No one ever
thinks about computers performing tasks of this naturc, so the typical
operating system has no support for a program of this type. However, if
we wished, we could program the raw computer and give it this capability.

Fortunately, the question of how to simulate a TM with a huge number
of states or tape symbols can be finessed. We shall see in Section 9.2.3
that one can design a TM that is in effect a “stored program” TM. This
TM, called “universal,” takes the transition function of any TM, encoded
in binary on its tape, and simulates that TM. The universal TM has
quite reasonable numbers of states and tape symbols. By simulating the
universal TM, a common computer can be programmed to accept any
recursively enumerable language that we wish, without having to resort
to simulation of numbers of states that stress the limits of what can be
stored on a disk.

on a given problem. Again, let us remind the reader that there are impor-
tant reasons to think of all running times that lie within a polynomial of one
another to be similar, while exponential differences in running time are “too
much.” We take up the theory of polynomial versus exponential running times
in Chapter 10.

To begin our study of how a TM simulates a computer, let us give a realistic
but informal model of how a typical computer operates.

a) First, we shall suppose that the storage of a computer consists of an indef-
initely long sequence of words, each with an address. In a real computer,
words might be 32 or 64 bits long, but we shall not put a limit on the
length of a given word. Addresses will be assumed to be integers 0, 1,


--- Page 374 ---
398 CHAPTER 8 INTRODUCTION TO TURING MACHINES

2, and so on. In a real computer, individual bytes would be numbered
by consecutive integers, so words would have addresses that are multiples
of 4 or 8, but this difference is unimportant. Also, in a real computer,
there would be a limit on the number of words in “memory,” but since we
want to account for the content of an arbitrary number of disks or other
storage devices, we shall assume there is no limit to the number of words.

b} We assume that the program of the computer is stored in some of the
words of memory. These words each represent a simple instruction, as in
the machine or assembly language of a typical computer. Examples are
instructions that move data from one word to another or that add one
word to another. We assume that “indirect addressing” is permitted, so
one instruction could refer to another word and use the contents of that
word as the address of the word to which the operation is applied. This
capability, found in all modern computers, is needed to perform array
accesses, to follow links in a list, or to do pointer operations in general.

a
—

We assume that each instruction involves a limited (finite) number of
words, and that each instruction changes the value of at most one word.

d) A typical computer has registers, which are memory words with especially
fast access. Often, operations such as addition are restricted to occur in
registers. We shall not make any such restrictions, but will allow any
operation to be performed on any word. The relative speed of operations
on different words will not be taken into account, nor need it be if we
are only comparing the language-recognizing abilities of computers and
Turing machines. Even if we are interested in running time to within a
polynomial, the relative speeds of different word accesses is unimportant,
since those differences are “only” a constant factor.

Figure 8.22 suggests how the Turing machine would be designed to simulate
a computer. This TM uses several tapes, but it could be converted to a one-tape
TM using the construction of Section 8.4.1. The first tape represents the entire
memory of the computer. We have used a code in which addresses of memory
words, in numerical order, alternate with the contents of those memory words.
Both addresses and contents are written in binary. The marker symbols * and
# are used to make it easy to find the ends of addresses and contents, and to tell
whether a binary string is an address or contents. Another marker, $, indicates
the beginning of the sequence of addresses and contents.

The second tape is the “instruction counter.” This tape holds one integer
in binary, which represents one of the memory locations on tape 1. The value
stored in this location will be interpreted as the next computer instruction to
be executed.

The third tape holds a “memory address” or the contents of that address
after the address has been located on tape 1. To execute an instruction, the
‘TM must find the contents of one or more memory addresses that hold data


--- Page 375 ---
8.6. TURING MACHINES AND COMPUTERS 359

Finite
control

Memory

Instruction
counter

Memory

address 1101110

Computer's
Input file

Scratch

Figure 8.22; A Turing machine that simulates a typical computer

involved in the computation. First, the desired address is copied onto tape 3 and
compared with the addresses on tape 1, until a match is found. The contents of
this address is copied onto the third tape and moved to wherever it is needed,
typically to one of the low-numbered addresses that represent the registers of
the computer.

Our TM will simulate the instruction cycle of the computer, as follows.

1. Search the first tape for an address that matches the instruction number
on tape 2. We start at the $ on the first tape, and move right, comparing
each address with the contents of tape 2. The comparison of addresses
on the two tapes is easy, since we need only move the tape heads right,
in tandem, checking that the symbols scanned are always the same.

2. When the instruction address is found, examine its value. Let us assume
that when a word is an instruction, its first few bits represent the action
to be taken (e.g., copy, add, branch), and the remaining bits code an
address or addresses that are involved in the action.

3. If the instruction requires the value of some address, then that address
will be part of the instruction. Copy that address onto the third tape, and


--- Page 376 ---
360

CHAPTER 8 INTRODUCTION TO TURING MACHINES

mark the position of the instruction. using a second track of the first tape
{not shown in Fig, 8.22), so we can find our way back to the instruction,
if necessary. Now, search for the memory address on the first tape, and
copy its value onto tape 3, the tape that holds the memory address.

bxecute the instruction, or the part of the instruction involving this value.
We cannot go into all the possible machine instructions. However, a
sample of the kinds of things we might do with the new value are:

(a)

(c)

Copy it to some other address. We get the second address from the
instruction, find this address by putting it on tape 3 and searching
for the address on tape 1, as discussed previously. When we find
the second address, we copy the value into the space reserved for the
value of that address. If more space is needed for the new value, or
the new value uses less space than the old value. change the available
space by shifting over. That is:

i. Copy, onto a scratch tape the entire nonblank tape to the right
of where the new value goes,
ii. Write the new value, using the correct amount of space for that
value.
ill. Recopy the seratch tape onto tape 1, immediately to the right
of the new value.

As a special case, the address may not yet appear on the first tape,
because it has not been used by the computer previously. In this
case, we find the place on the first tape where it belongs, shift-over
to make adequate room, and store both the address and the new
value there.

Add the value just found to the value of some other address. Go
back to the instruction te locate the other address. find this address
on tape 1. Perform a binary addition of the value of that address
and the value stored on tape 3. By scanning the two values from
their right cuds, a TM can perform a ripple-earry addition with
little difficulty. Should more space be needed for the result, use the
shifting-over technique to create space on tape L.

The instruction is a “jump,” that is, a directive to take the next
instruction from the address that is the value now stored on tape 3.
Simply copy tape 3 to tape 2 and begin the instruction cycle again.

5. After performing the instruction, and determining that the instruction is
not a jump, add 1 to the instrnetion counter on tape 2 and begin the
instruction cvele again.

There are many other details of how the TM simulates a typical computer,

We have suggested in Fig. 8.22 a fourth tape holding the simnlated input to the


--- Page 377 ---
8.6. TURING MACHINES AND COMPUTERS 361

computer, since the computer must read its input (the word whose membership
in a language it is testing) from a file. The TM can read from this tape instead.

A scratch tape is also shown. Simulation of some computer instructions
might make effective use of a scratch tape or tapes to compute arithmetic
operations such as multiplication.

Finally, we assume that the computer makes an output that tells whether
or not its input is accepted. To translate this action into terms that the Turing
machine ¢an execute, we shall suppose that there is an “accept” instruction of
the computer, perhaps corresponding to a function call by the computer to put
yes on an output file. When the TM simulates the execution of this computer
instruction, it enters an accepting state of its own and halts.

While the above discussion is far from a complete, formal proof that a TM
can simulate a typical computer, it should provide you with enough detail to
convince you that a TM is a valid representation for what a computer can
do. Thus, in the future, we shall use only the Turing machine as the formal
representation of what can be computed by any kind of computing device.

8.6.3 Comparing the Running Times of Computers and
Turing Machines

We now must address the issue of running time for the Turing machine that
simulates a computer. As we have suggested previously:

e The issue of running time is important because we shall use the TM not
only to examine the question of what can be computed at all, but what
can be computed with enough efficiency that. a problem’s computer-based
solution can be used in practice.

The dividing line between the tractable — that which can be solved effi-
ciently — from the intractable — problems that can be solved, but not
fast enough for the solution to be usable — is generally held to be between
what can be computed in polynomial time and what requires more than
any polynomial running time.

¢ Thus, we need to assure ourselves that if a problem can be solved in poly-
nomial time on a typical computer, then it can be solved in polynomial
time by a Turing machine, and conversely. Because of this polynomial
equivalence, our conclusions about what a Turing machine can or cannot
do with adequate efficiency apply equally well to a computer.

Recall that in Section 8.4.3 we determined that the difference in running
time between one-tape and multitape TM’s was polynomial — quadratic, in
particular. Thus, it is sufficient to show that anything the computer can do,
the multitape TM described in Section 8.6.2 can do in an amount of time that
is polynomial in the amount of time the computer takes. We then know that
the same holds for a one-tape TM.


--- Page 378 ---
362 CHAPTER 8. INTRODUCTION TO TURING MACHINES

Before giving the proof that the Turing machine described above can sim-
ulate n steps of a computer in O(n) time, we need to confront the issue of
multiplication as a computer instruction. The problem is that we have not put
a limit on the number of bits that one computer word can hold. If, say, the
computer were to start with a word holding integer 2, and were to multiply that
word by itself for n consecutive steps, then the word would hold the number
2?" This number requires 2” + 1 bits to represent, so the time the Turing
machine takes to simulate these n instructions would be exponential in n, at
least.

One approach is to insist that words retain a fixed maximum length, say
64 bits. Then, multiplications (or other operations) that produced a word too
long would cause the computer to halt, and the Turing machine would not have
to simulate it any further. We shall take a more liberal stance: the computer
Inay use words that grow to any length, but one computer instruction can only
produce a word that is one bit longer than the longer of its arguments.

Example 8.16: Under the above restriction, addition is allowed, since the
result can only be one bit longer than the maximum length of the addends.
Multiplication is not allowed, since two m-bit words can have a product of
length 2m. However, we can simulate a multiplication of m-bit integers by a
sequence of m additions, interspersed with shifts of the multiplicand one bit
left (which is another operation that only increases the length of the word by
1). Thus, we can still multiply arbitrarily long words, but the time taken by
the computer is proportional to the square of the length of the operands. O

Assuming one-bit maximum growth per computer instruction executed, we
can prove our polynomial relationship between the two running times. The
idea of the proof is to notice that after n instructions have been executed, the
number of words mentioned on the memory tape of the TM is O(n), and each
computer word requires O(n) Turing-machine cells to represent it. Thus, the
tape is O(n?) cells long, and the TM can locate the finite number of words
needed by one computer instruction in O(n?) time.

There is, however, one additional requirement that must be placed on the
instructions. Even if the instruction does not produce a long word as a result,
it could take a great deal of time to compute the result. We therefore make the
additional assumption that the instruction itself, applied to words of length up
to k, can be performed in O(k?) steps by a multitape Turing machine. Surely
the typical computer operations, such as addition, shifting, and comparison of
values, can be done in O{k) steps of a multitape TM, so we are being overly
liberal in what we allow a computer to do in one instruction.

Theorem 8.17: If a computer:

1. Has only instructions that increase the maximum word length by at most
1, and


--- Page 379 ---
8.7. SUMMARY OF CHAPTER 8 363

9. Has only instructions that a multitape TM can perform on words of length
k in O(k*) steps or less,

then the Turing machine described in Section 8.6.2 can simulate n steps of the
computer in O(n*) of its own steps.

PROOF: Begin by noticing that the first (memory) tape of the TM in Fig. 8.22
starts with only the computer’s program. That program may be long, but it is
fixed and of constant length, independent of n, the number of instruction steps
the computer executes. Thus, there is some constant ¢ that is the largest of
the computer’s words and addresses appearing in the program. There is also a
constant d that is the number of words occupied by the program.

Thus, after executing n steps, the computer cannot have created any words
longer than c+ 7, and therefore, it cannot have created or used any addresses
that are longer than c+n bits either. Each instruction creates at most one new
address that gets a value, so the total number of addresses after nm instructions
have been executed is at most d+. Since each address-word combination
requires at most 2(¢ +n) + 2 bits, including the address, the contents, and two
marker symbols to separate them, the total number of TM tape cells occupied
after n instructions have been simulated is at most 2(d+n)(c+n+1). Ase
and d are constants, this number of cells is O(n?).

We now know that each of the fixed number of lookups of addresses involved
in one computer instruction can be done in O(n”) time. Since words are O(n)
in length, our second assumption tells us that the instructions themselves can
each be carried out by a TM in O(n?) time. The only significant, remaining
cost of an instruction is the time it takes the TM to create more space on its
tape to hold a new or expanded word. However, shifting-over involves copying
at most O(n?) data from tape 1 to the scratch tape and back again. Thus,
shifting-over also requires only O(n?) time per computer instruction.

We conclude that the TM simulates one step of the computer in O(n?) of
its own steps. Thus, as we claimed in the theorem statement, n steps of the
computer can be simulated in O(n*) steps of the Turing machine. O

As a final observation, we now see that cubing the number of steps lets a
inultitape TM simulate a computer. We also know from Section 8.4.5 that a
one-tape TM can simulate a multitape TM by squaring the number of steps, at
most. Thus:

Theorem 8.18: A computer of the type described in Theorem 8.17 can be
simulated for n steps by a one-tape Turing machine, using at most O(n) steps
of the Turing machine. ©

8.7 Summary of Chapter 8

4 The Turing Machine: The TM is an abstract computing machine with
the power of both real computers and of other mathematical definitions


--- Page 380 ---
364

CHAPTER 8 INTRODUCTION TO TURING MACHINES

of what can be computed. The TM consists of a finite-state control and
an infinite tape divided into cells. Each cell holds one of a finite number
of tape symbols, and one cell is the current position of the tape head. The
TM makes moves based on its current state and the tape symbol at the
cell scanned by the tape head. In one move, it changes state, overwrites
the scanned cell with some tape symbol, and moves the head one ceil left
or right.

Acceptance by a Turing Machine: The TM starts with its input, a finite-
length string of tape symbols, on its tape, and the rest of the tape contain-
ing the blank symbol on each cell. The blank is one of the tape symbols,
and the input is chosen from a subset of the tape symbols, not including
blank, called the input symbols. The TM accepts its input if it ever enters
an accepting state.

Recursively Enumerable Languages: The languages accepted by TM’s are
called recursively enumerable (RE) languages. Thus, the RE languages
are those languages that can be recognized or accepted by any sort of
computing device.

Instantaneous Descriptions of a TM: We can describe the current. config-
uration of a TM by a finite-length string that includes all the tape cells
from the leftmost to the rightmost nonblank. The state and the position
of the head are shown by placing the state within the sequence of tape
symbols, just to the left of the cell scanned.

Storage in the Finite Control: Sometimes, it helps to design a TM for a
particular language if we imagine that the state has two or more compo-
nents. One component is the control component, and functions as a state
normally does. The other components hold data that the TM needs to
remember.

Multiple Tracks: It also helps frequently if we think of the tape symbols
as vectors with a fixed number of components. We may visualize each
component as a separate track of the tape.

Multitape Turing Machines: An extended TM model has some fixed num-
ber of tapes greater than one. A move of this TM is based on the state
and on the vector of symbols scanned by the head on each of the tapes.
in a move, the multitape TM changes state, overwrites symbols on the
cells scanned by each of its tape heads, and moves any or all of its tape
heads one cell in cither direction. Although able to recognize certain
languages fastcr than the conventional one-tape TM, the multitape TM
cannot recognize any language that is not RE.

Nondeterministic Turing Machines: The NTM has a finite number of
choices of next move (state, new symbol, and head move) for each state
and symbol scanned. It accepts an input if any sequence of choices leads


--- Page 381 ---
8.8.

REFERENCES FOR CHAPTER & 365

to an ID with an accepting state. Although seemingly more powerful than
the deterministic TM, the NTM is not able to recognize any language that
is not RE.

+ Semi-infinite-Tape Turing Machines: We can restrict a TM to have a tape

that is infinite only to the right, with no cells to the left of the initial head
position. Such a TM can accept any RE language.

+ Multistack Machines: We can restrict the tapes of a multitape TM to

behave like a stack. The input is on a separate tape, which is read once
from left-to-right, mimicking the input mode for a finite automaton or
PDA. A one-stack machine is really a DPDA, while a machine with two
stacks can accept any RE language.

+ Counter Machines: We may further restrict the stacks of a multistack

machine to have only one symbol other than a bottom-marker. Thus,
each stack functions as a counter, allowing us to store a nonnegative
integer, and to test whether the integer stored is 0, but nothing more. A
machine with two counters is sufficient to accept any RE language.

+ Simulating a Turing Machine by a real computer: It is possible, in prin-

ciple, to simulate a TM by a real computer if we accept that there is a
potentially infinite supply of a removable storage device such as a disk,
to simulate the nonblank portion of the TM tape. Since the physical
resources to make disks are not infinite, this argument is questionable.
However, since the limits on how much storage exists in the universe are
unknown and undoubtedly vast, the assumption of an infinite resource,
as in the TM tape, is realistic in practice and generally accepted.

+ Simulating a Computer by a Turing Machine: A TM can simulate the

storage and control of a real computer by using one tape to store all the
locations and their contents: registers, main memory, disks, and other
storage devices. Thus, we can be confident that something not doable by
a TM cannot be done by a real computer.

8.8 References for Chapter 8

The Turing machine is taken from (8]. At about the same time there were several
less machine-like proposals for characterizing what can be computed, including
the work of Church [1], Kleene [5], and Post [7]. All these were preceded by the
work of Gédel [3], which in effect showed that there was no way for a computer
to answer all mathematical questions.

The study of multitape Turing machines, especially the matter of how their

running time compares with that of the one-tape model initiated with Hart-
manis and Stearns [4]. The examination of multistack and counter machines
comes from [6], although the construction given here is from [2}.


--- Page 382 ---
366 CHAPTER 8 INTRODUCTION TO TURING MACHINES

The approach in Section 8.1 of using “hello, world” as a surrogate for ac-
ceptance or halting by a Turing machine appeared in unpublished notes of S.
Rudich.

1. A. Church, “An undecidable problem in elementary number theory,”
American J. Math. 58 (1936), pp. 345-363.

2. P. C. Fischer, “Turing machines with restricted memory access,” Infor-
mation and Control 9:4 (1966), pp. 364-379.

3. K. Gédel, “Uber formal unentscheidbare satze der Principia Mathematica
und verwander systeme,” Monatschefte fur Mathematik und Physik 38
(1931), pp. 173-198.

4. J. Hartmanis and R. E. Stearns, “On the computational complexity of
algorithms,” Transactions of the AMS 117 (1965), pp. 285-306.

5. 8. C. Kleene, “General recursive functions of natural numbers,” Mathe-
matische Annalen 112 (1936), pp. 727-742.

6. M. L. Minsky, “Recursive unsolvability of Post’s problem of ‘tag’ and
other topics in the theory of Turing machines,” Annals of Mathematics
74:3 (1961), pp. 437-455.

7. E. Post, “Finite combinatory processes-formulation,” J. Symbolic Logic 1
(1936), pp. 103-105.

8. A. M. Turing, “On computable numbers with an application to the Ent-
scheidungsproblem,” Proc. London Math. Society 2:42 (1936), pp. 230-
265, See also ibid. 2:43, pp. 544-546.


--- Page 383 ---
Chapter 9

Undecidability

This chapter begins by repeating, in the context of Turing machines, the ar-
gument of Section 8.1, which was a plausibility argument for the existence of
problems that could not be solved by computer. The problem with the latter
“proof” was that we were forced to ignore the real limitations that every imple-
mentation of C (or any other programming language) has on any real computer.
Yet these limitations, such as the size of the address space, are not fundamental
limits. Rather, as the years progress we expect computers will grow indefinitely
in measures such as address-space size, main-memory size, and others.

By focusing on the Turing machine, where these limitations do not exist,
we are better able to capture the essential idea of what some computing device
will be capable of doing, if not today, then at some time in the future. In this
chapter, we shall give a formal proof of the existence of a problem about Turing
machines that no Turing machine can solve. Since we know from Section 8.6
that Turing machines can simulate real computers, even those without the limits
that we know exist today, we shall have a rigorous argument that the following
problem:

e Does this Turing machine accept (the code for) itself as input?

cannot be solved by a computer, no matter how generously we relax those
practical limits.

We then divide problems that can be solved by a Turing machine into two
classes: those that have an algorithm (i.e., a Turing machine that halts whether
or not it accepts its input), and those that are only solved by Turing machines
that may run forever on inputs they do not accept. The latter form of accep-
tance is problematic, since no matter how long the TM runs, we cannot know
whether the input is accepted or not. Thus, we shall concentrate on techniques
for showing problems to be “undecidable,” i.e., to have no algorithm, regardless
of whether or not they are accepted by a Turing machine that fails to halt on
some inputs.

We prove undecidable the following problem:

367


--- Page 384 ---
368 CHAPTER 9. UNDECIDABILITY

e Does this Turing machine accept this input?

Then, we exploit this undecidability result to exhibit a number of other un-
decidable problems. For instance, we show that all nontrivial problems about
the language accepted by a Turing machine are undecidable, as are a number
of problems that have nothing at all to do with Turing machines, programs, or
computers.

9.1 A Language That Is Not Recursively
EKnumerable

Recall that a language L is recursively enumerable (abbreviated RE) if £ =
E{M) for some TM M. Also, we shall in Section 9.2 introduce “recursive”
or “decidable” languages that are not only recursively enumerable, but are
accepted by a TM that always halts, regardless of whether or not it accepts.

Our long-range goal is to prove undecidable the language consisting of pairs
(M,w) such that:

1. M is a Turing machine (suitably coded, in binary) with input alphabet
{0,1},

2. wis a string of 0’s and 1’s, and
3. M accepts input w.

If this problem with inputs restricted to the binary alphabet is undecidable,
then surely the more general problem, where TM’s may have any alphabet, is
undecidable.

Our first step is to set this question up as a true question about membership
in a particular language. Thus, we must give a coding for Turing machines that
uses only 0’s and 1’s, regardless of how many states the TM has. Once we have
this coding, we can treat any binary string as if it were a Turing machine. If the
string is not a well-formed representation of some TM, we may think of it as
representing a TM with no moves. Thus, we may think of every binary string
as some TM.

An intermediate goal, and the subject of this section, involves the language
La, the “diagonalization language,” which consists of all those strings w such
that the TM represented by w does not accept the input w. We shall show that
£q has no Turing machine at all that accepts it. Remember that showing there
is no Turing machine at: all for a language is showing something stronger than
that the language is undecidable (i.e., that it has no algorithm, or TM that
always halts).

The language Lz plays a role analogous to the hypothetical program A2
of Section 8.1.2, which prints hello, world whenever its input does not print
hello, world when given itself as input. More precisely, just as Hy cannot


--- Page 385 ---
9.1. A LANGUAGE THAT IS NOT RECURSIVELY ENUMERABLE 369

exist because its response when given itself as input is paradoxical, 2g cannot
be accepted by a Turing machine, because if it were, then that Turing machine
would have to disagree with itself when given a code for itself as input.

9.1.1 Enumerating the Binary Strings

In what follows, we shall need to assign integers to all the binary strings so
that each string corresponds to one integer, and each integer corresponds to
one string. If w is a binary string, treat lw as a binary integer 7. Then we
shall call w the ith string. That is, ¢ is the first string, 0 is the second, 1 the
third, 00 the fourth, 01 the fifth, and so on. Equivalently, strings are ordered
by length, and strings of equal length are ordered lexicographically. Hereafter,
we shall refer to the ith string as w;.

9.1.2 Codes for Turing Machines

Our next goal is to devise a binary code for Turing machines so that each TM
with input alphabet {0,1} may be thought of as a binary string. Since we just
saw how to enumerate the binary strings, we shall then have an identification of
the Turing machines with the integers, and we can talk about “the ith Turing
machine, M;.” To represent a TM M = (Q, {0,1},T,6,q,8,F) as a binary
string, we must first assign integers to the states, tape symbols, and directions
Land &.

e We shall assume the states are q1,92,---,4r for some r. The start state
will always be q;, and g2 will be the only accepting state. Note that, since
we may assume the TM halts whenever it enters an accepting state, there
is never any need for more than one accepting state.

e We shall assume the tape symbols are X,,X2,...,Xs for some s. Xy
always will be the symbol 0, X2 will be 1, and X3 will be B, the blank.
However, other tape symbols can be assigned to the remaining integers
arbitrarily.

e We shall refer to direction £ as D, and direction R as Do.

Since each TM M can have integers assigned to its states and tape symbols in
many different orders, there will be more than one encoding of the typical TM.
However, that fact is unimportant in what follows, since we shall show that no
encoding can represent a TM M such that E(M) = La.

Once we have established an integer to represent each state, symbol, and
direction, we can encode the transition function 6. Suppose one transition rule
is 5(qi,.X3) = (4k, Xt, Dm), for some integers 7, j, k, f, and m. We shall code
this rule by the string 0'10/10"10'10™. Notice that, since all of 4, 7, &, £, and m
are at least one, there are no occurrences of two or more consecutive 1’s within
the code for a single transition.


--- Page 386 ---
370 CHAPTER 9. UNDECIDABILITY

A code for the entire TM M consists of all the codes for the transitions, in
some order, separated by pairs of 1’s:

C111C_11---C,-, 110,
where each of the C’s is the code for one transition of M.

Example 9.1: Let the TM in question be

M = ({a, q2.93}; {0, 1}, {0, 1, B}, 6,91, B, {g2})
where 6 consists of the rules:

8(91,1) = (3,0, R)
5(q3, 0) = = (q1,1, R)
5(q9.B) = (43,1, L)

The codes for each of these rules, respectively, are:

0100100010100
0001010100100
00010010010100
0001006100010010

For example, the first rule can be written as 6(q, X2) = (93,1, Da), since
1 = X2, 0 = X,, and R = D2. Thus, its code is 0'107107101107, as was
indicated above. A code for A is:

01001000101001100010101001001100010010010100110001000100010010

Note that there are many other possible codes for M. In particular, the codes
for the four transitions may be listed in any of 4! orders, giving us 24 codes for
M. QO

In Section 9.2.3, we shall have need to code pairs consisting of a TM and a
string, (M,w). For this pair we use the code for M followed by 111, followed
by w. Note that, since no valid code for a TM contains three 1’s in a row, we
can be sure that the first occurrence of 111 separates the code for M from w.
For instance, if M were the TM of Example 9.1, and w were 1011, then the
code for (M,w) would be the string shown at the end of Example 9.1 followed
by 1111011.

9.1.3 The Diagonalization Language

In Section 9.1.2 we coded Turing machines so there is now a concrete notion of
M;, the “ith Turing machine”: that TM M whose code is wy, the ith binary
string. Many integers do not correspond to any TM at all. For instance, 11001


--- Page 387 ---
9.1. A LANGUAGE THAT IS NOT RECURSIVELY ENUMERABLE = 371

does not begin with 0, and 0010111010010100 is not valid because it has three
consecutive l’s. If w; is not a valid TM code, we shall take M; to be the TM
with one state and no transitions. That is, for these values of ¢, M7; is a Turing
machine that immediately halts on any input. Thus, L(A4;) is @ if w; fails to
be a valid TM code.

Now, we can make a vital definition.

e The language Lg, the diagonalization language, is the set of strings w;
such that w; is not in L(Ad;).

That is, Lg consists of all strings w such that the TM M whose code is w does
not accept when given w as input.

The reason Lg is called a “diagonalization” language can be seen if we
consider Fig. 9.1. This table tells for all i and j, whether the TM M; accepts
input string w,;; 1 means “yes it does” and 0 means “no it doesn’t.”! We may
think of the ith row as the characteristic vector for the language L(M;); that
is, the 1’s in this row indicate the strings that are members of this language.

J —
1234-7

1 hN 1 0

2 |anNdo o

* 3l0 oNN

1 alo 1

Diagonal
Figure 9.1: The table that represents acceptance of strings by Turing machines

The diagonal values tell whether M; accepts w;. To construct Ly, we com-
plement the diagonal. For instance, if Fig. 9.1 were the correct table, then
the complemented diagonal would begin 1,0,0,0,.... Thus, Lg would contain
w1 = €, not contain w2 through w4, which are 0, 1, and 00, and so on.

The trick of complementing the diagonal to construct the characteristic
vector of a language that cannot be the language that appears in any row,
is called diagonalization. It works because the complement of the diagonal is
itself a characteristic vector describing membership in some language, namely

lY¥ou should note that the actual table does not look anything like the one suggested by
the figure. Since all low integers fail to represent a valid TM code, and thugs represent the
trivial TM that makes no moves, the top rows of the table are in fact solid 0s.


--- Page 388 ---
—~

372 CHAPTER 9. UNDECIDABILITY

£4. This characteristic vector disagrees in some column with every row of the
table suggested by Fig. 9.1. Thus, the complement of the diagonal cannot be
the characteristic vector of any Turing machine.

9.1.4 Proof that Z, is not Recursively Enumerable

Following the above intuition about characteristic vectors and the diagonal, we
shall now prove formally a fundamental result about Turing machines: there is
no Turing machine that accepts the language Lg.

Theorem 9.2: Lg is not a recursively enumerable language. That is, there is
no Turing machine that accepts Lg.

PROOF: Suppose La were L(A) for some TM M. Since Lg is a language over
alphabet {0,1}, Af would be in the list of Turing machines we have constructed,
since it includes all TM’s with input alphabet {0,1}. Thus, there is at least
one code for M, say i; that is, Af = Mj.

Now, ask if w; is in Lg.

e Ifw; isin Ly, then M, accepts w;. But then, by definition of Lg, w; is not
in Lg, because Lg contains only those w; such that M; does not accept
Wy.

* Similarly, if w; is not in Lg, then M; does not accept w,, Thus, by defini-
tion of Lg, w; is im Lg.

Since w; can neither be in Lg nor fail to be in Lg, we conclude that there is a
contradiction of our assumption that M exists. That is, Ly is not a recursively
enumerable language. O

9.1.5 Exercises for Section 9.1
Exercise 9.1.1; What strings are:
* a) wy7?

b) wroo?

Exercise 9.1.2: Write one of the possible codes for the Turing machine of
Fig, 8.9.

Exercise 9.1.3: Here are two definitions of languages that are similar to the
definition of Lg, yet different from that language. For each, show that the
language is not accepted by a Turing machine, using a diagonalization-type
argument. Note that you cannot develop an argument based on the diagonal
itself, but must find another infinite sequence of points in the matrix suggested
by Fig. 9.1.

* a) The set of all w; such that w; is not accepted by Mo;.


--- Page 389 ---
—_

9.2, AN UNDECIDABLE PROBLEM THAT IS RE 373

b) The set of all w; such that we; is not accepted by Adj.

Exercise 9.1.4: We have considered only Turing machines that have input
alphabet {0,1}. Suppose that we wanted to assign an integer to all Turing ma-
chines, regardless of their input alphabet. That is not quite possible because,
while the names of the states or noninput tape symbols are arbitrary, the par-
ticular input symbols matter. For instance, the languages {0"1" | n > 1} and
{a"b” | n > 1}, while similar in some sense, are not the same language, and they
are accepted by different TM’s. However, suppose that we have an infinite set
of symbols, {@1,@2,...} from which all TM input alphabets are chosen. Show
how we could assign an integer to all TM’s that had a finite subset of these
symbols as its input alphabet.

9.2 An Undecidable Problem That is RE

Now, we have seen a problem — the diagonalization language Lg — that has
no Turing machine to accept it. Our next goal is to refine the structure of the
recursively enumerable (RE) languages (those that are accepted by TM’s) into
two classes. One class, which corresponds to what we commonly think of as an
algorithm, has a TM that not only recognizes the language, but it tells us when
it has decided the input string is not in the language. Such a Turing machine
always halts eventually, regardless of whether or not it reaches an accepting
state.

The second class of languages consists of those RE languages that are not
accepted by any Turing machine with the guarantee of halting. These languages
are accepted in an inconvenient way: if the input is in the language, we'll
eventually know that, but if the input is not in the language, then the Turing
machine may run forever, and we shall never be sure the input won’t be accepted
eventually. An example of this type of language, as we shall see, is the set of
coded pairs (M,w) such that TM M accepts input w.

9.2.1 Recursive Languages

We call a language L recursive if L = L(M) for some Turing machine M such
that:

1. If w is in L, then Af accepts (and therefore halts).

2. If w is not in L, then M eventually halts, although it never enters an
accepting state.

A TM of this type corresponds to our informal notion of an “algorithm,” a
well-defined sequence of steps that always finishes and produces an answer.
If we think of the language L as a “problem,” as will be the case frequently,
then problem Z is called decidabie if it is a recursive language, and it is called
undecidabie if it is not a recursive language.


--- Page 390 ---
374 CHAPTER 9. UNDECIDABILITY

recursive

Figure 9.2: Relationship between the recursive, RE, and non-RE languages

The existence or nonexistence of an algorithm to solve a problem is often
of more importance than the existence of some TM to solve the problem. As
mentioned above, the Turing machines that are not guaranteed to halt may not
give us enough information ever to conclude that a string is not in the language,
so there is a sense in which they have not “solved the problem.” Thus, dividing
problems or languages between the decidable — those that are solved by an
algorithm — and those that are undecidable is often more important than the
division between the recursively enumerable languages (those that have TM's of
some sort) and the non-recursively-enumerable languages (which have no TM
at all). Figure 9.2 suggests the relationship among three classes of languages:

1. The recursive languages.
2. The languages that are recursively enumerable but not recursive.
3. The non-recursively-enumerable (non-RE) languages.

We have positioned the non-RE language Lg properly, and we also show the
language Ly, or “universal language,” that we shall prove shortly not to be
recursive, although it is RE.

9.2.2 Complements of Recursive and RE languages

A powerful tool in proving languages to belong in the second ring of Fig. 9.2 (ie.,
to be RE, but not recursive) is consideration of the complement of the language.
We shall show that the recursive languages are closed under complementation.
Thus, if a language L is RE, but Z, the complement of £, is not RE, then we


--- Page 391 ---
9.2. AN UNDECIDABLE PROBLEM THAT IS RE 375

Why “Recursive”?

Programmers today are familiar with recursive functions. Yet these recur-
sive functions don’t seem to have anything to do with Turing machines
that always halt. Worse, the opposite —- nonrecursive or undecidable —
refers to languages that cannot be recognized by any algorithm, yet we
are accustomed to thinking of “nonrecursive” as referring to computations
that are so simple there is no need for recursive function calls.

The term “recursive,” as a synonym for “decidable,” goes back to
Mathematics as it existed prior to computers. Then, formalisms for com-
putation based on recursion (but not iteration or loops) were commonly
used as a notion of computation. These notations, which we shall not
cover here, had some of the flavor of computation in functional program-
ming languages such as LISP or ML. In that sense, to say a problem was
“recursive” had the positive sense of “it is sufficiently simple that I can
write a recursive function to solve it, and the function always finishes.”
That is exactly the meaning carried by the term today, in connection with
Turing machines.

The term “recursively enumerable” harks back to the same family of
concepts. A function could list all the members of a language, in some
order; that is, it could “enumerate” them. The languages that can have
their members listed in some order are the same as the languages that are
accepted by some TM, although that TM might run forever on inputs that
it does not accept.

know L cannot be recursive. For if L were recursive, then £ would also be
recursive and thus surely RE. We now prove this important closure property of
the recursive languages.

Theorem 9.3: If L is a recursive language, so is L.

PROOF: Let £ = L(M) for some TM M that always halts. We construct a ™
M such that E = Lie ) by the construction suggested in Fig. 9.3. That is, M
behaves just like Af. However, M is modified as follows to create M:

1. The accepting states of M are made nonaccepting states of M with no
transitions; i.e., in these states M will halt without accepting.

2. M has a new accepting state r; there are no transitions from r.
3. For each combination of a nonaccepting state of Af and a tape symbol of

M such that M has no transition (i.e., M halts without accepting), add
a transition to the accepting state r.


--- Page 392 ---
376 CHAPTER 9. UNDECIDABILITY

Accept
Reject

Figure 9.3: Construction of a TM accepting the complement of a recursive
language

Since M is guaranteed to halt, we know that M7 is also guaranteed to halt.
Moreover, MM accepts exactly those strings that Mf does not accept. Thus Mf
accepts £. O

There is another important fact about complements of languages that fur-
ther restricts where in the diagram of Fig. 9.2 a language and its complement
can fall. We state this restriction in the next theorem.

Theorem 9.4: If both a language L and its complement are RE, then L is
recursive. Note that then by Theorem 9.3, Z is recursive as well.

PROOF: The proof is suggested by Fig. 9.4. Let L = L(M,) and Z = E(My).
Both Mé, and Mp are simulated in parallel by a TM M. We can make M a
two-tape TM, and then convert it to a one-tape TM, to make the simulation
easy and obvious. One tape of M simulates the tape of M41, while the other tape
of M simulates the tape of Mz. The states of M, and M2 are each components

of the state of M.
Accept i, Accept

Accept —————_ Reject

Figure 9.4: Simulation of two TM’s accepting a language and its complement

If input w to M is in L, then At, will eventually accept. If so, M accepts
and halts. If w is not in L, then it is in Z, so My will eventually accept. When
Mz accepts, M halts without accepting. Thus, on ail inputs, Af halts, and


--- Page 393 ---
9.2. AN UNDECIDABLE PROBLEM THAT IS RE 377

L(M) is exactly L. Since M always halts, and L(A) = L, we conclude that L
is recursive. O

We may summarize Theorems 9.3 and 9.4 as follows. Of the nine possible
ways to place a language Z and its complement £ in the diagram of Fig. 9.2,
only the following four are possible:

1. Both £ and EF are recursive; i.e., both are in the inner ring.
2. Neither Z nor £ is RE; i.e., both are in the outer ring.

3. Lis RE but not recursive, and E is not RE; i.e., one is in the middle ring
and the other is in the outer ring.

4. Lis RE but not recursive, and I. is not RE; i.e., the same as (3), but with
E and L swapped.

In proof of the above, Theorem 9.3 eliminates the possibility that one language
(L or LE) is recursive and the other is in either of the other two classes. Theo-
rem 9.4 eliminates the possibility that both are RE but not recursive.

Example 9.5: As an example, consider the language Lg, which we know is
not RE. Thus, £q could not be recursive. It is, however, possible that Lg could
be either non-RE or RE-but-not-recursive. It is in fact the latter.

La is the set of strings w; such that MM; accepts w;. This language is similar
to the universal language L, consisting of all pairs (A¢,w) such that M accepts
w, which we shall show in Section 9.2.3 is RE. The same argument can be used
to show Lgis RE. O

9.2.3 The Universal Language

We already discussed informally in Section 8.6.2 how a Turing machine could be
used to simulate a computer that had becn loaded with an arbitrary program.
That is to say, a single TM can be used as a “stored program computer,”
taking its program as well as its data from one or more tapes on which input is
placed. In this section, we shall repeat the idea with the additional formality
that comes with talking about the Turing machine as our representation of a
stored program.

We define L,,, the universal language, to be the set of binary strings that
encode, in the notation of Section 9.1.2, a pair (M,w), where M is a TM with
the binary input alphabet, and w is a string in (0+1)*, such that w is in L(AZ).
That is, D, is the set of strings representing a TM and an input accepted by
that IM. We shail show that there is a TM U, often called the universal Turing
machine, such that L, = L(U). Since the input to U is a binary string, U is
in fact some M; in the list of binary-input Turing machines we developed in
Section 9.1.2.


--- Page 394 ---
378

CHAPTER 9. UNDECIDABILITY

It is easiest to describe U as a multitape Turing machine, in the spirit of
Fig. 8.22. In the case of U, the transitions of M are stored initially on the first
tape, along with the string w. A second tape will be used to hold the simulated
tape of M, using the same format as for the code of M. That is, tape symbol
X; of M will be represented by 0', and tape symbols will be separated by single
\’s. The third tape of U holds the state of M, with state g; represented by 7
0’s. A sketch of Y is in Fig. 9.5.

Finite
control

Input

Tape of M 0001000001010001

State of M 000 *** OBB ‘*:

Scratch

Figure 9.5: Organization of a universal Turing machine

The operation of U can be summarized as follows:

1.

Examine the input to make sure that the code for M is a legitimate code
for some TM. If not, U halts without accepting. Since invalid codes are
assumed to represent the TM with no moves, and such a TM accepts no
inputs, this action is correct.

. Initialize the second tape to contain the input w, in its encoded form.

That is, for each 0 of w, place 10 on the second tape, and for each 1 of
w, place 100 there. Note that the blanks on the simulated tape of M,
which are represented by 1000, will not actually appear on that tape; all
cells beyond those used for w will hold the blank of U. However, 7 knows
that, should it look for a simulated symbol of M and find its own blank,
it must replace that blank by the sequence 1000 to simulate the blank of
Ad.


--- Page 395 ---
9.2. AN UNDECIDABLE PROBLEM THAT IS RE 379

A More Efficient Universal TM

A efficient simulation of M by U, one that would not require us to shift
symbols on the tape, would have U first determine the number of tape
symbols M used. If there are between 2! + 1 and 2* symbols, U could

use a k-bit binary code to represent the different tape symbols uniquely.
Tape cells of M could be simulated by k of U’s tape cells. To make things
even easier, the given transitions of M could be rewritten by U to use
the fixed-length binary code instead of the variable-length unary code we
introduced.

3. Place 0, the start state of M, on the third tape, and move the head of
U’s second tape to the first simulated cell.

4, To simulate a move of M, U searches on its first tape for a transition
0'10/10*10'10", such that 0? is the state on tape 3, and 0 is the tape
symbol of M that begins at the position on tape 2 scanned by U. This
transition is the one M would next make. U should:

(a) Change the contents of tape 3 to 0*; that is, simulate the state change
of M. To do so, U first changes all the 0’s on tape 3 to blanks, and
then copies 0* from tape 1 to tape 3.

(b) Replace 0/ on tape 2 by 0'; that is, change the tape symbol of M.
If more or less space is needed (i.e., i #1), use the scratch tape and
the shifting-over technique of Section 8.6.2 to manage the spacing.

(c) Move the head on tape 2 to the position of the next 1 to the left
or right, respectively, depending on whether m = 1 (move left) or
m = 2 (move right). Thus, U simulates the move of M to the left or
to the right.

5. If Af has no transition that matches the simulated state and tape symbol,
then in (4}, no transition will be found. Thus, A¥ halts in the simulated
configuration, and U must do likewise.

6. If M enters its accepting state, then U accepts.

In this manner, U simulates M on w. U accepts the coded pair (M,w) if and
only if Af accepts w.

9.2.4 Undecidability of the Universal Language

We can now exhibit a problem that is RE but not recursive; it is the language
L,. Knowing that L,, is undecidable (i.¢., not a recursive language) is in many
ways more valuable than our previous discovery that Lg is not RE. The reason


--- Page 396 ---
380 CHAPTER 9. UNDECIDABILITY

The Halting Problem

One often hears of the halting problem for Turing machines as a problem
similar to £,, — one that is RE but not recursive. In fact, the original
Turing machine of A. M. Turing accepted by halting, not by final state.
We could define H(A!) for TM AM to be the set of inputs w such that M
halts given input w, regardless of whether or not M accepts w. Then, the
haiting problem is the set of pairs (A¢,w) such that w is in H(M). This
problem /language is another example of one that is RE but not recursive.

is that the reduction of L, to another problem P can be used to show there
is no algorithm to solve P, regardless of whether or not P is RE. However,
reduction of Ly to P is only possible if P is not RE, so La cannot be used to
show undecidability for those problems that are RE but not recursive. On the
other hand, if we want to show a problem not to be RE, then only Ly can be
used; £, is useless since it is RE.

Theorem 9.6: £,, is RE but not recursive.

PROOF: We just proved in Section 9.2.3 that L,, is RE. Suppose L,, were
recursive. Then by Theorem 9.3, E,, the complement of fu, would also be
recursive. However, if we have a TM M to accept F,,, then we can construct a
TM to accept La (by a method explained below). Since we already know that
Lg is not RE, we have a contradiction of our assumption that L,, is recursive.

Hypothetical
algorithm
M for L

Accept —|—* Accept

w Copy wlllw

Reject —/—* Reject
M’ for L, |

Figure 9.6: Reduction of Lg to Ly

Suppose L(A) = Ly. As suggested by Fig. 9.6, we can modify TM M into
a TM MM’ that accepts Lg as follows.

1. Given string w on its input, M' changes the input to wlllw. You may,
as an exercise, write a TM program to do this step on a single tape.
However, an easy argument that it can be done is to use a second tape to
copy w, and then convert the two-tape TM to a one-tape TM.


--- Page 397 ---
9.2. AN UNDECIDABLE PROBLEM THAT IS RE 381

2. M’ simulates M on the new input. If w is w; in our enumeration, then
M! determines whether M; accepts w;. Since M accepts Ly, it will accept
if and only if Mf; does not accept w,; ie., w; is in Lg.

Thus, Mf‘ accepts w if and only if w is in Lg. Since we know M’ cannot exist
by Theorem 9.2, we conclude that L,, is not recursive. OU

9.2.5 Exercises for Section 9.2

Exercise 9.2.1: Show that the halting problem, the set of (!M,w) pairs such
that M halts (with or without accepting) when given input w is RE but not
recursive. (See the box on “The Halting Problem” in Section 9.2.4.)

Exercise 9.2.2: In the box “Why ‘Recursive’?” in Section 9.2.1 we suggested
that there was a notion of “recursive function” that competed with the Turing
machine as a model for what can be computed. In this exercise, we shall
explore an example of the recursive-function notation. A recursive function
is a function F defined by a finite set of rules. Each rule specifies the value
of the function F' for certain arguments; the specification can use variables,
nonnegative-integer constants, the successor (add one) function, the function
F itself, and expressions built from these by composition of functions. For
example, Ackermann’s function is defined by the rules:

1. A(O,y) = 1 for any y > 0.
2. A(1,0) = 2.
3. A(w,0) =2+2 for 2 > 2.
4. A(a +1,y +1) = A(AGz,y + 1),y) for any 2 > 0 and y 2 0.
Answer the following:
* a) Evaluate A(2, 1).
tb) What function of a is A(z, 2)?
!c) Evaluate A(4, 3).
Exercise 9.2.3: Informally describe multitape Turing machines that enumer-
ate the following sets of integers, in the sense that started with blank tapes, it
prints on one of its tapes 10*'10"1--- to represent the set {a %2,...}.
* a) The set of all perfect squares {1,4,9,.-.}.

b) The set of all primes {2, 3,5, 7, 11,...}.


--- Page 398 ---
382 CHAPTER 9. UNDECIDABILITY

ic) The set of all ¢ such that M; accepts w;. Hint: It is not possible to gencrate
all these i’s in numerical order. The reason is that this language, which
is Lg, is RE but not recursive. In fact, a definition of the RE-but-not-
recursive languages is that they can be enumerated, but not in numerical
order. The “trick” to enumerating them at all is that we have to simulate
all M;’s on w,, but we cannot allow any M; to run forever, since it would
preclude trying any other M, for 7 4 i as soon as we encountered some
MM; that does not halt on w,;. Thus, we need to operate in rounds, where
in the kth round we try only a limited set of M;’s, and we do so for only
a limited number of steps. Thus, each round can be completed in finite
time. As long as for each TM M; and for each number of steps s there is
some round such that Mé, will be simulated for at least s steps, then we
shall eventually discover each M; that accepts w; and enumerate i.

* Exercise 9.2.4: Let Ly, £2,...,L,% be a collection of languages over alphabet
= such that:

1. For alli #7, £;N L; =; i.e., no string is in two of the languages.

2. £yU Le U---U Ly = BF; ie., every string is in one of the languages.

3. Each of the languages £,;, for i = 1,2,...,% is recursively enumerable.
Prove that each of the languages is therefore recursive.

*! Exercise 9.2.5; Let Z be recursively enumerable and let E be non-RE. Con-
sider the language

L' = {0w | w isin L} U {1w | w is not in D}

Can you say for certain whether L' or its complement are recursive, RE, or
non-RE? Justify your answer.

Exercise 9.2.6: We have not discussed closure properties of the recursive
languages or the RE languages, other than our discussion of complementation
in Section 9.2.2. Tell whether the recursive languages and/or the RE languages
are Closed under the following operations. You may give informal, but clear,
constructions to show closure.

* a) Union.

b) Intersection.

c) Concatenation.

d) Kleene closure (star).
* e) Homomorphism.

f) Inverse homomorphism.


--- Page 399 ---
9.3. UNDECIDABLE PROBLEMS ABOUT TURING MACHINES = 383

9.3 Undecidable Problems About Turing
Machines

We shall now use the languages L,, and Ly, whose status regarding decidability
and recursive enumerability we know, to exhibit other undecidable or non-RE
languages. The reduction technique will be exploited in each of these proofs.
Our first undecidable problems are all about Turing machines. In fact, our
discussion in this section culminates with the proof of “Rice’s theorem,” which
says that any nontrivial property of Turing machines that depends only on
the language the TM accepts must be undecidable. Section 9.4 will let us
investigate some undecidable problems that do not involve Turing machines or
their languages.

9.3.1 Reductions

We introduced the notion of a reduction in Section 8.1.3. In general, if we have
an algorithm to convert instances of a problem P, to instances of a problem
Py that have the same answer, then we say that P,; reduces to P.. We can
use this proof to show that PF, is at least as hard as P,. Thus, if P, is not
recursive, then P, cannot be recursive. If P; is non-RE, then P) cannot be RE.
As we mentioned in Section 8.1.3, you must be careful to reduce a known hard
problem to one you wish to prove to be at least as hard, never the opposite.

Ny

F, F

Figure 9.7: Reductions turn positive instances into positive, and negative to
negative

As suggested in Fig. 9.7, a reduction must turn any instance of P; that has
a “yes” answer into an instance of F2 with a “yes” answer, and every instance
of P, with a “no” answer must be turned into an instance of P2 with a “no”
answer. Note that it is not essential that every instance of FP, be the target of
one or more instances of P;, and in fact it is quite common that only a small


--- Page 400 ---
384 CHAPTER 9. UNDECIDABILITY

fraction of P2 is a target of the reduction.

Formally, a reduction from P, to P, is a Turing machine that takes an in-
stance of P, written on its tape and halts with an instance of PF on its tape.
In practice, we shall generally describe reductions as if they were computer
programs that take an instance of P, as input and produce an instance of Py
as output. The equivalence of Turing machines and computer programs allows
us to describe the reduction by either means. The importance of reductions is
emphasized by the following theorem, of which we shall see numerous applica-
tions.

Theorem 9.7: If there is a reduction from P, to P:, then:

a) If P, is undecidable then so is Py.
b) If P, is non-RE, then so is Ps.

PROOF: First suppose P, is undecidable. If it is possible to decide P, then we
can combine the reduction from P, to P2 with the algorithm that decides Po
to construct an algorithm that decides P;. The idea was suggested in Fig. 8.7.
In more detail, suppose we are given an instance w of P,. Apply to w the
algorithm that converts w into an instance x of-P,, Then apply the algorithm
that decides to P, to x. If that algorithm says “yes ”\then gis in Fy. Because
we reduced P; to Pe, we know the answer to w for P; is “yes”; ie., wisin Py.
Likewise, if z is not in PF, then w is not in P,, and whatever answer we give to
the question “is z in P)?” is also the correct answer to “is w in Pi?

We have thus contradicted the assumption that P; is undecidable. Our
conclusion is that if P, is undecidable, then P, is also undecidable.

Now, consider part (b). Assume that P, is non-RE, but Py is RE. Now,
we have an algorithm to reduce P; to P., but we have only 4 procedure to
recognize P2; that is, there isa TM that says “yes” if its input is in P, but may
not halt if its input is not in P,. As for part (a), starting with an instance w of
P,, convert it by the reduction algorithm to an instance ¢ of Ps. Then apply
the TM for F. to x. If x is accepted, then accept w.

This procedure describes a TM (which may not halt) whose language is P,.
If w is in P,, then is in Pe, so this TM will accept w. If w is not in F,, then z
is not in ). Then, the TM may or may not halt, but will surely not accept w.
Since we assumed no TM for P; exists, we have shown by contradiction that
no TM for P2 exists either; i.e., if P, is non-RE, then P, is non-RE. O

9.3.2 Turing Machines That Accept the Empty Language

As an example of reductions involving Turing machines, let us investigate two
languages called ZL, and ne. Each consists of binary strings. If w is a binary
string, then it represents some TM, M,, in the enumeration of Section 9.1.2.
If £(M;) = @, that is, M; does not accept any input, then w is in Le.
Thus, Le is the language consisting of all those encoded TM’s whose language


--- Page 401 ---
9.3. UNDECIDABLE PROBLEMS ABOUT TURING MACHINES 385

is empty. On the other hand, if L(M;) is not the empty language, then w is in
Ine Thus, Dne is the language of all codes for Turing machines that accept at
least one input string.

In what follows, it is convenient to regard strings as the Turing machines
they represent. Thus, we may define the two languages just mentioned as:

o L.={M | L(M) =9}
@ Lne = {M | L(M) £9}

Notice that ZL, and Lye are both languages over the binary alphabet {0,1},
and that they are complements of one another. We shall see that Dye is the
“easier” of the two languages; it is RE but not recursive. On the other hand,
EL, is non-RE.

Theorem 9.8: Ly, is recursively enumerable.

PROOF: We have only to exhibit a TM that accepts Ly. It is easiest to describe
a nondeterministic TM M, whose plan is shown in Fig. 9.8. By Theorem 8.11,
M can be converted to a deterministic TM.

Guessed

w Accept Accept

M for L

Figure 9.8: Construction of a NTM to accept Line

The operation of M is as follows.

1. M takes as input a TM code Mj.

2. Using its nondeterministic capability, M guesses an input w that Mf; might
accept.

3. M tests whether M; accepts w. For this part, M can simulate the uni-
versal TM U that accepts Ly.

4. If M; accepts w, then M aecepts its own input, which is M;.

In this manner, if M; accepts even one string, Mf will guess that string (among
all others, of course), and accept M;. However, if L(Mi) = 9, then no guess w
leads to acceptance by M;, so M does not accept Mj. Thus, (MM) = Ene. O


--- Page 402 ---
386 CHAPTER 9. UNDECIDABILITY

Our next step is to prove that Lye is not recursive. To do sO, we reduce
Ly tO Lye. That is, we shall describe an algorithm that transforms an input
(Af, w) into an output M’, the code for another Turing machine, such that w
is in L{M) if and only if L(Af‘) is not empty. That is, M accepts w if and
only if Af’ accepts at least one string. The trick is to have M' ignore its input,
and instead simulate Mf on input w. If Mf accepts, then M’ accepts its own
input; thus acceptance of w by M is tantamount to L(M’) being nonempty. If
Ene were recursive, then we would have an algorithm to tell whether or not Af
accepts w: construct M' and see whether L{Af") = 9.

Theorem 9.9: Lye is not recursive.

PROOF: We shall follow the outline of the proof given above. We must design
an algorithm that converts an input that is a binary-coded pair (M,w) into a
TM Af’ such that L(M‘) ¢ Oif and only if Af accepts input w. The construction
of AM" is sketched in Fig. 9.9. As we shall see, if M does not accept w, then M’
accepts none of its inputs; ie., L(A’) = 6. However, if M accepts w, then Md’
accepts every input, and thus L(Af’) surely is not @.

Figure 9.9: Plan of the TM M’ constructed from (M,w) in Theorem 9.9; Ag?
accepts arbitrary input if and only if M accepts w

M’ is designed to do the following:

1, Mf" ignores its own input z. Rather, it replaces its input by the string
that represents TM M and input string w. Since M’ is designed for a
specific pair (M,w), which has some length n, we may construct M’ to
have a sequence of states go, @1,---,@n, where gp is the start state.

(a} In state qi, for i = 0,1,...,. —1, M’ writes the (¢ + L)st bit of the
code for (M4, w), goes to state gi41, and moves right.

(b) In state q,, Af’ moves right, if necessary, replacing any nonblanks
(which would be the tail of z, if that input to M’ is longer than n)
by blanks.

2. When M’ reaches a blank in state qn, it uses a similar collection of states
to reposition its head at the left end of the tape.

3. Now, using additional states, AZ’ simulates a universal TM U on its
present tape.


--- Page 403 ---
9.8. UNDECIDABLE PROBLEMS ABOUT TURING MACHINES 387

4. If U accepts, then M4’ accepts. If U never accepts, then Af ’ never accepts
either.

The description of M’ above should be sufficient to convince you that you could
design a Turing machine that would transform the code for M and the string
w into the code for M’. That is, there is an algorithm to perform the reduction
of Ly to Ene. We also see that if M accepts w, then M’ accepts whatever
input was originally on its tape. The fact that 2 was ignored is irrelevant, the
definition of acceptance by a TM says that whatever was placed on the tape,
before commencing operation, is what the TM accepts. Thus, if M accepts w,
then the code for AM’ is in Jpg.

Conversely, if M does not accept w, then Af’ never accepts, no matter
what its input is. Hence, in this case the code for Af’ is not in Lye. We have
successfully reduced Ly to Lpe by the algorithm that constructs AM’ from M and
w; we may conclude that, since L,, is not recursive, neither is Lye. The existence
of this reduction is sufficient to complete the proof. However, to illustrate the
impact of the reduction, we shall take this argument one step further. If Ere
were recursive, then we could develop an algorithm for Ly as follows:

1. Convert (A, w) to the TM M’ as above.

2, Use the hypothetical algorithm for Lne to tell whether or not L(M’) = 0.
If so, say M does not accept w; if L(M’) #@, say M does accept w-

Since we know by Theorem 9.6 that no such algorithm for L, exists, we have
contradicted the assumption that Zne is recursive, and conclude that Ene is not
recursive. O

Now, we know the status of L,. If L. were RE, then by Theorem 9.4, both
it and Ene would be recursive. Since Lye is not recursive by Theorem 9.9, we
conclude that:

Theorem 9.10: £.is not RE. O

9.3.3 Rice’s Theorem and Properties of the RE Languages

The fact that languages like Le and Ene are undecidable is actually a special case
of a far more general theorem: all nontrivial properties of the RE languages are
undecidable, in the sense that it is impossible to recognize by a Turing machine
those binary strings that are codes for a TM whose language has the property.
An example of a property of the RE languages is “the language is context free.”
It is undecidable whether a given TM accepts a context-free language, as a
special case of the general principle that all nontrivial properties of the RE
languages are undecidable.

A property of the RE languages is simply a set of RE languages. Thus, the
property of being context-free is formally the set of all CFL’s. The property of
being empty is the set {6} consisting of only the empty language.


--- Page 404 ---
388 CHAPTER 9. UNDECIDABILITY

Why Problems and Their Complements are Different

Our intuition tells us that a problem and its complement are really the
same problem. To solve one, we can use an algorithm for the other, and
at the last step, complement the output: say “yes” instead of “no,” and
vice-versa. That instinct is exactly right, as long as the problem and its
complement are recursive.

However, as we discussed in Section 9.2.2, there are two other possi-
bilities. First, neither the problem nor its complement are even RE. Then,
neither can be solved by any kind of TM at all, so in a sense the two are
again similar. However, the interesting case, typified by L. and Ene, is
when one is RE and the other is non-RE.

For the language that is RE, we can design a TM that takes an input
w and searches for a reason why w is in the language. Thus, for Ine,
given a TM M as input, we set our TM looking for strings that the TM
M accepts, and as soon as we find one, we accept M. If Af isa TM with
an empty language, we never know for certain that M is not in ne, but
we never accept Af, and that is the correct response by the TM.

On the other hand, for the complement problem L,, which is not RE,
there is no way ever to accept all its strings. Suppose we are given a string
M that is a TM whose language is empty. We can test inputs to the TM
M, and we may never find one that M accepts, yet we can never be sure
that there isn’t some input we’ve not yet tested, that this TM accepts.
Thus, Mf can never be accepted, even if it should be.

A property is trivial if it is either empty (i.¢., satisfied by no language at
all}, or is all RE languages. Otherwise, it is nontrivial.

¢ Note that the empty property, @, is different from the property of being
an empty language, {9}.

We cannot recognize a set of languages as the languages themselves. The
reason is that the typical language, being infinite, cannot be written down as
a finite-length string that could be input to a TM. Rather, we must recognize
the Turing machines that accept those languages; the TM code itself is finite,
even if the language it accepts is infinite. Thus, if P is a property of the RE
languages, the language Lp is the set of codes for Turing machines Af; such that
£(M;} is a language in P. When we talk about the decidability of a property
P, we mean the decidability of the language Lp.

Theorem 9.11: (Rice’s Theorem) Every nontrivial property of the RE lan-
guages is undecidable.

PROOF: Let P be a nontrivial property of the RE languages. Assume to begin
that 6, the empty language, is not in P; we shall return later to the opposite


--- Page 405 ---
9.3. UNDECIDABLE PROBLEMS ABOUT TURING MACHINES 389

case. Since P is nontrivial, there must be some nonempty language L that is
in P. Let My be a TM accepting L.

We shall reduce L,, to Lp, thus proving that Lp is undecidable, since Ly,
is undecidable. The algorithm to perform the reduction takes as input a pair
(M,w) and produces a TM MM’. The design of M' is suggested by Fig. 9.10;
L(M’) is 0 if M does not accept w, and L(M’) = L if M accepts w.

Figure 9.10: Construction of M' for the proof of Rice’s Theorem

M' is a two-tape TM. One tape is used to simulate M on w. Remember
that the algorithm performing the reduction is given M and w as input, and
can use this input in designing the transitions of Md’. Thus, the simulation of
M on w is “built into” A’; the latter TM does not have to read the transitions
of M on a tape of its own.

The other tape of M’ is used to simulate My, on the input x to AM’, if
necessary. Again, the transitions of Af; are known to the reduction algorithm
and may be “built into” the transitions of M4’. The TM M’ is constructed to
do the following:

1. Simulate M on input w. Note that w is not the input to M’; rather, M’
writes Mf and w onto one of its tapes and simulates the universal TM U
on that pair, as in the proof of Theorem 9.8.

2. If M does not accept w, then M’ does nothing else. Af’ never accepts its
own input, «, so L(M") = 9. Since we assume @ is not in property P, that
means the code for M’ is not in Lp.

3. If M accepts w, then M' begins simulating M, on its own input x. Thus,
M! will accept exactly the language L. Since L is in P, the code for Af’
is in Lp.

You should observe that constructing M’ from M and w can be carried out by
an algorithm. Since this algorithm turns (M,w) into an M' that is in Lp if and
only if (M,w) is in L,, this algorithm is a reduction of L,, to Lp, and proves
that the property ? is undecidable.

We are not quite done. We need to consider the case where (isin P. If
so, consider the complement property P, the set of RE languages that do not
have property P. By the foregoing, P is undecidable. However, since every TM


--- Page 406 ---
390 CHAPTER 9. UNDECIDABILITY

accepts an RE language, Lp, the set of (codes for) Turing machines that do
not accept a language in P is the same as Lg, the set of TM’s that accept a
language in P. Suppose L-p were decidable. Then so would be Lz, because the
complement of a recursive language is recursive (Theorem 9.3). O

9.3.4 Problems about Turing-Machine Specifications

All problems about Turing machines that involve only the language that the
TM accepts are undecidable, by Theorem 9.11. Some of these problems are
interesting in their own right. For instance, the following are undecidable:

I. Whether the language accepted by a TM is empty (which we knew from
Theorems 9.9 and 9.3).

2. Whether the language accepted by a TM is finite.
3. Whether the language accepted by a TM is a regular language.

4, Whether the language accepted by a TM is a context-free language.

However, Rice’s Theorem does not imply that everything about a TM is
undecidable. For instance, questions that ask about the states of the TM,
rather than about the language it accepts, could be decidable.

Example 9.12: It is decidable whether a TM has five states. The algorithm
to decide this question simply looks at the code for the TM and counts the
number of states that appear in any of its transitions.

As another example, it is decidable whether there exists some input such
that the TM makes at least five moves. The algorithm becomes obvious when
we remember that if a TM makes five moves, then it does so looking only at
the nine cells of its tape surrounding its initial head position. Thus, we may
simulate the TM for five moves on any of the finite number of tapes consisting
of five or fewer input symbols, preceded and followed by blanks. If any of these
simulations fails to reach a halting situation, then we conclude that the TM
makes at least five moves on some input. O

9.3.5 Exercises for Section 9.3

Exercise 9.3.1: Show that the set of Turing-machine codes for TM’s that
accept all inputs that are palindromes (possibly along with some other inputs)
is undecidable.

Exercise 9.3.2: The Big Computer Corp. has decided to bolster its sagging
market share by manufacturing a high-tech version of the Turing machine, called
BWM, that is equipped with bells and whistles. The BWTM is basically the
same as your ordinary Turing machine, except that each state of the machine is
labeled either a “bell-state” or a “whistle-state.” Whenever the BWTM enters


--- Page 407 ---
9.3, UNDECIDABLE PROBLEMS ABOUT TURING MACHINES 391

a new state, it either rings the bell or blows the whistle, depending on which
type of state it has just entered. Prove that it is undecidable whether a given
BWTM J, on given input w, ever blows the whistle.

Exercise 9.3.3: Show that the language of codes for TM’s M that, when
started with blank tape, eventually write a 1 somewhere on the tape is unde-
cidable.

Exercise 9.3.4: We know by Rice’s theorem that none of the following prob-
lems are decidable. However, are they recursively enumerable, or non-RE?

a) Does L(M) contain at least two strings?
b) Is L(A2} infinite?
c) Is L(M) a context-free language?

* d) Is L(M) = (L(M))*?

Exercise 9.3.5: Let L be the language consisting of pairs of TM codes plus
an integer, (441, Me,k), such that L(M 1} M L(M2) contains at least & strings.
Show that £ is RE, but not recursive.

Exercise 9.3.6: Show that the following questions are decidable:

* a) The set of codes for TM’s M such that, when started with blank tape
will eventually write some nonblank symbol on its tape. Hint: If Mf has
m. states, consider the first m transitions that it makes.

!'b) The set of codes for TM's that never make a move left.

1c) The set of pairs (A4,w) such that TM M, started with input w, never
scans any tape cell more than once.

Exercise 9.3.7: Show that the following problems are not recursively enumer-
able:

* a) The set of pairs (M,w) such that TM M, started with input w, does not
halt.

b) The set of pairs (4,, Mz) such that (Mi) M L(M2) = %.

c) The set of triples (Mf, M2, M3) such that £(M,) = L(Me)L(Ms3); Le.,
the language of the first is the concatenation of the languages of the other
two TM’s.

Exercise 9.3.8: Tell whether each of the following are recursive, RE-but-not-
recursive, or non-RE.

* a) The set of all TM codes for TM’s that halt on every input.
b} The set of all TM codes for TM’s that halt on no input.
c) The set of all TM codes for TM’s that halt on at least one input.
* d) The set of all TM codes for TM’s that fail to halt on at least one input.


--- Page 408 ---
392 CHAPTER 9. UNDECIDABILITY

9.4 Post’s Correspondence Problem

In this section, we begin reducing undecidable questions about Turing machines
to undecidable questions about “real” things, that is, common matters that have
nothing to do with the abstraction of the Turing machine. We begin with a
problem called “Post’s Correspondence Problem” (PCP), which is still abstract,
but it involves strings rather than Turing machines. Our goal is to prove this
problem about strings to be undecidable, and then use its undecidability to
prove other problems undecidable by reducing PCP to those.

We shall prove PCP undecidable by reducing Z, to PCP. To facilitate the
proof, we introduce a “modified” PCP, and reduce the modified problem to the
original PCP. Then, we reduce £,, to the modified PCP. The chain of reductions
is suggested by Fig. 9.11. Since the original L, is known to be undecidable, we
conclude that PCP is undecidable.

PCP

algorithm

Figure 9.11: Reductions proving the undecidability of Post’s Correspondence
Problem

9.4.1 Definition of Post’s Correspondence Problem

An instance of Post’s Correspondence Problem (PCP) consists of two lists of
strings over some alphabet ©; the two lists must be of equal length. We generally
refer to the A and B lists, and write A = w,,we,..., wy and B = 21, 29,..-,2%,
for some integer k. For each i, the pair (w;,2;) is said to be a corresponding
pair.

We say this instance of PCP has a solution, if there is a sequence of one or
more integers ¢,/2,...,%m that, when interpreted as indexes for strings in the
A and 8 lists, yield the same string. That is, wi,wi.+-- wi, = £4, Lig “Ey.
We say the sequence i), %2....,im is @ solution to this instance of PCP, if so.
The Post’s correspondence problein is:

e Given an instance of PCP, tell whether this instance has a solution.

Example 9,13: Let © = {0,1}, and let the A and B lists be as defined in
Fig. 9.12. In this case, PCP has a solution. For instance, let m = 4,74, = 2,
tg = 1, tg = 1, and i4 = 3; i.e., the solution is the list 2,1,1,3. We verify that
this list is a solution by concatenating the corresponding strings in order for
the two lists. That is, wow.wiw 3 = fo2%12173 = 101111110. Note this solution
is not unique. For instance, 2,1, 1,3,2,1,1,3 is another solution. O


--- Page 409 ---
9.4. POST’S CORRESPONDENCE PROBLEM 393

List A | List B

Figure 9.12: An instance of PCP

PCP as a Language

Since we are discussing the problem of deciding whether a given instance
of PCP has a solution, we need to express this problem as a language. As
PCP allows instances to have arbitrary alphabets, the language PCP is
really a set of strings over some fixed alphabet, which codes instances of
PCP, much as we coded Turing machines that have arbitrary sets of states
and tape symbols, in Section 9.1.2. For example, if a PCP instance has
an alphabet with up to 2* symbols, we can use distinct k-bit binary codes
for each of the syrnbols.

Since each PCP instance has a finite alphabet, we can find some k
for each instance. We can then code all instances in a 3-symbol alphabet
consisting of 0, 1, and a “comma” symbol to separate strings. We begin
the code by writing & in binary, followed by a comma. Then follow each of
the pairs of strings, with strings separated by commas and their symbols
coded in a k-bit binary code.

Example 9.14: Here is an example where there is no solution. Again we let
Y= {0,1}, but now the instance is the two lists given in Fig. 9.13.

Suppose that the PCP instance of Fig. 9.13 has a solution, say #1, ¢2,---,%m,
for some m > 1. We claim i; = 1. For if 4; = 2, then a string beginning
with wo = 011 would have to equal a string that begins with sz = 11. But
that equality is impossible, since the first symbols of these two strings are 0
and 1, respectively. Similarly, it is not possible that i; = 3, since then a string
beginning with w3 = 101 would have to equal a string begining with z3 = 011.

If 4; = 1, then the two corresponding strings from lists A and B would have
to begin:

A: 10--:
B: 101-:-

Now, let us see what. #2 could be.

1. If i2 = 1, then we have a problem, since no string beginning with uw) =


--- Page 410 ---
394 CHAPTER 9. UNDECIDABILITY

List A | List B

Figure 9.13: Another PCP instance

1010 can match a string that begins with 2,2; = 101101; they must
disagree at the fourth position.

2. If ig = 2, we again have a problem, because no string that begins with
wwe = 10011 can match a string that begins with z)a2 = 10111; they
must differ at the third position.

3. Only ig = 3 is possible.

If we choose 72 = 3, then the corresponding strings formed from list of integers
41,43 are:

A: 10101---
B: 101011 ---

There is nothing about these strings that immediately suggests we cannot ex-
tend list 1,3 to a solution. However, we can argue that it is not possible to do
so. The reason is that we are in the same condition we were in after choosing
#4; = 1. The string from the B list is the same as the string from the A list
except that in the B list there is an extra 1 at the end. Thus, we are forced
to choose ig = 3, 74 = 3, and so on, to avoid creating a mismatch. We can
never allow the A string to catch up to the B string, and thus can never reach
a solution. O

9.4.2 The “Modified” PCP

It is easier to reduce L,, to PCP if we first introduce an intermediate version of
PCP, which we call the Modified Pest’s Correspondence Problem, or MPCP. In
the modified PCP, there is the additional requirement on a solution that the first
pair on the A and B lists must be the first pair in the solution. More formally,
an instance of MPCP is two lists A = w1,we,...,w, and B = Ti, 02,-.-, 2k,
and a solution is a list of 0 or more integers i), %2,...,%, such that

WW, Wig WW Hi, Lig “Li

Notice that the pair (u,,2,) is forced to be at the beginning of the two
strings, even though the index 1 is not mentioned at the front of the list that


--- Page 411 ---
9.4. POST’S CORRESPONDENCE PROBLEM 395

Partial Solutions

In Example 9.14 we used a technique for analyzing PCP instances that
comes up frequently. We considered what the possible parttal solu-
tions were, that is, sequences of indexes i1,i2,...,i, such that one of
Wi, Wig Wi, aNd Zi, Lig -++ i, is a prefix of the other, although the two
strings are not equal. Notice that if a sequence of integers is a solution,

then every prefix of that sequence must be a partial solution. Thus, un-
derstanding what the partial solutions are allows us to argue about what
solutions there might be.

Note, however, that because PCP is undecidable, there is no algorithm
to compute all the partial solutions. There can be an infinite number of
them, and worse, there is no upper bound on how different. the lengths of
the strings wyj, wi, «+: wy, and ©i, Fi, --+ 2, can be, even though the partial
solution leads to a solution.

is the solution. Also, unlike PCP, where the solution has to have at least one
integer on the solution list, in MPCP, the empty list could be a solution if
w, = x, (but those instances are rather uninteresting and will not figure in our
use of MPCP).

Example 9.15: The lists of Fig. 9.12 may be regarded as an instance of MPCP.
However, as an instance of MPCP it has no solution. In proof, observe that
any partial solution has to begin with index 1, so the two strings of a solution
would begin:

A: 1--:
B: 111---

The next integer could not be 2 or 3, since both wz and w3 begin with 10 and
thus would produce a mismatch at the third position. Thus, the next index
would have to be 1, yielding:

A: 1l---
B: 1LILI11--:

We can argue this way indefinitely. Only another 1 in the solution can avoid a
mismatch, but if we can only pick index 1, the B string remains three times as
long as the A string, and the two strings can never become equal.

An important step in showing PCP is undecidable is reducing MPCP to
PCP. Later, we show MPCP is undecidable by reducing L, to MPCP. At that
point, we will have a proof that PCP is undecidable as well; if it were decidable,
then we could decide MPCP, and thus £,.


--- Page 412 ---
396 CHAPTER 9, UNDECIDABILITY

Given an instance of MPCP with alphabet ©, we construct an instance of
PCP as follows. First, we introduce a new symbol * that, in the PCP instance,
goes between every symbol in the strings of the MPCP instance. However, in
the strings of the A list, the *’s follow the symbols of ©, and in the B list, the
*'s precede the symbols of ©. The one exception is a new pair that is based on
the first pair of the MPCP instance; this pair has an extra * at the beginning of
uy, so it can be used to start the PCP solution. A final pair (*, «$) is added to
the PCP instance. This pair serves as the last in a PCP solution that mimics
a solution to the MPCP instance.

Now, tet us formalize the above construction. We are given an instance of
MPCP with lists A = wi,we,..-,w, and B = a1,20,...,2%. We assume +
and $ are symbols not present in the alphabet 5 of this MPCP instance. We
construct a PCP instance C = yo, yi,..-,Yer1 and D = z,2,...,2%41, a8
follows:

1. Forti =1,2,...,h, let y; be w; with a * after each symbol of w;, and let
z; be «; with a * before each symbol of 2;.

2. Yo = *y1, and zp = z,. That is, the Oth pair looks like pair 1, except that
there is an extra * at the beginning of the string from the first list. Note
that the Oth pair will be the only pair in the PCP instance where both
strings begin with the same symbol, so any solution to this PCP instance
will have to begin with index 0.

3. Yet1 = $ and Zkh+1 = «8,

Example 9.16: Suppose Fig. 9.12 is an MPCP instance. Then the instance
of PCP constructed by the above steps is shown in Fig. 9.14. O

List D
a zi
QO | «1x *«lelxl
1 | 1x *1lxle]
2 | laDelelele | «1x0
3 | lxO+ *Q
41 % *B

Figure 9.14: Constructing an instance of PCP from an MPCP instance

Theorem 9.17: MPCP reduces to PCP.

PROOF: The construction given above is the heart of the proof. First, suppose
that ¢,22,...,%m is a solution to the given MPCP instance with lists A and B.
Then we know wyw,, wi, ++ Wi,, = ©12i, Ci, * + 2i,,. If we were to replace the


--- Page 413 ---
9.4. POST’S CORRESPONDENCE PROBLEM 397

w’s by y’s and the «’s by 2’s, we would have two strings that were almost the
SAME: Yi, Yin Yim ANA 2124, Ziq -+- Z,,- The difference is that the first string
would be missing a * at. the beginning, and the second would be missing a * ai,
the end. That is,

FYI YE; Yio Yin = 2128) Fiz 1° Zin, *

However, ¥o = #y1, and zp = 21, so we can fix the initial + by replacing the
first index by 0. We then have:

Voi, Yis > -* Yin = 407i, Zig + °° Zi ¥

We can take care of the final « by appending the index k + 1. Since y41 = $,
and zp41 = *$, we have:

YOU, Vig °° Hi, VRAL = 2024, Zig 77 Fi, Fh41

We have thus shown that 0,4),42,...,im,4 +1 is a solution to the instance of
PCP.

Now, we must show the converse, that if the constructed instance of PCP
has a solution, then the original MPCP instance has a solution as well. We
observe that a solution to the PCP instance must begin with index 0 and end
with index & +1, since only the Oth pair has strings yo and zp that begin with
the same symbol, and only the (k+1)st pair has strings that end with the same
symbol. Thus, the PCP solution can be written 0,41, %2,---,im,44+1.

We claim that 7), i2,...,im is a solution to the MPCP instance. The reason
is that if we remove the *’s and the final $ from the string yoYi, Vie *** Fin Yaoi
we get the string w1%;, wi, ---w;,,- Also, if we remove the *’s and $ from the
Strilg 292i, Zig °° Zi, Zht1 WE Bet TL, Ti, -* + Ti,,- We know that

Yoder Vig °° Vim Vk+1 = 202i, Fig 6° Fi Fh41

so it follows that

Wy Wi, Wig Wy, FS ay Big 1 PG

m m

Thus, 2 solution to the PCP instance implies a solution to the MPCP instance.

We now see that the construction described prior to this theorem is an
algorithm that converts an instance of MPCP with a solution to an instance of
PCP with a solution, and also converts an instance of MPCP with no solution
to an instance of PCP with no solution. Thus, there is a reduction of MPCP
to PCP, which confirms that if PCP were decidable, MPCP would also be
decidable. O

9.4.3 Completion of the Proof of PCP Undecidability

We now complete the chain of reductions of Fig. 9.11 by reducing £,, to MPCP.
That is, given a pair (M,w), we construct an instance (A,B) of MPCP such
that TM MM accepts input w if and only if (4, 8} has a solution.


--- Page 414 ---
398 CHAPTER 9. UNDECIDABILITY

The essential idea is that MPCP instance (A,B) simulates, in its partial
solutions, the computation of Mf on input w. That is, partial solutions will con-
sist of strings that are prefixes of the sequence of ID’s of M: #a, #as#a3# :--,
where ay is the initial ID of M with input w, and a; a4, for alli. The string
from the B list will always be one ID ahead of the string from the A list, unless
M enters an accepting state. In that case, there will be pairs to use that will
allow the A list to “catch up” to the B list and eventually produce a solution.
However, without entering an accepting state, there is no way that these pairs
can be used, and no solution exists.

To simplify the construction of an MPCP instance, we shall invoke Theo-
rem 8.12, which says that we may assume our TM never prints a blank, and
never moves left. from its initial head position. In that case, an ID of the Turing
machine will always be a string of the form ag@, where o and @ are strings of
nonblank tape symbols, and g is a state. However, we shall allow 8 to be empty
if the head is at the blank immediately to the right of a, rather than placing a
blank to the right of the state. Thus, the symbols of a and @ will correspond
exactly to the contents of the cells that held the input, plus any cells to the
right that the head has previously visited.

Let M = (Q,%,T,8,90,.8,F) be a TM satisfying Theorem 8.12, and let w
in £* be an input string. We construct an instance of MPCP as follows. To
understand the motivation behind our choice of pairs, remember that the goal
is for the first list to be one ID behind the second list, unless Mf accepts.

1. The first pair is:

List A List B
# gow

This, pair, which must start any solution according to the rules of MPCP,
begins the simulation of M on input w. Notice that initially, the B list is
a complete ID ahead of the A list.

2. Tape symbols and the separator # can be appended to both lists. The
pairs

List A List B
xX A for each X in T

# #

allow symbols not involving the state to be “copied.” In effect, choice of
these pairs lets us extend the A string to match the B string, and at the
same time copy parts of the previous ID to the end of the B string. So
doing helps to form the next ID in the sequence of moves of M, at the
end of the B string.


--- Page 415 ---
9.4. POST’S CORRESPONDENCE PROBLEM 399

3. To simulate a move of M, we have certain pairs that reflect those moves.
For all g in Q — F (i.e., g is a nonaccepting state), pin Q, and X, Y, and
Z in Tl we have:

List A List B

qX Yp if 6(g, X) = (p, ¥, R)
ZoX poy if 6(g,X) = (p, Y, L); 2 is any tape symbol
a# Yp# if 6(¢, B) = (p, Y, 2)

(

L2ao# pZY# if 5(¢,B) = (p, Y, £); Z is any tape symbol

Like the pairs of (2), these pairs help extend the @ string to add the next
ID, by extending the A string to match the B string. However, these pairs
use the state to determine the change in the current ID that is needed
to produce the next ID. These changes — a new state, tape symbol, and
head move — are reflected in the ID being constructed at the end of the
B string.

4, If the ID at the end of the B string has an accepting state, then we need
to allow the partial solution to become a complete solution. We do so by
extending with “ID’s” that are not really [D’s of M, but represent what
would happen if the accepting state were allowed to consume all the tape
symbols to either side of it. Thus, if g is an accepting state, then for all
tape symbols X and Y, there are pairs:

List A List B

XqY @q
Xq g
ay q

5. Finally, once the accepting state has consumed all tape symbols, it stands
alone as the last ID on the B string. That is, the remainder of the two
strings (the suffix of the B string that must be appended to the A string
to match the B string) is g#. We use the final pair:

List A List B
qt  #

to complete the solution.

In what follows, we refer to the five kinds of pairs generated above as the pairs
from rule (1), rule (2), and so on.

Example 9.18: Let us convert the TM
M= ({q1, 42,93}, {0, 1}, {0, 1, By,8,0, B, {g3})

where 6 is given by:


--- Page 416 ---
400 CHAPTER 9. UNDECIDABILITY

Gi 8(gi, B}

(go, 1, Ft) (92,0, £} (q2,41, £)
a2 (g3,0, L) (91,0, 2) (qo, 0, R)
@3 —_ — —_—

and input string w = 01 to an instance of MPCP. To simplify, notice that M
never writes a blank, so we shall never have B in an ID. Thus, we shall omit
all the pairs that involve B. The entire list of pairs is in Fig. 9.15, along with
explanations about where each pair comes from.

Rule = A | List B Source

from 6(g;,0) = (g2,1,R)
from é(q1, De = (g2,0,£)

from 6(q1,1) = (2,0, D)
Om# | q20i# | from d(q,B) = (q2,1,L)
Og20 | gs007% =| from 6(g2,0) = (¢3,0, £)
1qod g3 104 from é(g2,0) = (g3,0, £)
gol Oj from d(g2,1) = (q,,0, R)

Figure 9.15: MPCP instance constructed from TM M of Example 9.18

Note that M accepts the input 01 by the sequence of moves

Let us see the sequence of partial solutions that mimics this computation of Af
and eventually leads to a. solution. We must start with the first pair, as required
in any solution to MPCP:

A: #
B: #q,01#


--- Page 417 ---
9.4. POST’S CORRESPONDENCE PROBLEM 401

The only way to extend the partial solution is for the string from the A list
to be a prefix of the remainder, ¢i014. Thus, we must next choose the pair
(910, 1q2), which is one of those move-simulating pairs that we got from rule (3).
The partial solution is thus:

A: #40
B: #q01#1l¢e

We may now further extend the partial solution using the “copying” pairs from
rule (2), until we get to the state in the second ID. The partial solution is then:

A: #q,01#1
B: #q01#1q21#1

At this point, we can use another of the rule-(3) pairs to simulate a move; the
appropriate pair is (g21,0q1), and the resulting partial solution is:

A: #q,017#1g21
B:. #q01#flqol#l0n

We now could use rule-(2) pairs to “copy” the next three symbols: #, 1, and 0.
However, to go that far would be a mistake, since the next move of Jf moves
the head left, and the 0 just before the state is needed in the next rule-(3) pair.
Thus, we only “copy” the next two symbols, leaving partial solution:

A: #q,0171¢214#1
B: #q01#1q21#10q #1

The appropriate rule-(3) pair to use is (Ogi #, 42017), which gives us the partial
solution:

A: #q,01#1q.1#10q,#
B: #q,01#19¢21#10q) #1q2.01#

Now, we may use another rule-(3) pair, (1q20, g310), which leads to acceptance:

A: #q,01#41q21#10q1 #1920
B: #q,01#1q214 10g) #1 qa01¥# 93 10

At this point, we use pairs from rule (4) to eliminate all but g3 from the ID. We
also need pairs from rule (2) to copy symbols as necessary. The continuation of
the partial solution is:

A: #£9)01#1go1#109, #14201 49310149301 #9314
B: #q01#1q.1# 10M Fig Fg INL Fgs0l#¥galega#

With only g3 left in the ID, we can use the pair (q34#¥4, #) from rule (5) to
finish the solution:


--- Page 418 ---
402 CHAPTER 9. UNDECIDABILITY

Az #q1 019 1q21 410g; #1q201 4 g3101 #301 #93 Las HH
B: #01 #1q21# 10g, Hl q.01 #gslOl#gs0 Hes l#aHH

ao

Theorem 9.19: Post’s Correspondence Problem is undecidable.

PROOF: We have almost completed the chain of reductions suggested by Fig.
9.11. The reduction of MPCP to PCP was shown in Theorem 9.17. The con-
struction of this section shows how to reduce f,, to MPCP. Thus, we complete
the proof of undecidability of PCP by proving that the construction is correct,
that is:

e M accepts w if and only if the constructed MPCP instance has a solution.

{Only-if} Example 9.18 gives the fundamental idea. If w is in £(M), then we
can start with the pair from rule (1), and simulate the computation of M on
w. We use a pair from rule (3) to copy the state from each ID and simulate
one move of MZ, and we use the pairs from rule (2) to copy tape symbols and
the marker # as needed. If Af reaches an accepting state, then the pairs from
tule (4) and a final use of the pair from rule (5) allow the A string to catch up
to the B string and form a solution.

(If} We need to argue that if the MPCP instance has a solution, it could only be
because M accepts w. First, because we are dealing with MPCP, any solution
must. begin with the first pair, so a partial solution begins

A: #
B: #qowtt

As long as there is no accepting state in the partial solution, the pairs from
rules (4) and (5) are useless. States and one or two of their surrounding tape
symbols in an ID can only be handled by the pairs of rule (3), and all other
tape symbols and # must be handled by pairs from rule (2). Thus, unless Af
reaches an accepting state, all partial solutions have the form

Ais
B: xy

where « is a sequence of ID’s of M representing a computation of M on input
w, possibly followed by # and the beginning of the next ID a. The remainder
y is the completion of a, another #, and the beginning of the ID that follows
a, up to the point that z ended within a itself.

In particular, as long as Mf does not enter an accepting state, the partial
solution is not a solution; the B string is longer than the A string. Thus, if
there is a solution, M4 must at some point enter an accepting state; ie. Af
accepts w. O


--- Page 419 ---
*

—_

—

9.5. OTHER UNDECIDABLE PROBLEMS 403

9.4.4 Exercises for Section 9.4

Exercise 9.4.1: Tell whether each of the following instances of PCP has a
solution. Each is presented as two lists A and B, and the ith strings on the two
lists correspond for each ¢ = 1,2,....

* a) A= (01,001, 10); B = (011, 10,00).
b) A = (01,001, 10); B = (011,01, 00).
c) A= (ab, a, be,c); B = (be, ab, ca, a).

Exercise 9.4.2: We showed that PCP was undecidable, but we assumed that
the alphabet © could be arbitrary. Show that PCP is undecidable even if we
limit the alphabet to © = {0,1} by reducing PCP to this special case of PCP.

Exercise 9.4.3: Suppose we limited PCP to a one-symbol alphabet, say 2 =
{0}. Would this restricted case of PCP still be undecidable?

Exercise 9.4.4: A Post teg system consists of a set of pairs of strings chosen
from some finite alphabet E and a start string. If (w,a) is a pair, and y is
any string over 5, we say that wy t yx. That is, on one Move, we can remove
some prefix w of the “current” string wy and instead add at the end the second
component of a string « with which w is paired. Define F to mean zero or
more steps of F, just as for derivations in a context-free grammar. Show that
it is undecidable, given a set of pairs P and a start string z, whether z Fe.
Hint: For each TM M and input w, let z be the initial ID of M with input w,
followed by a separator symbol #. Select the pairs P such that any ID of M
must eventually become the ID that follows by one move of M. If M enters an
accepting state, arrange that the current string can eventually be erased, i.e.,
reduced to e.

9.5 Other Undecidable Problems

Now, we shall consider a variety of other problems that we can prove undecid-
able. The principal technique is reducing PCP to the problem we wish to prove
undecidable.

9.5.1 Problems About Programs

Our first observation is that we can write a program, in any conventional lan-
guage, that takes as input an instance of PCP and searches for solutions some
systematic manner, e.g., in order of the length (number of pairs) of potential
solutions. Since PCP allows arbitrary alphabets, we should encode the symbols
of its alphabet in binary or some other fixed alphabet, as discussed in the box
on “PCP as a Language” in Section 9.4.1.


--- Page 420 ---
404 CHAPTER 9. UNDECIDABILITY

We can have our program do any particular thing we want, e.g., halt or
print hello, world, when and if it finds a solution. Otherwise, the program
will never perform that particular action. Thus, it is undecidable whether a
program prinis hello, world, whether it halts, whether it calls a particular
function, rings the console bell, or makes any other nontrivial action. In fact,
there is an analog of Rice’s Theorem for programs: any nontrivial property that
involves what the program does (rather than a lexical or syntactic property of
the program itself) must be undecidable.

9.5.2 Undecidability of Ambiguity for CFG’s

Programs are sufficiently like Turing machines that the observations of Sec-
tion 9.5.1 are unsurprising. Now, we shall see how to reduce PCP to a problem
that looks nothing like a question about computers: the question of whether a
given context-free grammar is ambiguous.

The key idea is to consider strings that represent a list of indexes (integers),
in reverse, and the corresponding strings according to one of the lists of a
PCP instance. These strings can be generated by a grammar. The similar set
of strings for the other list in the PCP instance can also be generated by a
grammar. If we take the union of these grammars in the obvious way, then
there is a string generated through the productions of each original grammar if
and only if there is a solution to this PCP instance. Thus, there is a solution if
and only if there is ambiguity in the grammar for the union.

Let us now make these ideas more precise. Let the PCP instance consist of
lists A = wy ,Wo,..-,w, and B = z1,22,...,@%. For list A we shall construct
a CFG with A as the only variable. The terminals are all the symbols of the
alphabet © used for this PCP instance, plus a distinct set of index symbols
&,@2,...,a,4 that represent the choices of pairs of strings in a solution to the
PCP instance. That is, the index symbol a; represents the choice of w; from
the A list or x; from the B list. The productions for the CFG for the A list are:

A + w)Aa, | weAae |---| weAag |
Wa, | W2a2 |---| weap

We shall call this grammar Ga and its language L4. In the future, we shall
refer to a language like D4 as the language for the list A.

Notice that the terminal strings derived by G4 are all those of the form
Wi, Wig + Wi, Os, °° ° Gigdi, for some m > 1 and list of integers i),22,...,4m3
each integer is in the range 1 to k. The sentential forms of G4 all have a single
A between the strings (the w’s) and the index symbols (the a’s), until we use
one of the last group of k productions, none of which have an A in the body.
Thus, parse trees look like the one suggested in Fig. 9.16.

Observe also that any terminal string derivable from A in G4 has a unique
derivation. The index symbols at the end of the string determine uniquely
which production must be used at each step. That is, only two production
bodies end with a given index symbol a;: A 4 w,Aa; and A 4 wja;. We must


--- Page 421 ---
9.5. OTHER UNDECIDABLE PROBLEMS 405
A
Pa
Wj ; A a; ;
Ww; , a,

2 , ia

Pane

mn mn

tml

Figure 9.16; The form of parse trees in the grammar G4

use the first of these if the derivation step is not the last, and we must use the
second production if it is the last step.
Now, let us consider the other part of the given PCP instance, the list

B=2,,%2,...,2,. For this list we develop another grammar Ga:
B + 2 Ba, | s2Bay |---| ce Bag |
eT ay | e202 |---| LAE

The language of this grammar will be referred to as Ly. The same observations
that we made for G4 apply also to Gg. In particular, a terminal string in Lg
has a unique derivation, which can be determined by the index symbols in the
tail of the string.

Finally, we combine the languages and grammars of the two lists to form a
grammar Gp for the entire PCP instance. Gap consists of:

1. Variables A, B, and 5; the latter is the start symbol.
2. Productions S 4 A | B.

3. All the productions of G4.

4, All the productions of Gp.

We claim that Gap is ambiguous if and only if the instance (A, B) of PCP has
a solution; that argument is the core of the next theorem.

Theorem 9.20: It is undecidable whether a CFG is ambiguous.


--- Page 422 ---
406 CHAPTER 9. UNDECIDABILITY

PROOF: We have already given most of the reduction of PCP to the question
of whether a CFG is ambiguous; that reduction proves the problem of CFG
ambiguity to be undecidable, since PCP is undecidable. We have only to show
that the above construction is correct; that is:

« Gaz is ambiguous if and only if instance (A, B) of PCP has a solution.

(If) Suppose #1, 72,.-.,im is a solution to this instance of PCP. Consider the
two derivations in Gg:

S>A> wz, Aa, > Wi, Win AQi, O54, > S>
Wig Wig * Wig 4 AG, 1 7 igi, PH Wi, Wig Wig, Oi, Cin iy

S> B= «;, Ba; > 2;,%;,Bai.ai, > +--+ >
Bij Lig *** Lin, BAi,, , *-* Cig Gi, > Pi Tig Li, My, °° Aig Bi

Since #1,%2,..-,%m is a solution, we know that w;, wi, ---Wi,, = Gi, Lig i,
Thus, these two derivations are derivations of the same terminal string. Since
the derivations themselves are clearly two distinct, leftmost derivations of the
same terminal string, we conclude that Gan is ambiguous.

(Only-if} We already observed that a given terminal string cannot have more
than one derivation in G4 and not more than one in Gp. So the only way that
a terminal string could have two leftmost derivations in Gp is if one of them
begins S = A and continues with a derivation in G4, while the other begins
S => B and continues with a derivation of the same string in Gy.

The string with two derivations has a tail of indexes a;_ ---a;,@;,, for some
m > 1. This tail must be a solution to the PCP instance, because what pre-
cedes the tail in the string with two derivations is both w;,w;,---wi,, and
Filip 2i,,- O

Lal

9.5.3 The Complement of a List Language

Having context-free languages like D4 for the list A lets us show a number of
problems about CFL’s to be undecidable. More undecidability facts for CFL’s
can be obtained by considering the complement language L,4. Notice that the
language [4 consists of all strings over the alphabet © U {a1,02,...,a%} that
are not in £4, where © is the alphabet of some instance of PCP, and the a,’s
are distinct symbols representing the indexes of pairs in that PCP instance.

The interesting members of L4 are those strings consisting of a prefix in ©*
that is the concatenation of some strings from the A list, followed by a suffix
of index symbols that does not match the strings from A. However, there are
also many strings in L.4 that are simply of the wrong form: they are not in the
language of regular expression D*{a, + ag +--+ + a,)*.

We claim that £4 is a CFL. Unlike La, it is not very easy to design a
grammar for D4, but we can design a PDA, in fact a deterministic PDA, for
La. The construction is in the next theorem.


--- Page 423 ---
9,5. OTHER UNDECIDABLE PROBLEMS 407

Theorem 9.21: If L, is the language for list A, then La is a context-free
language.

PROOF: Let © be the alphabet of the strings on list A = wn, we,...,w,, and
let J be the set of index symbols: f = {ai,a2,.-- ,ax}. The DPDA P we design
to accept £4 works as follows.

1. As long as P sees symbols in ©, it stores them on its stack. Since all
strings in £* are in D4, P accepts as it goes.

2. As soon as a P sees an index symbol in J, say @;, it pops its stack to see if
the top symbols form w*, that is, the reverse of the corresponding string.

(a) If not, then the input seen so far, and any continuation of this input
is in L4. Thus, P goes to an accepting state in which it consumes
all future inputs without changing its stack.

(b) If w* was popped from the stack, but the bottom-of-stack marker
is not yet exposed on the stack, then P accepts, but remembers, in
its state that it is looking for symbols in J only, and may yet see a
string in L4 (which P will not accept). P repeats step (2) as long
as the question of whether the input is in 2,4 is unresolved.

(c) If w was popped from the stack, and the bottom-of-stack marker
is exposed, then P has seen an input in Ly. P does not accept this
input. However, since any input continuation cannot be in La, P
goes to a state where it accepts all future inputs, leaving the stack
unchanged.

3. If, after seeing one or more symbols of 7, P sees another symbol of &,
then the input is not of the correct form to be in L4. Thus, P goes toa
state in which it accepts this and all future inputs, without changing its
stack.

We can use £4, Lp and their complements in various ways to show unde-
cidability results about context-free languages. The next theorem summarizes
some of these facts.

Theorem 9.22: Let G, and Gy be context-free grammars, and let R be a
regular expression. Then the following are undecidable:

a) Is L(G) M L(G.) = 0?
b) Is L(G,) = L(G2)?

c) Is £(G,) = £(R)?

d) Is E(G,) = T* for some alphabet 1?


--- Page 424 ---
408 CHAPTER 9. UNDECIDABILITY

f) Is L(R) C L(G)?

PROOF: Each of the proofs is a reduction from PCP. We show how to take
an instance (A,B) of PCP and convert it to a question about CFG’s and/or
regular expressions that has answer “yes” if and only if the instance of PCP
has a solution. In some cases, we reduce PCP to the question as stated in the
theorem; in other cases we reduce it to the complement. It doesn’t matter, since
if we show the complement of a problem to be undecidable, it is not possible
that the problem itself is decidable, since the recursive languages are closed
under complementation (Theorem 9.3).

We shall refer to the alphabet of the strings for this instance as © and the
alphabet of index symbols as J. Our reductions depend on the fact that L4,
Lg, La, and Lg all have CFG’s. We construct these CFG’s either directly, as
in Section 9.5.2, or by the construction of a PDA for the complement languages
given in Theorem 9.21 coupled with the conversion from a PDA to a CFG by
Theorem 6.14.

a) Let £(G,) = La and L(G2) = Lg. Then L{G,) N L(G) is the set of
solutions to this instance of PCP. The intersection is empty if and only
if there is no solution. Note that, technically, we have reduced PCP to
the language of pairs of CF'G’s whose intersection is nonempty; i.e., we
have shown the problem “is the intersection of two CFG’s nonempty” to
be undecidable. However, as mentioned in the introduction to the proof,
showing the complement of a problem to be undecidable is tantamount
to showing the problem itself undecidable.

b) Since CFG’s are closed under union, we can construct a CFG G, for
D4 U Ep. Since (5 U I)* is a regular set, we surely may construct for it a
CFG Go. Now EL, ULg =LaM Lp. Thus, £(G1) is missing only those
strings that represent solutions to the instance of PCP. L(G2) is missing
no strings in (© U J}*. Thus, their languages are equal if and only if the
PCP instance has no solution.

The argument is the same as for (b), but we let R be the regular expression

(HU )*.

d) The argument. of (c) suffices, since © U J is the only alphabet of which
La U Lg could possibly be the closure.

e) Let G, be a CFG for (EU J)* and let Gz be a CFG for D4 U Lg. Then

L£(G1) C £(G2) if and only if L4 U Lp = (© U2)", i-e., if and only if the

PCP instance has no solution.

o
we

f} The argument is the same _as (e), but let R be the regular expression
(£2 U 7)*, and let L(G,) be La U Leg.



--- Page 425 ---
9.5. OTHER UNDECIDABLE PROBLEMS 409

9.5.4 Exercises for Section 9.5

Exercise 9.5.1: Let Z be the set of (codes for) context-free grammars G such
that L(G) contains at least one palindrome. Show that L is undecidable. Hint:
Reduce PCP to L by constructing, from each instance of PCP a grammar whose
language contains a palindrome if and only if the PCP instance has a solution.

Exercise 9.5.2: Show that the language L.4 U Ep is a regular language if and
only if it is the set of all strings over its alphabet; i.c., if and only if the instance
(A, B) of PCP has no solution. Thus, prove that it is undecidable whether or
not a CFG generates a regular language. Hint: Suppose there is a solution to
PCP; say the string wa is missing from L, U Lg, where w is a string from
the alphabet © of this PCP instance, and z is the reverse of the corresponding
string of index symbols. Define a homomorphism f{0) = w and h(1) = x. Then
what is h-1(Z4 U Lp)? Use the fact that regular sets are closed under inverse
homomorphism, complementation, and the pumping lemma for regular sets to
show that D4 U Ly is not regular.

Exercise 9.5.3: It is undecidable whether the complement of a CFL is also a
CFL. Exercise 9.5.2 can be used to show it is undecidable whether the comple-
ment of a CFL is regular, but that is not the same thing. To prove our initial
claim, we need to define a different language that represents the nonsolutions to
an instance (A, B) of PCP. Let Lag be the set of strings of the form wHr#y#z
such that:

1. w and z are strings over the alphabet © of the PCP instance.

2. y and z are strings over the index alphabet J for this instance.

3. # is a symbol in neither © nor J.

4.wHak,

5. y 2k,

6. 2 is not what the index string y generates according to list B.

7. wis not what the index string z* generates according to the list A.
Notice that Lap consists of all strings in ©*#&0*#J*#J" unless the instance
(A, B) has a solution, but Lag is a CFL regardless. Prove that Lag is a CFL
if and only if there is no solution. Hint: Use the inverse homomorphism trick

from Exercise 9.5.2 and use Ogden’s lemma to force equality in the lengths of
certain substrings as in the hint to Exercise 7.2.3(b).



--- Page 426 ---
410 CHAPTER 9. UNDECIDABILITY

9.6 Summary of Chapter 9

+ Recursive and Recursively Enumerable Languages: The languages ac-
cepted by Turing machines are called recursively enumerable (RE), and
the subset of RE languages that are accepted by a TM that always halts
are called recursive.

+ Complements of Recursive and RE Languages: The recursive languages
are closed under complementation, and if a language and its complement
are both RE, then both languages are actually recursive. Thus, the com-
plement of an RE-but-not-recursive language can never be RE.

 Decidability and Undecidability: “Decidable” is a synonym for “recur-
sive,” although we tend to refer to languages as “recursive” and prob-
lems (which are languages interpreted as 2 question) as “decidable.” If
a, language is not recursive, then we call the problem expressed by that
language “undecidable.”

@ The Language Lg: This language is the set of strings of 0’s and 1’s that,
when interpreted as a TM, are not in the language of that TM. The
language Lg is a good example of a language that is not RE; ie., no
Turing machine accepts it.

+ The Universal Language: The language L, consists of strings that are
interpreted as a TM followed by an input for that TM. The string is in
£, if the TM accepts that input. Ly is a good example of a language that
is RE but not recursive.

¢ Rice’s Theorem: Any nontrivial property of the languages accepted by
Turing machines is undecidable. For instance, the set of codes for Turing
machines whose language is empty is undecidable by Rice’s theorem. In
fact, this language is not RE, although its complement — the set of codes
for TM’s that accept at least one string — is RE but not recursive.

¢ Post's Correspondence Problem: This question asks, given two lists of the
same number of strings, whether we can pick a sequence of corresponding
strings from the two lists and form the same string by concatenation. PCP
is an important example of an undecidable problem. PCP is a good choice
for reducing to other problems and thereby proving them undecidable.

+ Undecidable Context-Free-Language Problems: By reduction from PCP,
we can show a number of questions about CFL's or their grammars to be
undecidable. For instance, it is undecidable whether a CFG is ambiguous,
whether one CFL is contained in another, or whether the intersection of
two CFL's is empty.


--- Page 427 ---
9.7, REFERENCES FOR CHAPTER 9 411

9.7 References for Chapter 9

The undecidability of the universal language is essentially the result of Turing
[9], although there it was expressed in terms of computation of arithmetic func-
tions and halting, rather than languages and acceptance by final state. Rice’s
theorem is from [8].

The undecidability of Post’s Correspondence problem was shown in [7], al-
though the proof used here was devised by R. W. Floyd, in unpublished notes.
The undecidability of Post tag systems (defined in Exercise 9.4.4) is from [6].

The fundamental papers on undecidability of questions about context-free
languages are [1] and [5]. However, the fact that it is undecidable whether a
CFG is ambiguous was discovered independently by Cantor [2], Floyd [4], and
Chomsky and Schutzenberger [3].

1. ¥. Bar-Hillel, M. Perles, and E. Shamir, “On formal properties of simple
phrase-structure grammars,” Z. Phonetik. Sprachwiss. Kommunikations-
forsch. 14 (1961), pp. 143-172.

2. D. C. Cantor, “On the ambiguity problem in Backus systems,” J. ACM
9:4 (1962}, pp. 477-479.

3. N. Chomsky and M. P. Schutzenberger, “The algebraic theory of con-
text-free languages,” Computer Programming and Formal Systems (1963),
North Holland, Amsterdam, pp. 118-161.

4. R. W. Floyd, “On ambiguity in phrase structure languages,” Communi-
cations of the ACM 5:10 (1962), pp. 526-534.

5. §. Ginsburg and G. F. Rose, “Some recursively unsolvable problems in
ALGOL-like languages,” J. ACM 10:1 (1963), pp. 29-47.

6. M. L. Minsky, “Recursive unsolvability of Post’s problem of ‘tag’ and
other topics in the theory of Turing machines,” Annals of Mathematics
74:3 (1961), pp. 437-455.

7. E. Post, “A variant of a recursively unsolvable problem,” Bulletin of the
AMS 52 (1946), pp. 264-268.

8. H. G. Rice, “Classes of recursively enumerable sets and their decision
problems,” Transactions of the AMS 89 (1953), pp. 25-59.

9. A.M. Turing, “On computable numbers with an application to the Ent-
scheidungsproblem,” Proc. London Math. Society 2:42 (1936), pp. 230-
265.


--- Page 428 ---


--- Page 429 ---
Chapter 10

Intractable Problems

We now bring our discussion of what can or cannot be computed down to the
level of efficient versus inefficient computation. We focus on problems that are
decidable, and ask which of them can be computed by Turing machines that
run in an amount of time that is polynomial in the size of the input. You should
review in Section 8.6.3 two important points:

¢ The problems solvable in polynomial time on a typical computer are ex-
actly the same as the problems solvable in polynomial time on a Turing
machine.

Experience has shown that the dividing line between problems that can be
solved in polynomial time and those that require exponential time or more
is quite fundamental. Practical problems requiring polynomial time are
almost always solvable in an amount of time that we can tolerate, while
those that require exponential time generally cannot be solved except for
small instances.

In this chapter we introduce the theory of “intractability,” that is, techniques
for showing problems not to be solvable in polynomial time. We start with a
particular problem — the question of whether a boolean expression can be
satisfied, that is, made true for some assignment of the truth values TRUE and
FALSE to its variables. This problem plays the role for intractable problems that
Ly, or PCP played for undecidable problems. That is, we begin with “Cook’s
Theorem,” which implies that the satisfiability of boolean formulas cannot be
decided in polynomial time. We then show how to reduce this problem to many
other problems, which are therefore shown intractable as well.

Since we are dealing with whether problems can be solved in polynomial
time, our notion of a reduction must change. It is no longer sufficient that there
be an algorithm to transform instances of one problem to instances of another.
The algorithm itself must take at most polynomial time, or the reduction does
not let us conclude that the target problem is intractable, even if the source

413


--- Page 430 ---
414 CHAPTER 10. INTRACTABLE PROBLEMS

problem is. Thus, we introduce the notion of “polynomial-time reductions” in
the first section.

‘There is another important distinction between the kinds of conclusions we
drew in the theory of undecidability and those that intractability theory lets
us draw. ‘The proofs of undecidability that we gave in Chapter 9 are incontro-
vertible; they depend on nothing but the definition of a Turing machine and
common mathematics. In contrast, the results on intractable problems that we
give here are all predicated on an unproved, but strongly believed, assumption,
often referred to as the assumption P 4 AP.

That is, we assume the class of problems that can be solved by nondetermin-
istic TM’s operating in polynomial time includes at least some problems that
cannot be solved by deterministic TM’s operating in polynomial time (even if
we allow a higher degree polynomial for the deterministic TM). There are lit-
erally thousands of problems that appear to be in this category, since they can
be solved easily by a polynomial time NTM, yet no polynomial-time DTM (or
computer program, which is the same thing) is known for their solution. More-
over, an important consequence of intractability theory is that either all these
problems have polynomial-time deterministic solutions, which have eluded us
for centuries, or none do; i.e., they really require exponential time.

10.1 The Classes P and VP

In this section, we introduce the basic concepts of intractability theory: the
classes P and A’P of problems solvable in polynomial time by deterministic
and nondeterministic TM’s, respectively, and the technique of polynomial-time
reduction. We also define the notion of “NP-completeness,” a property that
certain problems in A’P have; they are at least as hard {to within a polynomial
in time) as any problem in VP.

10.1.1 Problems Solvable in Polynomial Time

A Turing machine M is said to be of time complexity T(n) {or to have “running
time T(n)”] if whenever M is given an input w of length n, M halts after making
at most T'(n) moves, regardless of whether or not M accepts. This definition
applies to any function T(n), such as T(n) = 50n? or T(n) = 3" + 5n4: we
shall be interested predominantly in the case where T(n) is a polynomial in 7.
We say a language L is in class P if there is some polynomial T(n) such that
£ = E{M) for some deterministic TM A of time complexity T'(n).

10.1.2. An Example: Kruskal’s Algorithm

You are probably familiar with many problems that have efficient solutions;
perhaps you studied some in a course on data structures and algorithms. These
problems are generally in P. We shall consider one such problem: finding a
minimum-weight spanning tree (MWST) for a graph.


--- Page 431 ---
10.1. THE CLASSES P AND NP 415

Is There Anything Between Polynomials and
Exponentials?

In the introductory discussion, and subsequently, we shali often act as if
all programs either ran in polynomial time [time O(n*) for some integer
k] or in exponential time [time O(2") for some constant ¢ > OQ], or more.
In practice, the known algorithms for common problems generally do fall
into one of these two categories. However, there are running times that lie
between the polynomials and the exponentials. In all that we say about
exponentials, we really mean “any running time that is bigger than all the
polynomials.”

An example of a function between the polynomials and exponentials
is n/°62", This function grows faster than any polynomial in n, since logn
eventually (for large n) becomes bigger than any constant k, On the other
hand, nls2" = 2082"). if you don’t see why, take logarithms of both
sides. This function grows more slowly that 2°" for any c > 0. That is, no
matter how small the positive constant c is, eventually cn becomes bigger
than (log, 7)”.

Informally, we think of graphs as diagrams such as that of Fig. 10.1. There
are nodes, which are numbered 1-4 in this example graph, and there are edges
between some pairs of nodes. Each edge has a weight, which is an integer. A
spanning tree is a subset. of the edges such that all nodes are connected through
these edges, yet there are no cycles. An example of a spanning tree appears
in Fig. 10.1; it is the three edges drawn with heavy lines. A minimum-weight
spanning tree has the least possible total edge weight of all spanning trees.

Figure 10.1: A graph; its minimum-weight spanning tree is indicated by heavy
lines


--- Page 432 ---
416 CHAPTER 10. INTRACTABLE PROBLEMS

There is a well-known “greedy” algorithm, called Kruskal’s Algorithm,! for
finding a MWST. Here is an informal outline of the key ideas:

1. Maintain for each node the connected component in which the node ap-
pears, using whatever edges of the tree have been selected so far. Initially,
no edges are selected, so every node is then in a connected component by
itself,

2. Consider the lowest-weight edge that has not yet been considered; break
ties any way you like. If this edge connects two nodes that are currently
in different connected components then:

(a) Select that edge for the spanning tree, and

(b) Merge the two connected components involved, by changing the com-
ponent number of all nodes in one of the two components to be the
same as the component number of the other.

If, on the other hand, the selected edge connects two nodes of the same
component, then this edge does not belong in the spanning tree; it would
create a cycle.

3. Continue considering edges until either all edges have been considered, or
the number of edges selected for the spanning tree is one less than the
number of nodes. Note that in the latter case, all nodes must be in one
connected component, and we can stop considering edges.

Example 10.1: In the graph of Fig. 10.1, we first consider the edge (1,3),
because it has the lowest weight, 10. Since 1 and 3 are initially in different
components, we accept this edge, and make 1 and 3 have the same component
number, say “component 1.” The next edge in order of weights is (2,3), with
weight 12. Since 2 and 3 are in different components, we accept this edge and
merge node 2 into “component 1.” The third edge is (1,2), with weight 15.
However, 1 and 2 are now in the same component, so we reject this edge and
proceed to the fourth edge, (3,4). Since 4 is not in “component 1,” we accept
this edge. Now, we have three edges for the spanning tree of a 4-node graph,
and so may stop. O

It is possible to implement this algorithm (using a computer, not a Turing
machine} on a graph with m nodes and e edges in time O(m + eloge). A
simpler, easier-to-follow implementation proceeds in e rounds. A table gives
the current component of each node. We pick the lowest-weight remaining edge
in O{e) time, and find the components of the two nodes connected by the edge
in O(m) time. If they are in different components, merge all nodes with those
numbers in O(m) time, by scanning the table of nodes. The total time taken

1J. B. Kruskal Jr., “On the shortest spanning subtree of a graph and the traveling salesman
problem,” Proc. AMS 7:1 (1956), pp. 48-50.


--- Page 433 ---
10.1. THE CLASSES P AND NP 417

by this algorithm is O(e(e+m)). This running time is polynomial in the “size”
of the input, which we might informally take to be the sum of e and m.

When we translate the above ideas to Turing machines, we face several
issues:

e When we study algorithms, we encounter “problems” that ask for outputs
in a variety of forms, such as the list of edges ina MWST. When we deal
with Turing machines, we may only think of problems as languages, and
the only output is yes or no, ie., accept or reject. For instance, the
MWST tree problem could be couched as: “given this graph G and limit
W, does G have a spanning tree of weight W or less?” That problem
may seem easier to answer than the MWST problem with which we are
familiar, since we don’t even learn what the spanning tree is. However,
in the theory of intractability, we generally want to argue that a problem
is hard, not easy, and the fact that a yes-no version of a problem is
hard implies that a more standard version, where a full answer must be
computed, is also hard.

e While we might think informally of the “size” of a graph as the number
of its nodes or edges, the input to a TM is a string over a finite alphabet.
Thus, problem elements such as nodes and edges must be encoded suit-
ably. The effect of this requirement is that inputs to Turing machines are
generally slightly longer than the intuitive “size” of the input. However,
there are two reasons why the difference is not significant:

1. The difference between the size as a TM input string and as an
‘informal problem input is never more than a small factor, usually the
logarithm of the input size. Thus, what can be done in polynomial
time using one measure can be done in polynomial time using the
other measure.

2. The length of a string representing the input is actually a more ac-
curate measure of the number of bytes a real computer has to read
to get its input. For instance, if a node is represented by an integer,
then the number of bytes needed to represent that integer is propor-
tional to the logarithm of the integer’s size, and it is not “1 byte for
any node” as we might imagine in an informal accounting for input
s1lze.

Example 10.2: Let us consider a possible code for the graphs and weight lim-
its that could be the input to the MWST problem. The code has five symbols,
0, 1, the left and right parentheses, and the comma.

1. Assign integers 1 through m to the nodes.

2. Begin the code with the value of m in binary and the weight limit W in
binary, separated by a comma.


--- Page 434 ---
418 CHAPTER 10. INTRACTABLE PROBLEMS

3. If there is an edge between nodes i and j with weight w, place (i,j, w)
in the code. The integers 7, 7, and w are coded in binary. The order of
t and j within an edge, and the order of the edges within the code are
immaterial,

Thus, one of the possible codes for the graph of Fig. 10.1 with limit W = 40 is
100, 101000(1, 10, 1111)(1, 11, 1010}{10, 11, 1100) (10, 100, 10100)(11, 100, 10010)
O

If we represent inputs to the MWST problem as in Example 10.2, then
an input of length n can represent at most O(n/logn) edges. It is possible
that m, the number of nodes, could be exponential in n, if there are very few
edges. However, unless the number of edges, e, is at least m— 1, the graph
cannot be connected and therefore will have no MWST, regardless of its edges.
Consequently, if the number of nodes is not at least some fraction of n/logn,
there is no need to run Kruskal’s algorithm at all; we simply say “no; there is
no spanning tree of that weight.”

Thus, if we have an upper bound on the running time of Kruskal’s algorithm
as a function of m and e, such as the upper bound O{e(m-+e)) developed above,
we can conservatively replace both m and e by n and say that the running time,
as a function of the input length n is O(n(n + n)), or O(n?). In fact, a better
implementation of Kruskal’s algorithm takes time O(nlogn), but we need not
concern ourselves with that improvement here.

Of course, we are using a Turing machine as our model of computation, while
the algorithm we described was intended to be implemented in a programming
language with useful data structures such as arrays and pointers. However, we
claim that in O(n”) steps we can implement the version of Kruskal’s algorithm
described above on a multitape TM. The extra tapes are used for several jobs:

1. One tape can be used to store the nodes and their current component
numbers. The length of this table is O(n).

2. A tape can be used, as we scan the edges on the input tape, to hold the
currently least edge-weight found, among those edges that have not been
marked “used.” We could use a second track of the input tape to mark
those edges that were selected as the edge of least remaining weight in
some previous round of the algorithm. Scanning for the lowest-weight,
unmarked edge takes O(n) time, since each edge is considered only once,
and comparisons of weight can be done by a linear, right-to-left. scan of
the binary numbers.

3. When an edge is selected in a round, place its two nodes on a tape. Search
the table of nodes and components to find the components of these two
nodes. This task takes O(n) time.


--- Page 435 ---
10.1. THE CLASSES P AND NP 419

4. A tape can be used to hold the two components, ¢ and 7, being merged
when an edge is found to connect two previously unconnected components.
We then scan the table of nodes and components, and each node found
to be in component z has its component number changed to j. This scan
also takes O(n) time.

You should thus be able to complete the argument that says one round can
be executed in O(n) time on a multitape TM. Since the number of rounds, e,
is at. most m, we conclude that O(n”) time suffices on a multitape TM. Now,
remember Theorem 8.10, which says that whatever a multitape TM can do in
gs steps, a single-tape TM can do in O(s?) steps. Thus, if the multitape TM
takes O(n) steps, then we can construct a single-tape TM to do the same thing
in O((n?)?) = O(n*) steps. Our conclusion is that the yes-no version of the
MWST problem, “does graph G have a MWST of total weight W or less,” is
in P,

10.1.3 Nondeterministic Polynomial Time

A fundamental class of problems in the study of intractability is those problems
that can be solved by a nondeterministic TM that runs in polynomial timc.
Formally, we say a language L is in the class VP (nondeterministic polynomial)
if there is a nondeterministic TM M and a polynomial time complexity T(n)
such that E = L(M/), and when M is given an input of length », there are no
sequences of more than T(n) moves of Af.

Our first observation is that, since every deterministic TM is a nondeter-
ministic TM that happens never to have a choice of moves, P € WP. However,
it appears that A“P contains many problems not in P. The intuitive reason is
that a NTM running in polynomial time has the ability to guess an exponential
number of possible solutions to a problem and check each one in polynomial
time, “tn parallel.” However:

« It is one of the deepest open questions of Mathematics whether P = NP,
i.e., whether in fact everything that can be done in polynomial time by a
NTM can in fact be done by a DTM in polynomial time, perhaps with a
higher-degree polynomial.

10.1.4 An AVP Example: The Traveling Salesman
Problem

To get a feel for the power of A’P, we shall consider an example of a problem
that appears to be in WP but not in P: the Traveling Salesman Problem (TSP).
The input to TSP is the same as to MWST, a graph with integer weights on
the edges such as that of Fig. 10.1, and a weight limit W. The question asked
is whether the graph has a “Hamilton circuit” of total weight at most W. A
Hamilton circuit is a set of edges that connect the nodes into a single cycle,


--- Page 436 ---
420 CHAPTER 10. INTRACTABLE PROBLEMS

A Variant of Nondeterministic Acceptance

Notice that we have required of our NTM that it halt in polynomial time
along all branches, regardless of whether or not it accepts. We could just
as well have put the polynomial time bound T(r} on only those branches
that lead to acceptance; i-e., we could have defined NP as those languages
that are accepted by a NTM such that if it accepts, does so by at least
one sequence of at most 7'(m) moves, for some polynomial T{n).

However, we would get the same class of languages had we done so.
For if we know that M accepts within T(7) moves if it accepts at all, then
we could modify M to count up to T{n) on a separate track of its tape and
halt without accepting if it exceeds count T(n). The modified Mf might
take O(T?(n)) steps, but T?(n) is a polynomial if T(n) is.

In fact, we could also have defined P through acceptance by TM’s
that accept within time I'(n), for some polynomial T(n). These TM’s
might not halt if they do not accept. However, by the same construction
as for NTM’s, we could modify the DTM to count to T{n) and halt if the
limit is exceeded. The DTM would run in O(T?(n)) time.

with each node appearing exactly once. Note that the number of edges on a
Hamilton circuit must equal the number of nodes in the graph.

Example 10.3: The graph of Fig 10.1 actually has only one Hamilton circuit:
the cycle (1, 2,4,3,1). The total weight of this cycle is 15 + 20+ 18 +10 = 63,
Thus, if W is 63 or more, the answer is “yes,” and if W < 63 the answer is
tno.”

However, the TSP on four-node graphs is deceptively simple, since there
can never be more than two different Hamilton circuits once we account for the
different nodes at which the same cycle can start, and for the direction in which
we traverse the cycle. In m-node graphs, the number of distinct cycles grows
as O(m!), the factorial of m, which is more than 2° for any constante. O

It appears that all ways to solve the TSP involve trying essentially all cycles
and computing their total weight. By being clever, we can eliminate some
obviously bad choices. But it seems that no matter what we do, we must
examine an exponential number of cycles before we can conclude that there is
none with the desized weight limit W, or to find one if we are unlucky in the
order in which we consider the cycles.

On the other hand, if we had a nondeterministic computer, we could guess a
permutation of the nodes, and compute the total weight for the cycle of nodes in
that order. If there were a real computer that was nondeterministic, no branch
would use more than O(n) steps if the input was of length n. On a multitape
NTM, we can guess a permutation in O(n?) steps and check its total weight in


--- Page 437 ---
10.1. THE CLASSES P AND NP 421

a similar amount of time. Thus, a single-tape NIM can solve the TSP in O(n*)
time at most. We conclude that the TSP is in AP.

10.1.5 Polynomial-Time Reductions

Our principal methodology for proving that a problem P: cannot be solved in
polynomial time (i.e., Ps is not in P) is the reduction of a problem P,, which is
known not to be in P, to P2.2 The approach was suggested in Fig. 8.7, which
we reproduce here as Fig. 10.2.

P Construct B —»< Decide yes

instance instance

no
Figure 10.2: Reprise of the picture of a reduction

Suppose we want to prove the statement “if P, is in P, then so is P,.” Since
we claim that P; is not in P, we could then claim that P, is not in P either.
However, the mere existence of the algorithm labeled “Construct” in Fig. 10.2
is not sufficient to prove the desired statement.

For instance, suppose that when given an instance of P, of length m, the
algorithm produced an output string of length 2”, which it fed to the hypo-
thetical polynomial-time algorithm for P2. If that decision algorithm ran in,
say, time O(n*), then on an input of length 2” it would run in time O(2k™),
which is exponential in m. Thus, the decision algorithm for P, takes, when
given an input of length m, time that is exponential in m. These facts are
entirely consistent with the situation where P, is in P and P, is not in P.

Even if the algorithm that constructs a P; instance from a P, instance
always produces an instance that is polynomial in the size of its input, we can
fail to reach our desired conclusion. For instance, suppose that the instance of
Ps constructed is of the same size, m, as the P; instance, but the construction
algorithm itself takes time that is exponential in m, say O(2™). Now, a decision
algorithm for P; that takes polynomial time O(n*) on input of length n only
implies that there is a decision algorithm for P, that takes time O(2" + m*) on
input of length m. This running time bound takes into account the fact that we
have to perform the translation to P) as well as solve the resulting P2 instance.
Again it would be possible for P, to be in P and FP, not.

2That statement is a slight lie. In practice, we only assume P, is not in P, using the very
strong evidence that P, is NP-complete,” a concept we discuss in Section 10.1.6. We then
prove that P2 is also “NP-complete,” and thus suggest just as strongly that P is not in P.


--- Page 438 ---
422 CHAPTER 10. INTRACTABLE PROBLEMS

The correct restriction to place on the translation from P, to Fy is that it
requires time that is polynomial in the length of its input. Note that if the
translation takes time O(m/) on input of length m, then the output instance
of P2 cannot be longer than the number of steps taken, ice., it is at most em?
for some constant ¢. Now, we can prove that if P2 is in P, then so is Py.

For the proof, suppose that we can decide membership in P, of a string of
length n in time O(n*). Then we can decide membership in P;, of a string of
length m in time O(7n + (cm4)*) time; the term m/ accounts for the time to
do the translation, and the term (em?}* accounts for the time to decide the
resulting instance of P,. Simplifying the expression, we see that P, can be
solved in time O(m + ems*). Since c, j, and & are all constants, this time is
polynomial in m, and we conclude P, is in P.

Thus, in the theory of intractability we shall use polynomial-time reductions
only. A reduction from P, to P, is polynomial-time if it takes time that is some
polynomial in the length of the P, instance. Note that as a consequence, the P»
instance will be of a length that is polynomial in the length of the P, instance.

10.1.6 NP-Complete Problems

We shall next meet the family of problems that are the best-known candidates
for being in VP but not in P, Let L be a language (problem) in WP. We say
Lis NP-complete if the following statements are true about L:

1. Lisin NP.

2. For every language L' in A\’P there is a polynomial-time reduction of L’
to LE.

An example of an NP-complete problem, as we shall see, is the Traveling Sales-
man Problem, which we introduced in Section 10.1.4. Since it appears that
P # NP, and in particular, all the NP-complete problems are in NP — P, we
generally view a proof of NP-completeness for a problem as a proof that the
problem is not in P.

We shall prove our first problem, called SAT (for boolean satisfiability) to be
NP-complete by showing that the language of every polynomial-time NTM has
a polynomial-time reduction to SAT. However, once we have some NP-complete
problems, we can prove a new problem to be NP-complete by reducing some
known NP-complete problem to it, using a polynomial-time reduction. The
following theorem shows why such a reduction proves the target problem to be
NP-complete.

Theorem 10.4: If P, is NP-complete, P, is in WP, and there is a polynomial-
time reduction of P, to P,, then Py is NP-complete.

PROOF: We need to show that every language L in AP polynomial-time re-
duces to P,. We know that there is a polynomial-time reduction of L to Fi;


--- Page 439 ---
10.1. THE CLASSES P AND NP 423

NP-Hard Problems

Some problems L are so hard that although we can prove condition (2)
of the definition of NP-completeness (every language in A’P reduces to
L in polynomial time), we cannot prove condition (1): that L is in VP.
If so, we call LE NP-hard. We have previously used the informal term
“intractable” to refer to problems that appeared to require exponential
time. It is generally acceptable to use “intractable” to mean “NP-hard,”

although in principle there might be some problems that require exponen-
tial time even though they are not NP-hard in the formal sense.

A proof that E is NP-hard is sufficient to show that F is very likely to
require exponential time, or worse. However, if Z is not in A’P, then its
apparent difficulty does not support the argument that all NP-complete
problems are difficult. That is, it could turn out that P = AP, and yet £
still requires exponential time.

this reduction takes some polynomial time p(n). Thus, a string w in E of length
n is converted to a string xz in P, of length at most p(n).

We also know that there is a polynomial-time reduction of P, to P2; let
this reduction take polynomial time g(m). Then this reduction transforms 2 to
some string y in Py, taking time at most ¢(p(n)). Thus, the transformation of
w to y takes time at most p(n) + ¢({p{n)), which is a polynomial. We conclude
that EL is polynomial-time reducible to P,. Since £ could be any language in
NP, we have shown that all of WP polynomial-time reduces to P); i.e., P2 is
NP-complete. O

There is one more important theorem to be proven about NP-complete
problems: if any one of them is in P, then all of AP is in P. Since we believe
strongly that there are many problems in VP that are notin P, we thus consider
a proof that a problem is NP-complete to be tantamount to a proof that in has
no polynomial-time algorithm, and thus has no good computer solution.

Theorem 10.5: If some NP-complete problem P is in P, then P = A’P.

PROOF: Suppose P is both NP-complete and in P. Then all languages £ in
NP reduce in polynomial-time to P. If P isin P, then Lis in P, as we discussed
in Section 10.1.5. O

10.1.7 Exercises for Section 10.1

Exercise 10.1.1: Suppose we make the following changes to the weights of
the edges in Fig. 10.1. What would the resulting MWST be?

* a) Change the weight 10 on edge (1,3) to 25.


--- Page 440 ---
*

424 CHAPTER 10. INTRACTABLE PROBLEMS

Other Notions of NP-completeness

The goal of the study of NP-completeness is really Theorem 10.5, that is,
the identification of problems P for which their presence in the class P
implies P = A’P. The definition of “NP-complete” we have used, which is
often called Karp-completeness because it was first used in a fundamental
paper on the subject by R. Karp, is adequate to capture every problem
that we have reason to believe satisfies Theorem 10.5. However, there
are other, broader notions of NP-completeness that also allow us to claim
Theorem 10.5.

For instance, 5. Cook, in his original paper on the subject, defined
a problem P to be “NP-complete” if, given an oracle for the problem P
, Le, a mechanisin that in one unit of time would answer any question
about membership of a given string in P, it was possible to recognize
any language in VP in polynomial time. This type of NP-completencss
is called Cook-completeness. In a sense, Karp-completeness is the spe-
cial case where you ask only one question of the oracle. However, Cook-
completeness also allows complementation of the answer; e.g., you might
ask the oracle a question and then answer the opposite of what the oracle
says. A consequence of Cook's definition is that the complements of NP-
complete problems would also be NP-complete. Using the more restricted
notion of Karp-completeness, as we do, we are able tu make an important
distinction between the NP-complete problems (in the Karp sense) and
their complements, in Section 11.1.

b) Instead, change the weight on edge (2,4) to 16.

Exercise 10.1.2: If we modify the graph of Fig. 10.1 by adding an edge of
weight 19 between nodes 1 and 4, what is the minimum-weight Hamilton circuit?

Exercise 10.1.3: Suppose that there is an NP-complete problem that has
a deterministic solution that takes time O(n'82"). Note that this function
lies between the polynomials and the exponentials, and is in neither class of
functions. What could we say about the running time of any problem in AP?

Exercise 10.1.4: Consider the graphs whose nodes are grid points in an n-
dimensional cube of side m, that is, the nodes are vectors (i1,%2,-...%,), where
cach i; is in the range 1 to m. There is an edge between two nodes if and only
if they differ by one in exactly one dimension. For instance, the case n = 2 and
m = 2 is a square, n = 3 and m = 2isacube, and n = 2 and m = 3is the graph
shown in Fig. 10.3. Some of these graphs have a Hamilton circuit, and some do
not. For instance, the square obviously does, and the cube does too, although it


--- Page 441 ---
10.1. THE CLASSES P AND NP 425

Figure 10.3: A graph with n = 2; m= 3

may not be obvious; one is (0,0,0), (0,0, 1), (0,1, 1), (0,1,0), (1,1,0), (1,1),
(1,0, 1), (1,0,0), and back to (0,0,0). Figure 10.3 has no Hamilton circuit.

a) Prove that Fig. 10.3 has no Hamilton circuit. Hint: Consider what hap-
pens when a hypothetical Hamilton circuit passes through the central
node. Where can in come from, and where can it go to, without cutting
off one piece of the graph from the Hamilton circuit?

b) For what values of n and m is there a Hamilton circuit?

! Exercise 10.1.5: Suppose we have an encoding of context-free grammars us-
ing some finite alphabet. Consider the following two languages:

1. L, = {(G, A,B) | G is a (coded) CFG, A and B are (coded) variables of
G, and the sets of terminal strings derived from 4 and J are the same}.

2. Lo = {(G1,G2) | G, and Gy are (coded) CFG’s, and £(G1) = L(G2)}.
Answer the following:
* 4) Show that L, is polynomial-time reducible to Lo.

b) Show that £2 is polynomial-time reducible to £1.

* ¢) What do (a) and (b) say about whether or not £, and Lz are NP-
complete?

Exercise 10.1.6: As classes of languages, ? and \’P each have certain closure
properties. Show that P is closed under each of the following operations:

a) Reversal.
* b) Union.
*I c) Concatenation.
!d) Closure (star).

e) Inverse homomorphism.


--- Page 442 ---
426 CHAPTER 10. INTRACTABLE PROBLEMS

* f) Complementation.

Exercise 10.1.7: VP is also closed under each of the operations listed for P
in Exercise 10.1.6, with the (presumed) exception of (f) complementation. It is
not known whether or not A’P is closed under complementation, an issue we
discuss further in Section 11.1. Prove that each of Exercise 10.1.6(a} through
(e) hold for AP.

10.2.) An NP-Complete Problem

We now introduce you to the first NP-complete problem. This problem —
whether a boolean expression is satisfiable — is proved NP-complete by explic-
itly reducing the language of any nondeterministic, polynomial-time TM to the
satisfiability problem.

10.2.1 The Satisfiability Problem

The boolean expressions are built from:

1. Variables whose values are boolean; i.e., they either have the value 1 (true)
or 0 (false).

2. Binary operators A and V, standing for the logical AND and OR of two
expressions.

3. Unary operator = standing for logical negation.

4. Parentheses to group operators and operands, if necessary to alter the
default precedence of operators: — highest, then A, and finally V.

Example 10.6: An example of a boolean expression is x A 7(y V z). The
subexpression y V z is true whenever either variable y or variable z has the
value true, but the subexpression is false whenever both y and z are false. The
larger subexpression ~{y V 2) is true exactly when y V z is false, that is, when
both y and z are false. If either y or z or both are true, then 7(y V z) is false.

Finally, consider the entire expression. Since it is the logical AND of two
subexpressions, it is true exactly when both subexpressions are true. That is,
x A a{y V z) is true exactly when a is true, y is false, and z is false. O

A truth assignment for a given boolean expression E assigns either true or
false to each of the variables mentioned in E. The value of expression E given
a truth assignment T, denoted E(T), is the result of evaluating E with each
variable x replaced by the value 7'(z) (true or false) that T assigns to «.

A truth assignment T' satisfies boolean expression FE if E(T’) = 1; i.e., the
truth assignment T makes expression & true. A boolean expression E is said
to be satisfiable if there exists at least one truth assignment T that satisfies E.


--- Page 443 ---
10.2. AN NP-COMPLETE PROBLEM 427

Example 10.7: The expression x A -(y V 2) of Example 10.6 is satisfiable.
We saw that the truth assignment 7 defined by T(x) = 1, T(y) = 0, and
T(z) = 0 satisfies this expression, because it makes the value of the expression
true (1). We also observed that T' is the only satisfying assignment for this
expression, since the other seven combinations of values for the three variables
give the expression the value false (0).

For another example, consider the expression # = « A (-x V y) A my. We
claim that E is not satisfiable. Since there are only two variables, the number
of truth assignments is 2? = 4, so it is easy for you to try all four assignments
and verify that # has value 0 for all of them. However, we can also argue as
follows. E is true only if all three terms connected by A are true. That means
x must be true (because of the first term) and y must be false (because of the
last term). But under that truth assignment, the middle term 2 V y is false.
Thus, & cannot be made true and is in fact unsatisfiable.

We have seen an example where an expression has exactly onc satisfying
assignment and an example where it has none. There are also many examples
where an expression has more than one satisfying assignment. For a simple
example, consider F = 2 V my. The value of F is 1 for three assignments:

1. A(z) =1;N)=1.
2. Te(x) = 1; Te(y) = 0.
3. T3{2) = 0; T3(y) = 0.

F has value 0 only for the fourth assignment, where z = 0 and y = 1. Thus, F
is satisfiable. O

The satisfiability problem is:
« Given a boolean expression, is it satisfiable?

We shall generally refer to the satisfiability problem as SAT. Stated as a lan-
guage, the problem SAT is the set of (coded) boolean expressions that are
satisfiable. Strings that either are not valid codes for a boolean expression or
that are codes for an unsatisfiable boolean expression are not in SAT.

10.2.2 Representing SAT Instances

The symbols in a boolean expression are A, V, 7, the left and right parentheses,
and symbols representing variables. The satisfiability of an expression does
not depend on the names of the variables, only on whether two occurrences of
variables are the same variable or different variables. Thus, we may assume
that the variables are 2,,22,... , although in examples we shail continue to use
variable names like y or z, a8 well as z’s. We shall also assume that variables are
renamed so we use the lowest possible subscripts for the variables. For instance,
we would not use 25 unless we also used x, through x4 in the same expression.


--- Page 444 ---
428 CHAPTER 10. INTRACTABLE PROBLEMS

Since there are an infinite number of symbols that could in principle, ap-
pear in a boolean expression, we have a familiar problem of having to devise a
code with a fixed, finite alphabet to represent expressions with arbitrarily large
numbers of variables. Only then can we talk about SAT as a “problem,” that
is, aS a language over a fixed alphabet consisting of the codes for those boolean
expressions that are satisfiable. The code we shall use is as follows:

1. The symbols A, ¥, 7, (, and ) are represented by themselves.

2. The variable x; is represented by the symbol + followed by 0’s and 1's
that represent é in binary.

Thus, the alphabet for the SAT problem/language has only eight symbols. All
instances of SAT are strings in this fixed, finite alphabet.

Example 10.8: Consider the expression « A a{y V z) from Example 10.6. Our
first step in coding it is to replace the variables by subscripted z's. Since there
are three variables, we must use %,, ta, and xg. We have freedom regarding
which of a, y, and 2 is replaced by each of the 2,;’s, and to be specific, let. x = 21,
Y = £2, and z= «3. Then the expression becomes 4; A a(#2 V wz). The code
for this expression is:

#1 A a(#10 V 211)
Oo

Notice that the length of a coded boolean expression is approximately the
same as the number of positions in the expression, counting each variable oc-
currence as 1. The reason for the difference is that if the expression has m
positions, it can have O(7n) variables, so variables may take O(log) symbols
to code, Thus, an expression whose length is mm positions can have a code as
long as n = O(mlogm) symbols.

However, the difference hetween m and m log m is surely limited by a poly-
nomial. Thus, as long as we only deal with the issue of whether or not a problem
can be solved in time that is polynomial in its input length, there is no need
to distinguish between the length of an expression’s code and the number of
positions in the expression itsclf.

10.2.3 NP-Completeness of the SAT Problem

We now prove “Cook’s Theorem,” the fact that SAT is NP-complete. To prove
a problem is NP-complete, we need first to show that it is in AYP. Then, we
must show that every language in A’P reduces to the problem in question. In
general, we show the second part by offering a polynomial-time reduction from
some other NP-complete problem, and then invoking Theorem 10.5. But right
now, we don’t know any NP-complete problems to reduce to SAT. Thus, the
only strategy available is to reduce absolutely every problem in A’P to SAT.

Theorem 10.9: (Cook’s Theorem} SAT is NP-complete.


--- Page 445 ---
10.2, AN NP-COMPLETE PROBLEM 429

PROOF: The first part of the proof is showing that SAT is in VP. This part
is easy:

1. Use the nondeterministic ability of an NTM to guess a truth assignment
T for the given expression FE. If the encoded E is of length x, then O(n)
time suffices on a multitape NTM. Note that this NTM has many choices
of move, and may have as many as 2" different ID’s reached at the end of
the guessing process, where each branch represents the guess of a different
truth assignment.

%. Evaluate & for the truth assignment 7. If F(T) = 1, then accept. Note
that this part is deterministic. The fact that other branches of the NTM
may not lead to acceptance has no bearing on the outcome, since if even
one satisfying truth assignment is found, the NTM accepts.

The evaluation can be done easily in O(n”) time on a multitape NTM. Thus, the
entire recognition of SAT by the multitape NTM takes O(n?) time. Converting
to a single-tape NTM may square the amount of time, so O(n‘) time suffices
on a single-tape NTM.

Now, we must prove the hard part: that if ZL is any language in AP, then
there is a polynomial-time reduction of L to SAT. We may assume that there
is some single-tape NTM M and a polynomial p(n) such that M takes no
more than p(n) steps on an input of length 7, along any branch. Further, the
restrictions of Theorem 8.12, which we proved for DTM’s, can be proved in the
same way for NTM’s. Thus, we may assume that Jf never writes a blank, and
never moves its head left of its initial head position.

Thus, if M accepts an input w, and |w| = nm, then there is a sequence of
moves of Af such that:

1. & is the initial ID of Af with input w.
. gay Fs b ay, where & < p(n).

. ag is an ID with an accepting state.

mem Gc bh

. Each a; consists of nonblanks only (except if a; cnds in a state and a
blank), and extends from the initial head position — the leftmost input
symbol — to the right.

Our strategy can be summarized as follows.

a) Each a; can be written as a sequence of symbols XioXin---Xipinj- One
of these symbols is a state, and the others are tape symbols. As always,
we assume that the states and tape symbols are disjoint, so we can tell
which Xj; is the state, and therefore tell where the tape head is. Note
that there is no reason to represent symbols to the right of the first p(m)
symbols on the tape [which with the state makes an ID of length p(n) + 1].
because they cannot influence a move of M if M is guaranteed to halt
after p(7) moves or less.


--- Page 446 ---
430

b)

c

—

CHAPTER 10. INTRACTABLE PROBLEMS

To describe the sequence of ID’s in terms of boolean variables, we create
variable y,;4 to represent the proposition that X;; = A. Here, i and j are
each integers in the range 0 to p(n), and A is either a tape symbol or a
state.

We express the condition that the sequence of ID’s represents acceptance
of an input w by writing a boolean expression that is satisflable if and
only if M accepts w by a sequence of at most p(n) moves. The satisfying
assignment will be the one that “tells the truth” about the ID’s; that is,
yija will be true if and only if X;; = A, To make sure that the polynomial-
time reduction of L(.M) to SAT is correct, we write this expression so that
it says the computation:

it. Starts right. That is, the initial ID is ggw followed by blanks.

ii. Next move is right (i.e., the move correctly follows the rules of the
TM). That is, each subsequent ID follows from the previous by one
of the possible legal moves of M.

iti. Finishes right. That is, there is some ID that is an accepting state.

There are a few details that must be introduced before we can make the
construction of our boolean expression precise.

First, we have specified ID’s to end when the infinite tail of blanks begin.
However, it is more convenient when simulating a polynomial-time com-
putation to think of all ID’s as having the same length, p(n) + 1. Thus,
a tail of blanks may be present im an ID.

Second, it is convenient to assume that all computations continue for
exactly p(m) moves [and therefore have p(n) + 1 ID’s], even if acceptance
occurs earlier. We therefore allow each ID with an accepting state to be
its own successor. That is, if a has an accepting state, we allow a “move”
aka, Thus, we can assume that if there is an accepting computation,
then d,) Will have an accepting ID, and that is all we have to check for
the condition “finishes right.”

Figure 10.4 suggests what a polynomial-time computation of M looks like. The
rows correspond to the sequence of ID's, and the columns are the cells of the
tape that can be used in the computation. Notice that the number of squares
in Fig. 10.4 is (p(n) + 1)’. Also, the number of variables that represent, each
square is finite, depending only on M; it is the sum of the number of states and
tape symbols of M.

Let us now give an algorithm to construct from Mf and w a boolean expres-
sion Ey4,y- The overall form of By, is S AN A F, where S, N, and F are
expressions that say Ad starts, moves, and finishes right.


--- Page 447 ---
10.2. AN NP-COMPLETE PROBLEM 431

ID 0 i _ ... | pln)

ao || Xoo | Xo ee ee ee
ee ee ee

|
int a iL
|
PXivrjei ||

a a

a fT | | Xin jot |

Figure 10.4: Constructing the array of cell/ID facts

Starts Right

Xoo must be the start state go of M, Xo. through Xon must be w (where 7
is the length of w), and the remaining Xo;, must be the blank, B. That is, if
wW = @109°-°G,, then:

S = Yoog A Yola, A Yoran Att A Yona, A Yont1,8 A Yo.n42,8 A+++ A Yo.p(n),B

Surely, given the encoding of M and given w, we can write S in O(p{n)) time
on a second tape of a multitape TM.

Finishes Right

Since we assume that an accepting ID repeats forever, acceptance by M is the
same as finding an accepting state in ayn). Remember that we assume M is
an NTM that, if it accepts, does so within p(n) steps. Thus, F is the OR of
expressions F;, for j = 0,1,...,p(n), where F; says that Xpry),j is an accepting
state. That is, Fj is Yp(a), 3.01 V Ypin) dice Vou Yotri ice? where @),@2,...,&
are all the accepting states of M4. Then,

FaRVEV---¥ Pom


--- Page 448 ---
432 CHAPTER 10. INTRACTABLE PROBLEMS

Notice that each F; uses a constant number of symbols, that depends on
M, but not on the length n of its input w. Thus, F has length O(n). More
importantly, the time to write F, given an encoding of M and the input w is
polynomial in n; actually, F can be written in O(p(n)) time on a multitape
TM.

Next Move is Right

Assuring that the moves of M are correct is by far the most complicated part.
The expression N will be the AND of expressions Nj, for i = 0,1,...,p(n) — 1,
and each N; will be designed to assure that ID a;,, is one of the ID’s that
M allows to follow a;. To begin the explanation of how to write N;, observe
symbol Xi41,; in Fig. 10.4. We can always determine Xj4,,; from:

1. The three symbols above it: X;j-1, Xiz, and X;j41, and

2. If one of these symbols is the state of a;, then the particular choice of
move by the NTM M.

We shall write N; as the A of expressions A,;; V B,;, where j = 0,1,..., p(n).

» Expression A;; says that:

a) The state of a; is at position j {i.e., Xi; is the state), and

b) There is a choice of move of M, where X;; is the state and Xj;41 is
the symbol scanned, such that this move transforms the sequence of
symbols Xi Mig Xi jai into Nini j—1Xi41,j Ait jt Note that. if
ij i$ an accepting state, there is the “choice” of making no move
at all, so all subsequent ID’s are the same as the one that first led
to acceptance.

* Expression B;; says that:

a) The state of a; is sufficiently far away from X;; that it cannot influ-
ence Xj41,; {1.¢., neither Xj,;-1, Xu;, nor X;,;4) is a state).

b) Xigig = Xy-

Bi; is the easier to write. Let 91,42,.-.,@m be the states of M, and let
21, 22,..-,Z, be the tape symbols. Then:

By = (Yag—ijzy V ¥iag—1.22 Vo V ¥ig—1.2,.) A
(Yi5,21 V YigZe Veo V ¥ig,Zn) A
(yigti,21 V ¥ijeize Vi: Voyager.) A

(Yiazi A Yir1gZ1) V (Yi5.Z2 A Yiri5,22) Veo V via. A ¥id1G,2.))


--- Page 449 ---
10.2. AN NP-COMPLETE PROBLEM 433

The first line of By; says that X;,;-1 is one of the tape symbols; the second
line says X;, is one of the tape symbols, and the third line says the same about
Xi,j+1- The final line says that Xij = Xiy1,; by enumerating all the possible
tape symbols Z and saying that either both are Z), or both are Ze, and so on.

There are two important special cases: either j = 0 or 7 = p(n). In one case
there are no variables y;,;-1,z, and in the other, no variables y:,;+1,2. However,
we know the head never moves to the left of its initial position, and we know it
will not have time to get more than p(n) cells to the right of where it started.
Thus, we may eliminate certain terms from Big and B; piny; we leave you to
make the simplification.

Now, let us consider the expressions 4;;. These expressions reflect all possi-
ble relationships among the 2 x 3 rectangle of symbols in the array of Fig. 10.4:
Xij-1s Aaj; Xijtds Xi41,j-13 Xit.js and Neat j+i: An assignment of symbols
to each of these six variables is valid if:

1. Xi; is a state, but X;j;-1 and Xi,j;41 are tape symbols.
2. Exactly one of Xi41,3-1, Ai4i,j, and Xi41,j41 18 & state.

3. There is a move of M that explains how Xj 3-1 XiyjXi,j+1 becomes

Xi4ij—-1Xig1,j M4. jt

There are thus a finite number of assignments of symbols to the six variables
that are valid. Let A;; be the OR of terms, one term for each set of six variables
that form a valid assignment.

For instance, suppose that one move of M comes from the fact that 6(q, A)
contains (p,C,L). Let D be some tape symbol of M. Then one valid assignment
is Xi j—-1 Xi Kip = DgA and Xiptj—1 Xigi gj Mit 1 p41 = pDe. Notice how
this assignment reflects the change in ID that is caused by making this move of
iM. The term that reflects this possibility is

Yig-1,D A Vijg A Yaga A Yeti g—lLep A YitLgD A Yi+1,j4+L0
If, instead, 6(g, A) contains (p, C, R) (i.e., the move is the same, but the head
moves right), then the corresponding valid assignment is Xi,j-1XijXij+1 =
DgA and Xi41 5-1 Xi41,3Xi41,341 = DCp. The term for this valid assignment
is

Yij—1,D A Yaga A Yigtia A Yi41g-1,.D0 A Yitigc A +1 7+

Aj; is the OR of all valid terms. In the special cases j = 0 and j = p(n),

we must make certain modifications to reflect the nonexistence of the variables
yijz for j < 0 or j > p(n), as we did for Aj;. Finally,

N,= (Aw V Big) A (Aa Vv Bis) Act A (Ai p(n) V Bipiny)

and then


--- Page 450 ---
434 CHAPTER 10. INTRACTABLE PROBLEMS

N=NoAN A+++ A Nowy-1

Although A,; and Bj; can be very large if M has many states and/or tape
symbols, their size is actually a constant as far as the length of input w is
concerned; that is, their size is independent of n, the length of w. Thus, the
length of N; is O(p(n)), and the length of N is O{p?(n)). More importantly,
we can write NV on a tape of a multitape TM in an amount of time that is
proportional to its length, and that amount of time is polynomial in n, the
length of w.

Conclusion of the Proof of Cook’s Theorem

Although we have described the construction of the expression
ua =SANAF

as a function of both M and w, the fact is that only the “starts right” part
S that depends on w, and it does so in a simple way (w is on the tape of the
initial ID). The other parts, N and F’, depend on M and on n, the length of w,
only.

Thus, for any NTM M that runs in some polynomial time p(n), we can
devise an algorithm that takes an input w of length n, and produces Euw- The
running time of this algorithm on a multitape, deterministic TM is O(p’ (n)),
and that multitape TM can be converted to a single-tape TM that runs in time
O(p*(n)). The output of this algorithm is a boolean expression Ey, that is
satisfiable if and only if M accepts w within p(n) moves. O

To emphasize the importance of Cook’s Theorem 10.9, let us see how The-
orem 10.5 applies to it. Suppose SAT had a deterministic TM that recognized
its instances in polynomial time, say time q(n). Then every language accepted
by an NTM M that accepted within polynomial time p(n) would be accepted
in deterministic polynomial time by the DTM whose operation is suggested by
Fig. 10.5. The input w to Af is converted to a boolean expression Em. This
expression is fed to the SAT tester, and whatever this tester answers about
fu_w, our algorithm answers about w.

10.2.4. Exercises for Section 10.2

Exercise 10.2.1: How many satisfying truth assignments do the following
boolean expressions have? Which are in SAT?

* a) EA (y V a2) A (2 V 7).

b) (ge Vy) A (Ala Vz) Vv (2A =y)).


--- Page 451 ---
10.3. A RESTRICTED SATISFIABILITY PROBLEM 435

Polynomial-

time SAT
w converter E Mw decide yes
for M
no

Figure 10.5: If SAT is in P, then every language in NP could he shown to be
in P by a DTM designed in this manner

! Exercise 10.2.2: Suppose G is a graph of four nodes: 1, 2, 3, and 4. Let
vyj,forl<i<j<4bea propositional variable that we interpret as saying
“there is an edge between nodes i and j. Any graph on these four nodes can
be represented by a truth assignment. For instance, the graph of Fig. 10.1
is represented by making 214 false and the other five variables true. For any
property of the graph that involves only the existence or nonexistence of edges,
we can express that property as a boolean expression that is true if and only if
the truth assignment to the variables describes a graph that has the property.
Write expressions for the following properties:

* a) G has a Hamilton circuit-
b) G is connected.

c) G contains a clique of size 3, that is, a set of three nodes such that there
is an edge between every two of them (i.e., a triangle in the graph).

d) G contains at least one isolated node, that is, a node with no edges.

10.3. A Restricted Satisfiability Problem

Our plan is to demonstrate a wide variety of problems, such as the TSP problem
mentioned in Section 10.1.4, to be NP-complete. In principle, we do so by
finding polynomial-time reductions from the problem SAT to each problem of
interest. However, there is an important intermediate problem, called “3SAT,”
that is much easier than SAT to reduce to typical problems. 3SAT is still a
problem about satisfiability of boolean expressions, but these expressions have
a very regular form: they are the AND of “clauses,” each of which is the OR
of exactly three variables or negated variables.

In this section we introduce some important terminology about boolean
expressions. We then reduce satisfiability for any expression to satisfiability
for expressions in the normal form for the 35AT problem. It is interesting to
observe that, while every boolean expression E has an equivalent expression
in the normal form of 3SAT, the size of F may be exponential in the size of


--- Page 452 ---
436 CHAPTER 10. INTRACTABLE PROBLEMS

&. Thus, our polynomial-time reduction of SAT to 3SAT must be more subtle
than simple boolean-algebra manipulation. We need to convert each expression
E& in SAT to another expression F in the normal form for 3SAT. Yet F is not
necessarily equivalent to E. We can be sure only that F is satisfiable if and
only if & is.

10.3.1 Normal Forms for Boolean Expressions

The following are three essential definitions: _

e A literal is either a variable, or a negated variable. Examples are x and
ay. To save space, we shall often use an overbar 9 in place of a literal
such as 7y.

© A clause is the logical OR of one or more literals. Examples are x, x V y,
anda V WV z.

e A boolean expression is said to be in conjunctive normal form? or CNF,
if it is the AND of clauses.

To further compress the expressions we write, we shall adopt the alternative
notation in which V is treated as a sum, using the + operator, and A is treated
as a product. For products, we normally use juxtaposition, i.e., no operator,
just as we do for concatenation in regular expressions. It is also then natural
to refer to a clause as a “sum of literals” and a CNF expression as a “product
of clauses.”

Example 10.10: The expression {x V -y) A (aa V z) will be written in our
compressed notation as (xz + #)(% + z). It is in conjunctive normal form, since
it is the AND (product) of the clauses (« + 9) and (% + 2).

Expression (x + yZ)(@+y+2)(¥+2Z) is not in CNF. It is the AND of three
subexpressions, (x + yZ), (2 +y +z), and (¥+2Z). The last two are clauses, but
the first is not; it is the sum of a literal and a product of two literals.

Expression xyz is in CNF. Remember that a clause can have only one literal.
Thus, our expression is the product of three clauses, (x), (y), and (z). 0

An expression is said to be in k-conjunctive normal form (k-CNF) if it is
the product of clauses, each of which is the sum of exactly k distinct literals.
For instance, (z + 9)(y + 2){z + %) is in 2-CNF, because each of its clauses has
exactly two literals.

All of these restrictions on boolean expressions give rise to their own prob-
lems about satisfiability for expressions that meet the restriction. Thus, we
shall speak of the following problems:

e CSAT is the problem: given a boolean expression in CNF, is it satisfiable?

3“Conjunction” is a fancy term for logical AND.


--- Page 453 ---
10.3. A RESTRICTED SATISFIABILITY PROBLEM 437

Handling Bad Input

Each of the problems we have discussed — SAT, CSAT, 3SAT, and so
on — are languages over a fixed, 8-symbol alphabet, whose strings we
sometimes may interpret as boolean expressions. A string that is not
interpretable as an expression cannot be in the language SAT. Likewise,
when we consider expressions of restricted form, a string that is a well-
formed boolean expression, but not an expression of the required form,
is never in the language. Thus, an algorithm that decides the CSAT
problem, for example, will say “no” if it is given a boolean expression that
is satishable, but not in CNF.

» kSAT is the problem: given a boolean expression in k-CNF, is it satisii-
able?

We shall see that CSAT, 3SAT, and ASAT for all & higher than 3 are NP-
complete. However, there are linear-time algorithms for ISAT and 2SAT.

10.3.2 Converting Expressions to CNF

Two boolean expressions are said to be equivalent if they have the same result
on any truth assignment to their variables. If two expressions are equivalent,
then surely either both are satisfiable or neither is. Thus, converting arbitrary
expressions to equivalent CNF expressions is a promising approach to devel-
oping a polynomial-time reduction from SAT to CSAT. That reduction would
show CSAT to be NP-complete.

However, things are not quite so simple. While we can convert any expres-
sion to CNF, the conversion can take more than polynomial time. In particular,
it may exponentiate the length of the expression, and thus surely take expo-
nential time to generate the output.

Fortunately, conversion of an arbitrary boolean expression to an expression
in CNF is only one way that we might reduce SAT to CSAT, and thus prove
CSAT is NP-complete. All we have to do is take a SAT instance & and convert
it to a CSAT instance F such that F is satisfiable if and only if E is. It is not
necessary that E and F be equivalent. It is not even necessary for E and F to
have the same set of variables, and in fact, generally F will have a superset of
the variables of E.

The reduction of SAT to CSAT will consist of two parts. First, we push ail
—’s down the expression tree so that the only negations are of variables; i.e., the
boolean expression becomes an AND and OR of literals. This transformation
produces an equivalent expression and takes time that is at most quadratic
in the size of the expression. On a conventional computer, with a carefully
designed data structure, it takes only linear time.


--- Page 454 ---
438 CHAPTER 10. INTRACTABLE PROBLEMS

Expression

a((n(@+y))(E+y) start
a(n(z+y)) +7 +y) | (1)
e+y+A(E+y) (3)
e+yt (A(z))9 (2)
etytsy (3)

Figure 10.6: Pushing ~’s down the expression tree so they appear only in literals

The second step is to write an expression that is the AND and OR of literals
as a product of clauses; i.e., to put it in CNF. By introducing new variables,
we are able to perform this transformation in time that is a polynomial in the
size of the given expression. The new expression F will not be equivalent to
the old expression E, in general. However, F will be satisfiable if and only if F
is. More specifically, if T is a truth assignment that makes F& true, then there
is an extension of T, say S, that makes F true; we say S is an extension of T if
S assigns the same value as T to each variable that T assigns, but S' may also
assign a value to variables that 7’ does not mention.

Our first step is to push —’s below A’s and V’s. The rules we need are:

1. -(£ A F) > A(E) V A(F). This rule, one of DeMorgan’s laws, allows us
to push ~ below A. Note that as a side-effect, the A is changed to an V.

2. “(EV F) => 7A{£) A A(F). The other “DeMorgan’s law” pushes — below
Vv. The V is changed to A as a side-effect.

3. a(-(E)) = E. This law of double negation cancels a pair of 7's that
apply to the same expression.

Example 10.11: Consider the expression E = a((r@ + y))(E+ v)). Notice

that we have used a mixture of our two notations, with the — operator used
explicitly when the expression to be negated is more than a single variable.
Figure 10.6 shows the steps in which expression & has all its s’s pushed down
until they become parts of literals.

The final expression is equivalent to the original and is an OR-and-AND
expression of literals. It may be further simplified to the expression x + y, but
that simplification is not essential to our claim that every expression can be
rewritten so the 7’s appear only in literals. O

Theorem 10.12: Every boolean expression E is equivalent to an expression
F in which the only negations occur in literals; ie., they apply directly to
variables. Moreover, the length of F is linear in the number of symbols of E,
and F can be constructed from EZ in polynomial time.


--- Page 455 ---
10.3. A RESTRICTED SATISFIABILITY PROBLEM 439

PROOF: The proof is an induction on the number of operators (A, V, and
a) in &. We show that there is an equivalent expression F with —’s only in
literals. Additionally, if E has n > 1 operators, then F has no more than 2n —1
operators.

Since & need not have more than one pair of parentheses per operator, and
the number of variables in an expression cannot exceed the number of operators
by more than one, we conclude that the length of F is linearly proportional to
the length of #. More importantly, we shall see that, because the construction
of F is quite simple, the time it takes to construct F is proportional to its
length, and therefore proportional to the length of E.

BASIS: If EF has one operator, it must be of the form -2, z V y, or x A y, for
variables z and y. In each case, F is already in the required form, so F = EF
serves. Note that since FE and F each have one operator, the relationship “F
has at most twice the number of operators of #, minus 1” holds.

INDUCTION: Suppose the statement is true for all expressions with fewer op-
erators than EF. If the highest operator of F is not =, then FE must be of the
form F, V E2 or Ey A Eo. In either case, the inductive hypothesis applies to
F, and E>; it says that there are equivalent expressions F; and Fo, respectively,
in which ali -’s occur in literals only. Then F = F, v Fy or F = (Fi) A (F4)
serves as a suitable equivalent for FE. Let Fi and Fe have a and 6 operators,
respectively. Then F has a +6+ 1 operators. By the inductive hypothesis, Fy
and Fy have at most 2a —1 and 2b— 1 operators, respectively. Thus, F has at
most 2a +2b—1 operators, which is no more than 2(a +6+1)—1, or twice the
number of operators of £, minus 1.

Now, consider the case where £ is of the form —#,. There are three cases,
depending on what the top operator of Zi is. Note that #, must have an
operator, or & is really a basis case.

1. By = 48g. Then by the law of double negation, B = (£2) is equivalent
to Ey. Since Ey has fewer operators than £, the inductive hypothesis
applies. We can find an equivalent F for E2 in which the only —’s are in
literals. F serves for EF as well. Since the number of operators of F is at
most twice the number in Hy minus 1, it is surely no more than twice the
number of operators in & minus I.

2. B, = Ey V Es. By DeMorgan’s law, E = 7{Ey V Es) is equivalent
0 (-(E2)) A (A(B3)). Both +(#,) and -(£3) have fewer operators
than E, so by the inductive hypothesis they have equivalents f, and F3
that have -’s only in literals. Then F = (Fz) A (Fs) serves as such an
equivalent for £. We also claim that the number of operators in F is not
too great. Let Z, and Es; have a and & operators respectively. Then E& has
a+6+2 operators. Since =(E2) and (£3) have a+ 1 and 6+ 1 operators,
respectively, and Fy and F3 are constructed from these expressions, by the
inductive hypothesis we know that £ and F3 have at most 2(a+1)—-1
and 2(6+ 1})-—1 operators, respectively. Thus, F’ has 24+ 26+ 3 operators


--- Page 456 ---
446 CHAPTER 10. INTRACTABLE PROBLEMS

Descriptions of Algorithms

While formally, the running time of a reduction is the time it takes to
execute on a single-tape Turing machine, these algorithms are needlessly
complex. We know that the sets of problems that can be solved on con-
ventional computers, on multitape TM’s and on single tape TM’s in some
polynomial time are the same, although the degrees of the polynomials
may differ. Thus, as we describe some fairly sophisticated algorithms that
are needed to reduce one NP-complete problem to another, let us agree
that times will be measured by efficient implementations on a conventional
computer. That understanding will allow us to avoid details regarding ma-
nipulation of tapes and will let us emphasize the important algorithmic
ideas.

at most. This number is exactly twice the number of operators of E,
minus 1,

3. &, = Ey A Ey. This argument, using the second of DeMorgan’s laws, is
essentially the same as (2).

10.3.3 NP-Completeness of CSAT

Now, we need to take an expression & that is the AND and OR of literals and
convert it to CNF. As we mentioned, in order to produce in polynomial time
an expression # from & that is satisfiable if and only if F is satisfiable, we
must forgo an equivalence-preserving transformation, and introduce some new
variabies for F that do not appear in HE. We shall introduce this “trick” in the
proof of the theorem that CSAT is NP-complete, and then give an example of
the trick to make the construction clearer.

Theorem 10.13: CSAT is NP-complete.

PROOF: We show how to reduce SAT to CSAT in polynomial time. First,
use the method of Theorem 10.12 to convert a given instance of SAT to an
expression £ whose ~’s are only in literals. We then show how to convert E to
a CNF expression F in polynomial time and show that F is satisfiable if and
only if F is. The construction of F is by an induction on the length of E. The
particular property that F has is somewhat more than we need. Precisely, we
show by induction on the number of symbol occurrences (“length”) E that:

¢ There is a constant ¢ such that if F is a boolean expression of length n
with —’s appearing only in literals, then there is an expression F such
that:


--- Page 457 ---
10.3. A RESTRICTED SATISFIABILITY PROBLEM 44]

a) F is in CNF, and consists of at most n clauses.
b) F is constructible from E in time at most ¢|E|*.

c) A truth assignment T for H makes F true if and only if there exists
an extension $ of T that makes F true.

BASIS: If E consists of one or two symbols, then it is a literal. A literal is a
clause, so # is already in CNF.

INDUCTION: Assume that every expression shorter than & can be converted
to a product of clauses, and that this conversion takes at most en? time on an
expression of length n. There are two cases, depending on the top-level operator
of £.

Case 1: E = E, A E. By the inductive hypothesis, there are expressions
F, and Fy derived from E, and £2, respectively, in CNF. All and only the
satisfying assignments for E; can be extended to a satisfying assignment for
F,, and similarly for E. and F,. Without loss of generality, we may assume
that the variables of F, and F) are disjoint, except for those variables that
appear in £; i.e., if we have to introduce variables into Fy and/or F), use
distinct variables.

Let F = F, A Fy. Evidently F, A Fp is a CNF expression if #) and Ff are.
We must show that a truth assignment T for E can be extended to a satisfying
assignment for F if and only if T satisfies £.

(If) Suppose T satisfies EB. Let T, be T restricted so it applies only to the
variables that appear in E,, and let T2 be the same for Ez. Then by the
inductive hypothesis, T; and T2 can be extended to assignments 5, and 2 that
satisfy F, and Fo, respectively. Let S agree with 5S; and S2 on each of the
variables they define. Note that, since the only variables F, and F2 have in
common are the variables of E, and S; and Sj must agree on those variables if
both are defined, it is always possible to construct S. But S is then an extension
of T that satisfies F.

(Only-if) Conversely, suppose that T has an extension S that satisfies F’. Let
T, (resp., T2) be T restricted to the variables of Ey (resp., Ez). Let 5 restricted
to the variables of F, (resp., F2) be 5, (resp., Se). Then 5S; is an extension
of T,, and S2 is an extension of T2. Because F' is the AND of Fy and Fy, it
must be that S; satisfies F,, and S» satisfies F,. By the inductive hypothesis,
T, (resp., Tz) must satisfy B, (resp., H2). Thus, T satisfies B.

Case 2: E = B, V E>. As in case 1, we invoke the inductive hypothesis to
assert that there are CNF expressions F and F) with the properties:
1. A truth assignment for E, (resp., £2) satisfies E, (resp., Be), if and only
if it can be extended to a satisfying assignment for F; (resp., F2).

9. The variables of F, and F are disjoint, except for those variables that
appear in &.


--- Page 458 ---
442 CHAPTER 10. INTRACTABLE PROBLEMS

3. FF, and Fy are in CNF.

We cannot simply take the OR of F, and F2 to construct the desired F,
because the resulting expression would not be in CNF. However, a more com-
plicated construction, which takes advantage of the fact that we only want to
preserve satisfiability, rather than equivalence, will work. Suppose

FlL=nAgeN-'Agp

and Fy = hi A hz A--- A Ag, where the g’s and h’s are clauses. Introduce a
new variable y, and let

FPeytalatyt ga At gpAG+HMAA G+) A AG+hg)

We must prove that a truth assignment T for £ satisfies E if and only if T can
be extended to a truth assignment S that satisfies F.

(Only-if) Assume T' satisfies E. As in Case 1, let T, (resp., T2) be T restricted
to the variables of FE, (resp., Ez). Since E = FE, V Eo, either T satisfies Ey or
T satisfies E2. Let us assume 7 satisfies E,. Then T,, which is T restricted
to the variables of £1, can be extended to 5), which satisfies F,. Construct an
extension S for 7’, as follows; S will satisfy the expression F defined above:

1. For all variables z in F,, S(x) = S,(2).

2. S(y} = 0. This choice makes all the clauses of F that are derived from Fy
true.

3. For all variables z that are in F) but not in F,, S(x) is T(z) if the latter
is defined, and otherwise may be 0 or 1, abribtrarily.

Then S$ makes all the clauses derived from the g’s true because of rule 1. $
makes all the clauses derived from the h’s true by rule 2— the truth assignment
for y. Thus, S satisfies F.

If Z does not satisfy E,, but satisfies H., then the argument is the same,
except S(y) = 1 in rule 2. Also, S(z) must agree with S2(x) whenever S2(x) is
defined, but S(x) for variables appearing only in S, is arbitrary. We conclude
that S satisfies F in this case also.

(If) Suppose that truth assignment T for E is extended to truth assignment 5
for F’, and S satisfies F. There are two cases, depending on what truth-value
is assigned to y. First suppose that S{y) = 0. Then all the clauses of F derived
from the ’’s are true. However, y is no help for the clauses of the form (y+ gs)
that are derived from the g’s, which means that. S must make true each of the
gi’s themselves; in essence, 5 makes F, true.

More precisely, let 5S; be S restricted to the variables of Fi. Then Sj) satisfies
fF. By the inductive hypothesis, T,, which is T restricted to the variables of
&,, must satisfy £,. The reason is that S$; is an extension of T,. Since T\
satisfies F|, T must satisfy EB, which is B, V Eo.


--- Page 459 ---
10.3. A RESTRICTED SATISFIABILITY PROBLEM 443

We must also consider the case that S(y} = 1, but this case is symmetric
to what we have just seen, and we leave it to the reader. We conclude that T
satisfies EF whenever S satisfies P.

Now, we must show that the time to construct F from E is at most quadratic,
in n, the length of #. Regardless of which case applies, the splitting apart of #
into £, and £2, and construction of F from Fy and F each take time that is
linear in the size of E. Let dn be an upper bound on the time to construct Fy
and Eo from £ plus the time to construct F from F, and Fa, in either case 1
or case 2. Then there is a recurrence equation for T(n), the time to construct
F from any E of length n; its form is:

T(1) =T(2) < e for some constant e
T(n) < dn + emaxpcien-1 (TH) + Tin —1- #)) for n > 3

Where c is a constant as yet to be determined, such that we can show T(n)} <
en?. The basis rule for T(1) and T(2) simply says that if £ is a single symbol
or a pair of symbols, then we need no recursion because & can only be a single
literal, and the entire process takes some amount of time e. The recursive rule
uses the fact that if & is composed of subexpressions B, and E» connected
by an operator A or V, and £, is of length i, then #2 is of length n —#— 1.
Moreover, the entire conversion of E to F consists of the two simple steps —
changing E to F, and E, and changing F, and Fy) to F — that we know take
time at most dn, plus the two recursive conversions of £, to Fy and EH» to Fo.

We need to show by induction on x that there is a constant c such that for
all n, T{n) < cn’.

BASIS: For n = 1, we just need to pick c at least as large as e.

INDUCTION: Assume the statement for lengths less than n. Then T'(i) < ci?
and T(n —i—1) < e(n —i— 1). Thus,

TG) +T(n—i-1) <n? -2i(n—i)-—2(n-7) +1 (10.1)

Since n > 3, and O<i<n—1, 2i(m—i) is at least n, and 2(n —{) is at least 2.
Thus, the right side of (10.1) is less than n? — n, for any i in the allowed range.
The recursive rule in the definition of T(n) thus says T(n) < dn + en? ~ en. If
we pick ¢ > d, we may infer that T(n) < en? holds for n, which concludes the
induction. Thus, the construction of F from F takes time O(n’). O

Example 10.14: Let us show how the construction of Theorem 10.13 applies
to a simple expression: E = sy + Z(y +2). Figure 10.7 shows the parse of this
expression. Attached to each node is the CNF expression constructed for the
expression represented by that node.

The leaves correspond to the literals, and for each literal, the CNF expres-
sion is one clause consisting of that literal alone. For instance, we see that
the leaf labeled 7 has an associated CNF expression (7). The parentheses are


--- Page 460 ---
444 CHAPTER 10. INTRACTABLE PROBLEMS

(u+x (uty (UTX )(Utvey) (GUt0V4z)

(x)G) (vey (Vez)
Con) 7 790 #2)
(x) (y) (x) é

(y ) (z)

Figure 10.7: Transforming a boolean expression into CNF

unnecessary, but we put them in CNF expressions to help remind you that we
are talking about a product of clauses.

For an AND node, the construction of a CNF expression is simply to take the
product (AND) of all the clauses for the two subexpressions. Thus, for instance,
the node for the subexpression Z(y + 2) has an associated CNF expression that
is the product of the one clause for F, namely (x), and the two clauses for y+,
namely (u+y)(0+z).4

For an OR, node, we must introduce a new variable. We add it to all the
clauses for the left operand, and we add its negation to the clauses for the right
operand. For instance, consider the root node in Fig. 10.7. It is the OR of
expressions z¥ and Z{y + z}, whose CNF expressions have been determined to
be (x)(y) and (&)(v + y)(@ + 2), respectively. We introduce a new variable u,
which is added without negation to the first group of clauses and negated in
the second group. The result is

FP=(utzjutPw+D@+u+y@+s+2)

Theorem 10.13 tells us that any truth assignment T that satisfies E can be
extended to a truth assignment 5 that satisfies F. For instance, the assignment
T(x) = 0, T(y) = 1, and T(z) = 1 satisfies E. We can extend T to S by adding
S(w) = 1 and S(v) = 0 to the required S(x) = 0, S{y) = 1, and S(z) = 1 that
we get from T. You may check that S satisfies F.

Notice that in choosing S, we were required to pick S(u) = 1, because T
makes only the second part of Z, that is (y+ z), true. Thus, we need S(u) = 1

41n this special case, where the subexpression y+ z is already a clause, we did not have to
perform the general construction for the OR of expressions, and could have produced (y +z)
as the product of clauses equivalent to y4+2z. However, in this example, we stick to the general
Tules.


--- Page 461 ---
10.3. A RESTRICTED SATISFIABILITY PROBLEM 445

to make true the clauses (u+x)(u+ 9), which come from the first part of F.
However, we could pick either value for v, because in the subexpression y + 2,
both sides of the OR are true according toT. OU

10.3.4 NP-Completeness of 3SAT

Now, we show an even smailer class of boolean expressions with an NP-complete
satisfiability problem. Recall the problem 3SAT is:

@ Given a boolean expression £ that is the product of clauses, each of which
is the sum of three distinct literals, is & satisfiable’

Although the 3-CNF expressions are a small fraction of the CNF expressions,
they are complex enough to make their satisfiability test NP-complete, as the
next theorem shows.

Theorem 10.15: 3SAT is NP-complete.

PROOF: Evidently 3SAT is in A’P, since SAT is in WP. To prove NP-
completeness, we shall reduce CSAT to 3SAT. The reduction is as follows.
Given a CNF expression E = e1 A eo A--: A ex, we replace each clause e; a8
follows, to create a new expression F. The time taken to construct F is linear
in the length of FE, and we shali see that a truth assignment satisfies F if and
only if it can be extended to a satisfying truth assignment for F,

1. If e; is a singie literal, say (2),> Introduce two new variables and wv.
Replace (x) by the four clauses (c+ u+v)(e+utd)(@+E+ v\(e+Et+D).
Since u and » appear in all combinations, the ouly way to satisfy all four
clauses is to make x true. Thus, all and only the satisfying assignments
for E can be extended to a satisfying assignment. for F.

2. Suppose e; is the sum of two literals, (2 + y). Introduce a new variable z,
and replace e; by the product of two clauses (x + y+ z)(e +y +7). As in
case 1, the only way to satisfy both clauses is to satisfy (x + y).

3. If e; is the sum of three literals, it is already in the form required for
3-CNF, so we leave e; in the expression F' being constructed.

4. Suppose e; — (21 +22+---+2m) for some m > 4. Introduce new variables
#1, Y2)-+-,¥m-—3 and replace e; by the product of clauses

(x, +22 + yi)(ea +H + yo) (as + Fe + a) (10.2)
(@m—2 + mat Ym—3)(2m-1 + 2m + Ym—3)
An assignment T that satisfies E must make at least one literal of 2; true;
say it makes 2; true (recali 2; could be a variable or a negated variable).

'For convenience, we shall talk of literals as if they were unnegated variables, like 2.
However, the constructions apply equally well if some or all of the literals are negated, like 2.


--- Page 462 ---
—

446 CHAPTER 10. INTRACTABLE PROBLEMS

Then, if we make y,yo,.-.,%;-2 true and make y;_1,4;,.--,Y¥m—3 false,
we satisfy all the clauses of (10.2). Thus, T may be extended to satisfy
these clauses. Conversely, if T makes all the «’s false, it is not possible to
extend T to make (10.2) true. The reason is that there are m — 2 clauses,
and each of the m — 3 y’s can only make one clause true, regardless of
whether it is true or false.

We have thus shown how to reduce each instance E of CSAT to an instance
F of 3SAT, such that F is satisfiable if and only if FE is satisfiable. The con-
struction evidently requires time that is linear in the length of £, because none
of the four cases above expand a clause by more than a factor 32/3 (that is the
ratio of symbol counts in case 1), and it is easy to calculate the needed sym-
bois of F in time proportional to the number of those symbols. Since CSAT is
NP-complete, it follows that 3-SAT is likewise NP-complete. O

10.3.5 Exercises for Section 10.3
Exercise 10.3.1: Put the following boolean expressions into 3-CNF:
* al oy + Ez.

b) wayz tut.

c} way + Fun.

Exercise 10.3.2: The problem {TA-SAT is defined as follows: Given a bool-
ean expression &, does # have at ieast four satisfying truth assignments. Show
that 4TA-SAT is NP-complete.

Exercise 10.3.3: In this exercise, we shall define a family of 3-CNF expres-
sions. The expression E,, has n variables, r,,29,...,2,. For each set of three
distinct integers between lL and n, E, has clauses (z1+22+23) and (F1+=9+%3).
Is Ey, satisfiable for:

*ta) n= 4?
Nob) n= 5?

Exercise 10.3.4: Give a polynomial-time algorithm to soive the problem
2SAT, i.e., satisfiability for CNF boolean expressions with only two literals
per clause. Hiné: If one of two literals in a clause is false, the other is forced to
be true. Start with an assumption about the truth of one variabie, and chase
down all the consequences for other variables.


--- Page 463 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 447

10.4 Additional NP-Complete Problems

We shall now give you a small sample of the process whereby one NP-complete
problem leads to proofs that other problems are also NP-complete. This process
of discovering new NP-complete problems has two important effects:

* When we discover a problem to be NP-complete, it tells us that there
is little chance an efficient algorithm can be developed to solve it. We
are encouraged to look for heuristics, partial solutions, approximations,
or other ways to avoid attacking the problem head-on. Moreover, we can
do so with confidence that we are not just “missing the trick.”

e Each time we add a new NP-complete problem P to the list, we re-enforce
the idea that all NP-complete problems require exponential time. The
effort that has undoubtedly gone into finding a polynomial-time algorithm
for problem P was, unknowingly, effort devoted to showing P = AP. It
is the accumulated weight of the unsuccessful attempts by many skilled
scientists and mathematicians to show something that is tantamount to
P = NP that ultimately convinces us that it is very unlikely that P =
NP, but rather that all the NP-complete problems require exponential
time.

In this section, we meet several NP-complete problems involving graphs.
These problems are among those graph problems most commonly used in the
solution to questions of practical importance. We shall talk about the Traveling
Salesman problem (TSP), which we met earlier in Section 10.1.4. We shall show
that a simpler, and also important version, called the Hamilton-Circuit problem
(HC), is NP-complete, thus showing that the more general TSP is NP-complete.
We introduce several other problems involving “covering,” of graphs, such as
the “node-cover problem,” which asks us to find the smallest set of nodes that
“cover” all the edges, in the sense that at least one end of every edge is in the
selected set.

10.4.1 Describing NP-complete Problems

As we introduce new NP-complete problems, we shall use a stylized form of
definition, as follows:

1. The name of the problem, and usually an abbreviation, like 38AT or TSP.
2. The input to the problem: what is represented, and how.

3. The output desired: under what circumstances should the output be
“yes”?

4. The problem from which a reduction is made to prove the problem NP-
complete.


--- Page 464 ---
448 CHAPTER 10. INTRACTABLE PROBLEMS

Example 10.16: Here is how the description of the problem 3S5AT and its
proof of NP-completeness might look:

PROBLEM: Satisfiability for 3-CNF expressions (3SAT),
INPUT: A boolean expression in 3-CNF.
OUTPUT: “Yes” if and only if the expression is satisfiable.

REDUCTION FROM: CSAT. OG

10.4.2. The Problem of Independent Sets

Let G be an undirected graph. We say a subset J of the nodes of G is an inde-
pendent set if no two nodes of I are connected by an edge of G. An independent
set is mazimal if it is as large (has as many nodes) as any independent set for
the same graph.

Example 10.17: In the graph of Fig. 10.1 (See Section 10.1.2), {1,4} is a
maximal independent set. It is the only set of size two that is independent,
because there is an edge between any other pair of nodes. Thus, no set of size
three or more is independent; for instance, {1, 2, 4} is not independent because
there is an edge between 1 and 2. Thus, {1,4} is a maximal independent set. In
fact, it is the only maximal independent set for this graph, although in general
a graph may have many maximal independent sets. As another example, {1}
is an independent set for this graph, but not maximal. 0

In combinatorial optimization, the maximal-independent-set problem is usu-
ally stated as: given a graph, find a maximal independent set. However, as with
all problems in the theory of intractable problems, we need to state our problem
in yes/no terms. Thus, we need to introduce a lower bound into the statement
of the problem, and we phrase the question as whether a given graph has an
independent set at least as large as the bound. The formal definition of the
maximal-independent-set problem is:

PROBLEM: Independent Set (IS).

INPUT: A graph G and a lower bound k, which must be between 1 and the
number of nodes of G.

OUTPUT: “Yes” if and only if G has an independent set of & nodes.
REDUCTION FROM: 3SAT.

We must prove IS to be NP-complete by a polynomial-time reduction from
35AT, as promised. That reduction is in the next theorem.

Theorem 10.18: The independent-set problem is NP-complete.


--- Page 465 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 449

PROOF: First, it is easy to see that 1S is in. WP. Given a graph G anda bound
k, guess & nodes and check that they are independent.

Now, let us show how to perform the reduction of 3SAT to IS. Let B=
(e1)(€2) +++ (€m) be a 3-CNF expression. We construct from & a graph G with
3m nodes, which we shall give the names [f, jj, where 1 <i < mand j= 1, 2,
or 3. The node [i,j] represents the jth literal in the clause e;. Figure 10.8 is
an example of a graph G, based on the 3-CNF expression

(x) + 2 + 23)(Ei + 2 + 4)(H2 + 23 + 25)(E3 + Fa + Fs)

The columns represent the clauses; we shall explain shortly why the edges are
as they are.

Figure 10.8: Construction of an independent set from a satisfiable boolean
expression in 3-CNF

The “trick” behind the construction of G is to use edges to force any inde-
pendent set with m nodes to represent a way to satisfy the expression E. There
are two key ideas.

1. We want to make sure that only one node corresponding to a given clause
can be chosen. We do so by putting edges between all pairs of nodes
in a column, ie., we create the edges ((é,1], [i,2]), ((é1), [3)), and
({é, 2], [#,3]), for all ¢, as in Fig. 10.8.

2. We must prevent nodes from being chosen for the independent set if they
represent literals that are complementary. Thus, if there are two nodes
[i171] and [2, j2], such that one of them represents a variable x, and the
other represents %, we place an edge between these two nodes. Thus, it is
not possible to choose both of these nodes for an independent set.


--- Page 466 ---
450 CHAPTER 10. INTRACTABLE PROBLEMS

Are Yes-No Problems Easier?

We might worry that a yes/no version of a problem is easier than the
optimization version. For instance, it might be hard to find a largest
independent set, but given a small bound &, it might be easy to verify
that there is an independent set of size k. While true, it is also the case
that we might be given a constant k that is exactly largest size for which
an independent set exists. If so, then solving the yes/no version requires
us to find a maximal independent set.

In fact, for all the common problems that are NP-complete, their
yes/no versions and optimization versions are equivalent in complexity, at
least to within a polynomial. Typically, as in the case of IS, if we had
4 polynomial-time algorithm to find maximal independent sets, then we
could solve the yes/no problem by finding a maximal independent set,
and seeing if it was at least as large as the limit &. Since we shall show
the yes/no version is NP-complete, the optimization version must be in-
tractable as well.

The comparison can also be made the other way. Suppose we had a
polynomial-time algorithm for the yes/no problem IS. If the graph has n
nodes, the size of the maximal independent set is between 1 and n. By
running IS with all bounds between 1 and n, we can surely find the size
of a maximal independent set (although not necessarily the set itself) in
n times the amount of time it takes to solve IS once. In fact, by using
binary search, we need only a log, n factor in the running time.

The bound & for the graph G' constructed by these two rules is m.

It is not hard to see how graph G and bound & can be constructed from
expression # in time that is proportional to the square of the length of FE, so
the conversion of & to G is a polynomial-time reduction. We must show that
it correctly reduces 3SAT to IS. That is:

¢ F is satisfiable if and only if G has an independent set of size m.

(If) First, observe that an independent set may not include two nodes from
the same clause, [é, 71] and [2,32] for some j, 4 jo. The reason is that there
are edges between each pair of such nodes, as we observe from the columns in
Fig. 10.8. Thus, if there is an independent set of size m., this set must include
exactly one node from each clause.

Moreover, the independent set may not include nodes that correspond to
both a variable x and its negation %. The reason is that all pairs of such nodes
also have an edge between them. Thus, the independent set J of size m yields
a satisfying truth assignment 7 for EF as follows. If a node corresponding to a
variable z is in I, then make T(z) = 1; if a node corresponding to a negated


--- Page 467 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 451

variable Z is in J, then choose T(z) = 0. If there is no node in J that corresponds
to either x or Z, then pick T(x) arbitrarily. Note that item (2) above explains
why there cannot be a contradiction, with nodes corresponding to both z and
Zin f.

We claim that T' satisfies E. The reason is that each clause of & has the node
corresponding to one of its literals in J, and T is chosen so that literal is made
true by J. Thus, when an independent set of size m exists, £ is satisfiable.

(Only-if) Now suppose E is satisfied by some truth assignment, say T. Since T
makes each clause of E’ true, we can identify one literal from each clause that
T makes true. For some clauses, we may have a choice of two or three of the
literals, and if so, pick one of them arbitrarily. Construct a set of m nodes I by
picking the node corresponding to the selected literal from each clause.

We claim J is an independent set. The edges between nodes that come from
the same clause (the columns in Fig. 10.8) cannot have both ends in J, because
we pick only one node from each clause. An edge connecting a variable and its
negation cannot have both ends in J, because we selected for I only nodes that
correspond to literals made true by the truth assignment T- Of course J will
make one of z and = true, but never both. We conclude that if F is satisfiable,
then G has an independent set of size m.

Thus, there is a polynomial time reduction from 3SAT to IS. Since 3SAT is
known to be NP-complete, so is IS by Theorem 10.5. O

Example 10.19: Let us see how the construction of Theorem 10.18 works for
the case where

E = (x1 + £2 + 23)(By + fy + 24) (Fe + 23 + 25)(B3 + Ea + Fs)

We already saw the graph obtained from this expression in Fig. 10.8. The
nodes are in four columns corresponding to the four clauses. We have shown
for each node not only its name (a pair of integers), but the literal to which
it corresponds. Notice how there are edges between each pair of nodes in a
column, which corresponds to the literals of one clause. There are also edges
between each pair of nodes that corresponds to a variable and its complement.
For instance, the node [3,1], which corresponds to Te, has edges to the two
nodes, [1,2] and [2,2], each of which corresponds to an occurrence of 2.

We have selected, by boldface outline, a set I of four nodes, one from each
column. These evidently form an independent set. Since their four literals are
"1, Zo, £3, and Z4, we can construct from them a truth assignment T that has
T(21) = 1, T(za) = 1, T(ws) = 1, and T(aq) = 0. There must also be an
assignment for 25, but we may pick that arbitrarily, say T(zs} = 0. Now T
satisfies E, and the set of nodes J indicates a literal from each clause that is
made trueby 7. O


--- Page 468 ---
452 CHAPTER 10. INTRACTABLE PROBLEMS

What are Independent Sets Good For?

It is not the purpose of this book to cover applications of the problems we
prove NP-complete. However, the selection of problems in Section 10.4 was
taken from a fundamental paper on NP-completeness by R. Karp, where
he examined the most important problems from the field of Operations
Research and showed a good number of them to be NP-complete. Thus,
there is ample evidence available of “real” problems that are solved using
these abstract problems.

As an example, we could use a good algorithm for finding large inde-
pendent sets to schedule final exams. Let the nodes of the graph be the
classes, and place an edge between two nodes if one or more students are
taking both those classes, and therefore their finals could not be scheduled
for the same time. If we find a maximal independent set, then we can
schedule all those classes for finals at the same time, sure that no student
will have a conflict.

10.4.3 The Node-Cover Problem

Another important class of combinatorial optimization problems involves “cov-
ering” of a graph. For instance, an edge covering is a set of edges such that
every node in the graph is an end of at least one edge in the set. An edge
covering is minimal if it has as few edges as any edge covering for the same
graph. The problem of deciding whether a graph has an edge covering with k
edges is NP-complete, although we shall not prove it here.

We shall prove NP-complete the problem of node covering. A node cover
of a graph is a set of nodes such that each edge has at least one of its ends at
a node of the set. A node cover is minimal if it has as few nodes as any node
cover for the given graph.

Node covers and independent sets are closely related. In fact, the comple-
ment of an independent set is a node cover, and vice-versa. Thus, if we state
the yes/no version of the node-cover problem (NC) property, a reduction from
IS is very simple.

PROBLEM: The Node-Cover Problem (NC).

INPUT: A graph G and an upper limit k, which must be between 0 and one
less than the number of nodes of G.

OUTPUT: “Yes” if and only if G has a node cover with k or fewer nodes.

REDUCTION FROM: Independent Set,.

Theorem 10.20: The node-cover problem is NP-complete.


--- Page 469 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 453

PROOF: Evidently, NC is in VP. Guess a set of & nodes, and check that each
edge of G has at least one end in the set.

To complete the proof, we shall reduce IS to NC. The idea, which is suggested
by Fig. 10.8, is that the complement of an independent set is a node cover. For
instance, the set of nodes that do not have boldface outlines in Fig. 10.8 form
a node cover. Since the boldface nodes are in fact a maximal independent set,
the other nodes form a minimal node cover.

The reduction is as follows. Let G with lower limit & be an instance of the
independent-set problem. If G has n nodes, let G with upper limit n — k be the
instance of the node-cover problem we construct. Evidently this transformation
can be accomplished in linear time. We claim that

e G has an independent set of size k if and only if G has a node cover of
size n — k.

(If) Let N be the set of nodes of G, and let C’ be the node cover of size n — k.
We claim that N — C is an independent set. Suppose not; that is, there is a
pair of nodes v and w in N — C’ that has an edge between them in G. Then
since neither v nor w is in C, the edge (v, w) in G is not covered by the alleged
node cover C. We have proved by contradiction that N — C is an independent
set. Evidently, this set has k nodes, so this direction of the proof is complete.

(Only-if) Suppose J is an independent set of & nodes. We claim that NV — I is
a node cover with n — k& nodes. Again, we proceed by contradiction. If there
is some edge (v, w) not covered by N — J, then both v and w are in /, yet are
connected by an edge, which contradicts the definition of an independent set.
oO

10.4.4 The Directed Hamilton-Circuit Problem

We would like to show NP-complete the Traveling Salesman Problem (TSP),
because this problem is one of great interest in combinatorics. The best known
proof of its NP-completeness is actually a proof that a simpler problem, called
the “Hamilton-Circuit Problem” (HC) is NP-complete. The Hamilton-Circuit
Problem can be described as follows:

PROBLEM: Hamilton-Circuit Problem.
INPUT: An undirected graph G.

OUTPUT: “Yes” if and only if G has a Hamilton circuit, that is, a cycle that
passes through each node of G exactly once.

Notice that the HC problem is a special case of the TSP, in which all the weights
on the edges are 1. Thus, a polynomial-time reduction of HC to TSP is very
simple: just add a weight of 1 to the specification of each edge in the graph.
The proof of NP-completeness for HC is very hard. Our approach is to
introduce a more constrained version of HC, in which the edges have directions


--- Page 470 ---
454 CHAPTER 10. INTRACTABLE PROBLEMS

{i.e., they are directed edges, or arcs), and the Hamilton circuit is required to
follow arcs in the proper direction. We reduce 3SAT to this directed version of
the HC problem, then reduce it to the standard, or undirected, version of HC.
Formally:

PROBLEM: The Directed Hamilton-Circuit Problem (DHC).
INPUT: A directed Graph G.

OUTPUT: “Yes” if and only if there is a directed cycle in G that passes through
each node exactly once.

REDUCTION FROM: 35AT.

Theorem 10.21: The Directed Hamilton-Circuit Problem is NP-complete.

PROOF: The proof that DHC is in AP is easy; guess a cycle and check that all
the arcs it needs are present in the graph. We must reduce 3SAT to DHC, and
this reduction requires the construction of a complicated graph, with “gadgets,”
or specialized subgraphs, representing each variable and each clause of the 3SAT
instance.

To begin the construction of a DHC instance from a 3-CNF boolean expres-
sion, let the expression be EF = e; A ez A--- A eg, where each e; Is a clause,
the surn of three literals, say e; = (ag + ay + a3). Let 21, 22,...,%n be the
variables of E. For each clause and for each variable, we construct a “gadget,”
suggested in Fig. 10.9.

For each variable z; we construct a subgraph H; with the structure shown
in Fig. 10.9(a). Here, m; is the larger of the number of occurrences of 2; and
the number of occurrences of Z; in E. In the two columns of nodes, the d’s and
the c’s, there are arcs between 8;; and ¢;; in both directions. Also, each of the
b’s has an arc to the ¢ below it; ie., ij; has an arc to Ci,j+1, a8 long as 7 < mj.
Likewise, c;; has an arc to 5,341, for j < m;. Finally, there is a head node ai
with arcs to both 6,9 and ep, and a foot node d;, with arcs from bjp,, and Cim,-

Figure 10.9(b) outlines the structure of the entire graph. Each hexagon
represents one of the gadgets for a variable, with the structure of Fig. 10.9(a).
The foot node of one gadget has an arc to the head node of the next gadget, in
a cycle.

Suppose we had a directed Hamilton circuit for the graph of Fig. 10.9{b).
We may as well suppose the cycle starts at a). If it next goes to bio, we claim
it must then go to cig, for if not, then cig could never appear on the cycle. In
proof, note that if the cycle goes from a, to bip to €11, then as both predecessors
of cio (that is, @q and 4:9) are already on the cycle, the cycle can never include
Cla-

Thus, if the cycle begins a, 6,9, then it must continue down the “ladder,”
alternating between the sides, as

1, bio, C19, 831, 11,-++ 5 bimysCimis


--- Page 471 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 455

(b)

{c)

Figure 10.9: Constructions used in the proof that the Hamilton-circuit problem
is NP-complete


--- Page 472 ---
456 CHAPTER 10. INTRACTABLE PROBLEMS

H the cycle begins with a1,¢19, then the ladder is descended in an order where
the c at a level precedes the & as:

41,C10,; bio; 11,511, nae »Ctmy11m,, di

A crucial point in the proof is that we can treat the first order, where descent
is from e’s to lower ’s as if the variable corresponding to the gadget is made
true, while the order in which descent is from 8’s to the lower c’s corresponds
to making that variable false.

After traversing the gadget Hi, the cycle must go to a2, where there is
another choice: go to bgp or cg9 next. However, as we argued for H), once we
inake a choice of whether to go left or right from a2, the path through A is
fixed. In general, when we enter each H; we have a choice of going left or right,
but no other choices if we are not to render a node inaccessible (i.e., the node
cannot appear on a directed Hamilton circuit, because all of its predecessors
have appeared already).

In what follows, it helps to think of making the choice of going from a; to big
as making variable x; true, while choosing to go from a; to ¢io is tantamount
to making 2; false. Thus, the graph of Fig. 10.9(b) has exactly 2” directed
Hamilton circuits, corresponding to the 2” truth assignments to 7 variables.

However, Fig. 10.9(b) is only the skeleton of the graph that we generate for
3-CNF expression &. For each clause €j, we introduce another subgraph J;,
shown in Fig. 10.9(c). Gadget J; has the property that if a cycle enters at Ty},
it must leave at uj; if it enters at s; it must leave at v;, and if it enters at
#; it must leave at wj. The argument we shall offer is that if the cycle, once
it reaches I;, does anything but leave by the node below the one in which it
entered, then one or more nodes are inaccessible — they can never appear on
the cycle. By symmetry, we can consider only the case where r; is the first
node of I; on the cycle. There are three cases:

1. The next two vertices on the cycle are s; and ¢;. If the cycle then goes
to w; and leaves, vj is inaccessible. If the cycle goes to w; and v; and
then leaves, uw; is inaccessible. Thus, the cycle must leave at uj, having
traversed all six nodes of the gadget.

2. The next two vertices after rj are s; and v;. If the cycle does not next
go to uj, then u; becomes inaccessible. If after uj, the cycle next goes to
wy, then tj can never appear on the cycle. The argument is the ‘reverse”
of the inaccessibility argument. Now, t; can be reached from outside, but
if the cycle later includes t;, there will be no next node possible, because
both successors of t; appeared earlier on the cycle. Thus, in this case also,
the cycle leaves by u;. Note, however, that t; and w; are left untraversed;
they will have to appear later on the cycle, which is possible.

3. The circuit goes from rj directly to uj. If the cycle then goes to w;, then
ij cannot appear on the cycle because its successors have both appeared
previously, a8 we argued in case (2). Thus, in this case, the cycle must


--- Page 473 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 457

leave directly by uj, leaving the other four nodes to be added to the cycle
later.

To complete the construction of the graph G for expression &, we connect.
the I;’s to the H;’s as follows: Suppose the first literal in clause ej; is z;, an
unnegated variable. Pick some node cjp, for p in the range 0 to m; — 1, that
has not yet been used for the purpose of connecting to one of the J gadgets.
Introduce arcs from ej, to r; and from wu; to bj,p41- If the first literal of clause e;
is Z;, a negated literal, then find an unused 8). Connect &;, to rj and connect
uy to Ciyp+1-

For the second and third literals of e;, we make the same additions to the
graph, with one exception. For the second literal, we use nodes s; and v;, and
for the third literal we use nodes ¢; and w;. Thus, each /; has three connections
to the H gadgets that represent the variables involved in the clause e;. The
connection comes from a c-node and returns to the &node below if the literal
is unnegated, and it comes from a b-node, returning to the c-node below, if the
literal is negated. We claim that:

e The graph G so constructed has a directed Hamilton circuit if and only
if the expression & is satisfiable.

(If) Suppose there is a satisfying truth assignment T for E. Construct a directed
Hamilton circuit as follows.

1. Begin with the path that traverses only the A’s [i.e., the graph of Fig.
10.9(b)] according to the truth assignment 7’. That is, the cycle goes from
a; to bio if T(2;) = 1, and it goes from a; to ei if T(2;) = 9.

2. However, if the cycle constructed so far follows an arc from djp to ¢j,p+1,
and 8, has another arc to one of the J,’s that has not yet been included
in the cycle, introduce a “detour” in the cycle that includes all six nodes
of J; on the cycle, returning to ci,p41. The arc dip + Ci.p+1 will no longer
be on the cycle, but the nodes at its ends remain on the cycle.

3. Likewise, if the cycle has an arc from ¢jp to bjy41, and cj, has another arc
out that goes to an I; that has not yet been incorporated into the cycle,
modify the cycle to “detour” through all six nodes of J;.

The fact that T satisfies H assures us that the original path constructed by
step (1) will include at least one arc that, in step (2) or (3), allows us to include
the gadget I; for each clause e;. Thus, all the /;’s get included in the cycle,
which becomes a directed Hamilton circuit.

(Only-if) Now, suppose that the graph G has a directed Hamilton circuit. we
must show that EF is satisfiable. First, recall two important points from the
analysis we have done so far:

1. If a Hamilton circuit enters some J; at rj, $;, or tj, then it must leave at
Uj, Uj, OF wW;, respectively.


--- Page 474 ---
458 CHAPTER 10. INTRACTABLE PROBLEMS

2. Thus, if we view the Hamilton circuit as moving through the cycle of H
gadgets, as in Fig. 10.9(b), the excursions that the path makes to some q;
can be viewed as if the cycle followed an arc that was “in parallel” with
one of the arcs bip 4 Ci p41 OF Cip 4 Bip4i.

If we ignore the excursions to the J,’s, then the Hamilton circuit must be one
of the 2” cycles that are possible using the H;’s only — those that make choices
to move from each a; to either big or cin. Each of these choices corresponds to a
truth assignment for the variables of E. If one of these choices yields a Hamilton
circuit including the J;’s, then this truth assignment must satisfy E.

The reason is that if the cycle goes from a; to 69, then we can only make an
excursion to J; if the jth clause has x; as one of its three literals. If the cycle
goes from a; to cj, then we can only make an excursion to f; if the jth clause
has 3 as a literal. Thus, the fact that all Z; gadgets can be included implies
that the truth assignment makes at least one of the three literals of each clause
true; i.e., £ is satisfiable. O

Example 10.22: Let us give a very simple example of the construction of
Theorem 10.21, based on the 3-CNF expression £ = (2 +29 +23)(Z7+%5+23).
The constructed graph is shown in Fig. 10.16. Arcs that connect H. -type gadgets
to f-type gadgets are shown dotted, to improve readability, but there is no other
distinction between dotted and solid arcs.

For instance, at the top left, we see the gadget for 2). Since 21 appears
once negated and once unnegated, the “ladder” needs only one step, so there
are two rows of 6’s and c’s. At the bottom left, we see the gadget for x3, which
appears twice unnegated and does not appear negated. Thus, we need two
different ¢3, + bgp41 arcs that we can use to attach the gadgets for #, and Js
to represent uses of x3 in these clauses. That is why the gadget for x3 needs
three b-e rows.

Let us consider the gadget J2, which corresponds to the clause (27 +Z5+ x3).
For the first literal, Z7, we attach bi) to re and we attach we to ¢,. For the
second literal, Ez, we do the same with beg, $2, v2, and ¢s;. The third literal,
being unnegated, is attached to a ¢ and the 6 below; that, is, we attach c3; to
ts and wa to b3e.

One of several satisfying truth assignments is 2, = 1; zo = 0, and 23 = 0.
For this assignment, the first clause is satisfied by its first literal x1, while the
second clause is satisfied by the second literal, Z. For this truth assignment,
we can devise a Hamilton circuit in which the arcs @, 3 bio, a2 3 Cen, and
a3 — Cgq are present. The cycle covers the first clause by detouring from H, to
J; Le., it uses the arc cig > 71, traverses all the nodes of J,, and returns to by.
The second clause is covered by the detour from H to Jp starting with the arc
bog — 59, traversing all of Z2, and returning to co. The entire hamilton cycle is
shown with thicker lines (solid or dotted) and very large arrows, in Fig. 10.10.
oO


--- Page 475 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 459

Figure 10.10: Example of the Hamilton-circuit construction


--- Page 476 ---
460 CHAPTER 10. INTRACTABLE PROBLEMS

10.4.5 Undirected Hamilton Circuits and the TSP

The proofs that the undirected Hamilton-circuit problem and the Traveling
Salesman problem are also NP-complete are relatively easy. We already saw in
Section 10.1.4 that TSP is in AP. HC is a special case of TSP, so it is also in
NP. We must perform the reductions of DHC to HC and HC to TSP.

PROBLEM: Undirected Hamilton-Circuit Problem.
INPUT: An undirected graph G.

OUTPUT: “Yes” if and only if G has a Hamilton circuit.
REDUCTION FROM: DHC.

Theorem 10.23: HC is NP-complete.

PROOF: We reduce DHC to HC, as follows. Suppose we are given a directed
graph Gg. The undirected graph we construct will be called G,. For every
node v of Gq, there are three nodes uv, v™, and v!?) in G,. The edges of Gy,
are:

I. For all nodes v of Gq, there are edges (vo! v)) and (vy), vl) in Gy.
2. If there is an arc v + w in Gg, then there is an edge (v'!?), w) in Gy.

Figure 10.11 suggests the pattern of edges, including the edge for an arc v 3 w.

Figure 10.11: Arcs in Gg are replaced by edges in G,, that go from rank 2 to
rank 0

Clearly the construction of G, from Gg can be performed in polynomial
time. We must show that

¢ G, has a Hamilton circuit if and only if Gg has a directed Hamilton
circuit.


--- Page 477 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 461

(If) Suppose 7, v2,-..,%n,¥1 is a directed Hamilton circuit. Then surely

{0}

yo, 92), ol), nf?)

(0)
7 Uy +05 1 Ug + Ug 7 Vag roe

0
+U3 0) ov? vf

is an undirected Hamilton circuit in G,. That is, we go down each column, and
then jump to the top of the next column to follow an arc of Ga.

{Only if) Observe that each node v4) of G,, has only two edges, and therefore
must appear in a Hamilton circuit with one of v) and v@) its immediate
predecessor, and the other its immediate successor. Thus, a Hamilton circuit in
G,, must have superscripts on its nodes that vary in the pattern 0,1,2,0, 1, 2,-..
or its opposite, 2,1,0,2,1,0,.... Since these patterns correspond to traversing
a cycle in the two different directions, we may as well assume the pattern is
0,1,2,0,1,2,.,.. Thus, if we look at the edges of the cycle that go from a node
with superscript 2 to one with superscript 0, we know that these edges are arcs
of Gg, aud that each is followed in the direction in which the arc points. Thus,
an undirected Hamilton circuit in G, yields a directed Hamilton circuit in Gy.
Oo

PROBLEM: Traveling Salesman Problem.

INPUT: An undirected graph G with integer weights on the edges, and a limit
k.

OUTPUT: “Yes” if and only if there is a Hamilton circuit of G, such that the
sum of the weights on the edges of the cycle is less than or equal to k.

REDUCTION FROM: HC.

Theorem 10.24: The Traveling Salesman Problem is NP-complete.

PROOF: The reduction from HC is as follows. Given a graph G, construct a
weighted graph G’ whose nodes and edges are the same as the edges of G, with
a weight of 1 on cach edge, and a limit & that is equal to the number of nodes
n of G. Then a Hamilton circuit of weight n exists in G’ if and only if there is
a Hamilton circuit in G. O

10.4.6 Summary of NP-Complete Problems

Figure 10.12 indicates all the reductions we have made in this chapter. Notice
that we have suggested reductions from all the specific problems, like TSP, to
SAT. What happened was that we reduced the language of every polynomial-
time, nondeterministic Turing machine to SAT in Theorem 10.9. Without men-
tioning it explicitly, these TM’s included at least one that solves TSP, one that
solves IS, and so on. Thus, all the NP-complete probiems are polynomial-time
reducible to one another, and are, in effect, different faces of the same problem.


--- Page 478 ---
462 CHAPTER 10. INTRACTABLE PROBLEMS

All of XP

sat >
CesalD
C3saT>
Cis > CDH)

Figure 10.12: Reductions among NP-complete problems

10.4.7 Exercises for Section 10.4

* Exercise 10.4.1: A k-cligue in a graph G is a set of k nodes of G such that
there is an edge between every two nodes in the clique. Thus, a 2-clique is just
a pair of nodes connected by an edge, and a 3-clique is a triangle. The problem
CLIQUE is: given a graph G and a constant k, does G have a k-clique?

a) What is the largest & for which the graph G of Fig. 10.1 satisfies CLIQUE?
b) How many edges does a k-clique have, as a function of ?

c} Prove that CLIQUE is NP-complete by reducing the node-cover problem
to CLIQUE.

*! Exercise 10.4.2: The coloring problem is: given a graph G and an integer k,
is G “k-colorable”; that is, can we assign one of k& colors to each node of G in
such a way that no edge has both of its ends colored with the same color. For


--- Page 479 ---
10.4. ADDITIONAL NP-COMPLETE PROBLEMS 463

example, the graph of Fig. 10.1 is 3-colorable, since we can assign nodes 1 and
4 the color red, 2 green, and 3 blue. In general, if a graph has a k-clique, then
it can be no less than k-colorable, although it might require many more than k
colors.

Figure 10.13: Part of the construction showing the coloring problem to be NP-
complete

In this exercise, we shall give part of a construction to show that the coloring
problem is NP-complete; you must fill in the rest. The reduction is from 35AT.
Suppose that we have a 3-CNF expression with n variables. The reduction
converts this expression into a graph, part of which is shown in Fig. 10.13.
There are, as seen on the left, n + 1 nodes cp,e1,..-,¢n that form an (n + 1)-
clique. Thus, each of these nodes must be colored with a different color. We
should think of the color assigned to cj as “the color cj.”

Also, for each variable 2;, there are two nodes, which we may think of as
x; and %;. These two are connected by an edge, so they cannot get the same
color. Moreover, each of the nodes for 7; are connected to c; for all 7 other
than 0 and i. As a result, one of z; and %; must be colored cp, and the other
is colored c;. Think of the one colored cp as true and the other as false. Thus,
the coloring chosen corresponds to a truth assignment.

To complete the construction, you need to design a portion of the graph for
each clause of the expression. It should be possible to complete the coloring
of the graph using only the colors eg through c, if and only if each clause is
made true by the truth assignment corresponding to the choice of colors. Thus,
the constructed graph is (n+ 1)-colorable if and only if the given expression is
satisfiable.


--- Page 480 ---
464 CHAPTER 10. INTRACTABLE PROBLEMS

Figure 10.14: A graph

! Exercise 10.4.3: A graph does not have to be too large before NP-complete
questions about it become very hard to solve by hand. Consider the graph of
Fig. 10.14.

* a} Does this graph have a Hamilton circuit?
b) What is the largest independent set?
c) What is the smallest node cover?
d) What is the smallest edge cover (see Exercise 10.4.4(c))?
e) Is the graph 2-colorable?

Exercise 10.4.4: Show the following problems to be NP-complete:

a) The subgraph-isomorphism problem: given graphs G, and G2, does Gi
contain a copy of G2 as a subgraph? That is, can we find a subset of the
nodes of G; that, together with the edges among them in G, forms an
exact copy of G2 when we choose the correspondence between nodes of Gz
and nodes of the subgraph of G, properly? Hint: Consider a reduction
from the clique problem of Exercise 10.4.1.


--- Page 481 ---
if.4.

!b)

—
Oo
—

jor
—

ary
—

*1 f)

tt g)

ADDITIONAL NP-COMPLETE PROBLEMS 465

The feedback edge problem: given a graph G and an integer k, does G
have a set of &k edges such that every cycle of G contains at least one of
the & edges? °

The linear integer programming problem: given a set of linear constraints
of the form D7, aia, < cor $7, ajax; > c, where the a’s and ¢ are integer
constants and 21, @2,...,2%, are variables, does there exist an assignment
of integers to each of the variables that makes all the constraints true?

The dominating-set problem: given a graph G and an integer k, does
there exist a subset S of & nodes of G such that each node is either in $
or adjacent to a node of $?

The firehouse problem: given a graph G, a distance d, and a budget f of
“firehouses,” is it possible to choose f nodes of G such that no node is of
distance (number of edges that must be traversed) greater than d from
some firehouse?

The hAaif-eliqgue problem: Given a graph G with an even number of vertices,
does there exist a clique of G (see Exercise 10.4.1} consisting of exactly
half the nodes of G? Hint: Reduce CLIQUE to the half-clique problem.
You must figure out how to add nodes to adjust the size of the largest
clique.

The unit-execution-time-scheduling problem: given k “tasks”
11, 75,..., 8

a number of “processors” p, a ‘time limit t, and some “precedence con-
straints” of the form 7; < 7; between pairs of tasks, does there exist a
schedule of the tasks, such that:

1. Each task is assigned to one time unit between 1 and ¢,
2. At most p tasks are assigned to any one time unit, and

3. The precedence constraints are respected; that is, if ZT, < 7; is a
constraint, then T; is assigned to an earlier time unit than 7}?

The exact-cover problem: given a set S and a set of subsets S),52,...,5n
of S, is there a set of sets TC {$),S2,...,5n} such that each element x
of S$ is in exactly one member of T’?

The knapsack problem: given a list of & integers 71, 12,..-,2%, Can we
partition them into two sets whose sums are the same? Note: This prob-
lem appears superficially to be in P, since you might assume that the
integers themselves are small. Indeed, if the values of the integers are
limited to some polynomial in the number of integers k, then there is a
polynomial-time algorithm. However, in a list of & integers represented in
binary, having total length n, we can have certain integers whose values
are almost exponential in a.


--- Page 482 ---
466 CHAPTER 10. INTRACTABLE PROBLEMS

Exercise 10.4.5: A Hamilton path in a graph G is an ordering of all the nodes
71, 2,-..,nx Such that there is an edge from n; to ni41, for alli = 1,2,..., 4-1.
A directed Hamilton path is the same for a directed graph; there must be an arc
from each n; to 7:41. Notice that the Hamilton path requirement is just slightly
weaker than the Hamilton-circuit condition. If we also required an edge or arc
from nz, to m,, then it would be exactly the Hamilton-circuit condition. The
(directed) Hamilton-path problem is: given a (directed) graph, does it have at
least one (directed) Hamilton path?

* a) Prove that the directed Hamilton-path problem is NP-complete. Hint:
Perform a reduction from DHC. Pick any node, and split it into two, such
that these two nodes must be the endpoints of a directed Hamilton path,
and such a path exists if and only if the original graph has a directed
Hamilton circuit.

oF

Show that the (undirected) Hamilton-path problem is NP-complete. Hint:
Adapt the construction of Theorem 10.23.

*1¢

—

Show that the following problem is NP-complete: given a graph G and
an integer k, does G have a spanning tree with at most & leaf vertices?
Hint: Perform a reduction from the Hamilton-path problem.

!'d) Show that the following problem is NP-complete: given a graph G and
an integer d, does G have a spanning tree with no node of degree greater
than d? (The degree of a node 1 in the spanning tree is the number of

edges of the tree that have n as an end.)

eet

10.56 Summary of Chapter 10

+ The Classes P and NP: P consists of all those languages or problems
accepted by some Turing machine that runs in some polynomial amount
of time, as a function of its input length. A’P is the class of languages or
problems that are accepted by nondeterministic TM’s with a polynomial
bound on the time taken along any sequence of nondeterministic choices.

¢ The P = NP Question: It is unknown whether or not P and VP are
really the same classes of languages, although we suspect. strongly that
there are languages in AP that are not in P.

+ Polynomial-Time Reductions: If we can transform instances of one prob-
lem in polynomial time into instances of a second problem that has the
same answer — yes or no — then we say the first problem is polynomial-
time reducible to the second.

+ NP-Complete Problems: A language is NP-complete if it is in WP, and
there is a polynomial-time reduction from each language in AP to the
language in question. We believe strongly that none of the NP-complete


--- Page 483 ---
10.6. REFERENCES FOR CHAPTER 10 467

problems are in P, and the fact that no one has ever found a polynomial-
time algorithm for any of the thousands of known NP-complete problems
is mutually re-enforcing evidence that none are in P.

+ NP-Complete Satisfiability Problems: Cook’s theorem showed the first
NP-complete problem — whether a boolean expression is satisfiable —
by reducing all problems in A’P to the SAT problem in polynomial time.
In addition, the problem remains NP-complete even if the expression is
restricted to consist of a product of clauses, each of which consists of only
three literals — the problem 3SAT.

+ Other NP-Complete Problems: There is a vast collection of known NP-
complete problems; each is proved NP-complete by a polynomial-time
reduction from some previously known NP-complete problem. We have
given reductions that show the following problems NP-complete: inde-
pendent set, node cover, directed and undirected versions of the Hamilton
circuit problem, and the traveling-salesman problem.

10.6 References for Chapter 10

The concept of NP-completeness as evidence that the problem could not be
solved in polynomial time, as well as the proof that SAT, CSAT, and 3SAT are
NP-complete, comes from Cook [3]. A follow-on paper by Karp [6] is generally
accorded equal importance, because that paper showed that NP-completeness
was not just an isolated phenomenon, but rather applied to very many of the
hard combinatorial problems that people in Operations Research and other
disciplines had been studying for years. Each of the problems proved NP-
complete in Section 10.4 are from that paper: independent set, node cover,
Hamilton circuit, and TSP. In addition, we can find there the solutions to
several of the problems mentioned in the exercises: clique, edge cover, knapsack,
coloring, and exact-cover.

The book by Garey and Johnson [4] summarizes a great deal about what
is known concerning which problems are NP-complete, and special cases that
are polynomial-time. In [5] are articles about approximating the solution to an
NP-complete problem in polynomial time.

Several other contributions to the theory of NP-completeness should be ac-
knowledged. The study of classes of languages defined by the running time
of Turing machines began with Hartmanis and Stearns [8]. Cobham [2] was
the first to isolate the concept of the class P, as opposed to algorithms that
had a particular polynomial running time, such as O(n?). Levin [7] was an
independent, although somewhat later, discovery of the NP-completeness idea.

NP-completeness of linear integer programming [Exercise 10.4.4(c)] appears
in [1] and also in unpublished notes of J. Gathen and M. Sieveking. NP-
completeness of unit-execution-time scheduling [Exercise 10.4.4(g)] is from [9].


--- Page 484 ---
468

CHAPTER 10. INTRACTABLE PROBLEMS

. I. Borosh and L. B. Treybig, “Bounds on positive integral solutions of lin-
ear Diophantine equations,” Proceedings of the AMS 55 (1976), pp. 299-
304.

. A. Cobham, “The intrinsic computational difficulty of functions,” Proe.

1964 Congress for Logic, Mathematics, and the Philosophy of Science,
North Holland, Amsterdam, pp. 24-30.

. 8. C. Cook, “The complexity of theorem-proving procedures,” Third ACM
Symposium on Theory of Computing (1971}, ACM, New York, pp. 151-
158.

. M. R. Garey and D. 8. Johnson, Computers and Intractability: a Guide
to the Theory of NP-Completeness, H. Freeman, New York, 1979.

. D. 5. Hochbaum (ed.), Approtimation Algorithms for NP-Hard Problems,
PWS Publishing Co., 1996.

. R.M. Karp, “Reducibility among combinatorial problems,” in Complexity
of Computer Computations (R. E. Miller, ed.), Plenum Press, New York,
pp. 85-104, 1972.

. L.A. Levin, “Universal sorting problems,” Problemi Peredachi Infarmatsi
9:3 (1973), pp. 265-266.

. J. Hartmanis and R. E. Stearns, “On the computational complexity of
algorithms,” Transactions of the AMS 117 (1965), pp. 285-306.

. J. D. Ullman, “NP-complete scheduling problems,” J. Computer and Sys-
tem Sciences 10:3 (1975), pp. 384-393.


--- Page 485 ---
Chapter 11

Additional Classes of
Problems

The story of intractable problems does not begin and end with VP. There are
many other classes of problems that appear to be intractable, or are interest-
ing for some other reason. Several questions involving these classes, like the
P —NP question, remain unresolved.

We shall begin by looking at a class that is closely related to P and WP: the
class of complements of \’P languages, often called “co VP.” If P = NP, then
coNVP is equal to both, since P is closed under complementation. However, it
is likely that co-VP is different from both these classes, and in fact likely that
no NP-complete problem is in co-V’P.

Then, we consider the class PS, which is ail the problems that can be solved
by a Turing machine using an amount of tape that is polynomial in the length of
its input. These TM’s are allowed to use an exponential amount of time, as long
as they stay within a limited region of the tape. In contrast to the situation for
polynomial time, we can prove that nondeterminism doesn’t increase the power
of the TM when the limitation is polynomial space. However, even though PS
clearly includes all of A’P, we do not know whether PS is equal to VP, or even
whether it is equal to P. We expect that neither equality is true, however, and
we give a problem that is complete for PS and appears not to be in VP.

Then, we turn to randomized algorithms, and two classes of languages that
lie between P and VP. One is the class RP of “random polynomial” languages.
These languages have an algorithm that runs in polynomial time, using some
“coin flipping” or (in practice) a random-number generator. The algorithm
either confirms membership of the input in the language, or says “I don't know.”
Moreover, if the input is in the language, then there is some probability greater
than 0 that the algorithm will report success, so repeated application of the
algorithm will, with probability approaching 1, confirm membership.

The second class, called 2PP (zero-error, probabilistic polynomial), also
involves randomization. However, algorithms for languages in this class either

469


--- Page 486 ---
470 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

say “yes” the input is in the language, or “no” it is not. The expected running
time of the algorithm is polynomial. However, there might be runs of the
algorithm that take more time than would be allowed by any polynomial bound.

To tie these concepts together, we consider the important issue of primality
testing. Many cryptographic systems today rely on both:

1. The ability to discover large primes quickly (in order to allow communi-
cation between machines in a way that is not subject to interception by
an outsider} and

2. The assumption that it takes exponential time to factor integers, if time
is measured as a function of the length 7 of the integer written in binary.

We shall see that testing primes is both in VP and co-A’P, and therefore it is
unlikely that we can prove primality testing to be NP-complete. That is un-
fortunate, because proofs of NP-completeness are the most common arguments
that a problem most likely requires exponential time. We shall also see that
primality testing is in the class RP. This situation is both good news and bad
news. It is good, because in practice, cryptographic systems that require primes
really do use an algorithm in the RP class to find them. It is bad because it
provides further weight to the assumption that we shall not be able to prove
primality testing to be NP-complete.

11.1 Complements of Languages in AP

The class of languages P is closed under complementation (see Exercise 10.1.6).
For a simple argument why, let E be in P and let M be a TM for L. Modify
M as follows, to accept LE. Introduce a new accepting state q and have the new
TM transition to g whenever M halts in a state that is not accepting. Make the
former accepting states of M be nonaccepting. Then the modified TM accepts
2, and runs in the same amount of time that Af does, with the possible addition
of one move. Thus, L£ is in P if LZ is.

It is not known whether VP is closed under complementation. It appears
not, however, and in particular we expect that whenever a language L is NP-
complete, then its complement is not in AVP.

11.1.1 The Class of Languages Co-NVP

Co-NP is the set of languages whose complements are in W“P. We observed
at the beginning of Section 11.1 that every language in P has its complement
also in P, and therefore in AP. On the other hand, we believe that none
of the NP-complete problems have their complements in 'V P, and therefore
no NP-complete problem is in co-V/P. Likewise, we believe the complements
of NP-complete problems, which are by definition in co-VP, are not in NP.
Figure 11.1 shows the way we believe the classes P, W“P, and co-N’P relate.


--- Page 487 ---
11.1. COMPLEMENTS OF LANGUAGES IN NP 471

However, we should bear in mind that, should P turn out to equal VP, then
all three classes are actually the same.

NP-complete problems

Complements of
NP-complete problems

Figure 11.1: Suspected relationship between co-VP and other classes of lan-
guages

Example 11.1: Consider the complement of the language SAT, which is surely
a member of co-V’P. We shall refer to this complement as USAT (unsatisfiable).
The strings in USAT include all those that code boolean expressions that are
not satisfiable. However, also in USAT are those strings that do not code valid
boolean expressions, because surely none of those strings are in SAT. We believe
that USAT is not in VP, but there is no proof.

Another example of a problem we suspect. is in coNVP but not in AP is
TAUT, the set of all (coded) boolean expressions that are tautologies; i.e., they
are true for every truth assignment. Note that an expression F is a tautology
if and only if ~# is unsatisfiable. Thus, TAUT and USAT are related in that
whenever boolean expression & is in TAUT, 7£ is in USAT, and vice-versa.
However, USAT also contains strings that do not represent valid expressions,
while all strings in TAUT are valid expressions. O

11.1.2 NP-Complete Problems and Co-NP

Let us assume that P 4 AP. It is still possible that the situation regarding
co-NP is not exactly as suggested by Fig. 11.1, because we could have (’P and
co-A’P equal, but larger than P. That is, we might discover that problems like
USAT and TAUT can be solved in nondeterministic polynomial time (i.¢., they


--- Page 488 ---
472 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

arein AP), and yet not be able to solve them in deterministic polynomial time.
However, the fact that we have not been able to find even one NP-complete
problem whose complement is in A’P is strong evidence that WP 4 co-NVP, as
we prove in the next theorem.

Theorem 11.2: NP = co-A’P if and only if there is some NP-complete prob-
lem whose complement is in VP.

PROOF: (Only-if) Should A’P and co-N’P be the same, then surely every NP-
complete problem L, being in A’P, is also in co.WP. But the complement of a
problem in co-A’P is in VP, so the complement of ZL is in WP.

{If} Suppose P is an NP-complete problem whose complement P is in AP.
Then for every language L in VP, there is a polynomial-time reduction of L
to P. The same reduction also is a polynomial-time reduction of L to P. We
prove that WP = co-NP by proving containment in both directions.

NP © co-M’P: Suppose Eis in MP. Then © is in co-NP. Combine the
polynomial-time reduction of E to P with the assumed nondeterministic, poly-
nomial-time algorithm for P to yield a nondeterministic, polynomial-time algo-
rithm for Z. Hence, for any £ in A’P, E is also in WP. Therefore L, being the
complement of a language in VP, is in co-WP. This observation tells us that
NP Cco-NP.

co-NP C NP: Suppose L is in co-WP. Then there is a polynomial-time
reduction of £ to P, since P is NP- complete, and Lis in WP. This reduction
is also a reduction of L to P. Since P is in NP, we combine the reduction
with the nondeterministic, polynomial-time algorithm for P to show that L is
in VP. O

11.1.3 Exercises for Section 11.1

Exercise 11.1.1: Below are some problems. For each, tell whether it is in
A’P and whether it is in co-WP. Describe the complement of each problem. If
either the problem or its complement is NP-complete, prove that as well.

* a) The problem TRUE-SAT: given a boolean expression E that is true when
all the variables are made true, is there some other truth assignment
besides all-true that makes F true?

b}) The problem FALSE-SAT: given a boolean expression E that is false
when all its variables are made false, is there some other truth assignment
besides all-faise that makes BH false?

¢) The problem DOUBLE-SAT: given a boolean expression E, are there at
least two truth assignments that make & true?

d) The problem NEAR-TAUT: given a boolean expression E, is there at
most one truth assignment that makes E false?


--- Page 489 ---
11.2. PROBLEMS SOLVABLE IN POLYNOMIAL SPACE 473

*1 Exercise 11.1.2: Suppose there were a function f that is a one-one function
from n-bit integers to n-bit integers, such that:

1. f(z) can be computed in polynomial time.
2. f—(x) cannot be computed in polynomial time.

Show that the language consisting of pairs of integers (x,y) such that
f(z) <y

would then be in (VPM co-NP) - P.

11.2 Problems Solvable in Polynomial Space

Now, let us look at a class of problems that includes all of NP, and appears to
include more, although we cannot be certain it dees. This class is defined by
allowing a Turing machine to use an amount of space that is polynomial in the
size of its input, no matter how much time it uses. Initially, we shall distinguish
between the languages accepted by deterministic and nondeterministic TM’s
with a polynomial space bound, but we shall soon see that these two classes of
languages are the same.

There are complete problems P for polynomial space, in the sense that all
problems in this class are reducible in polynomial time to P. Thus, if P is in
P or in AP, then all languages with polynomial-space-bounded TM’s are in P
or AP, respectively. We shall offer one example of such a problem: “quantified
boolean formulas.”

11.2.1 Polynomial-Space Turing Machines

A polynomial-space-bounded Turing machine is suggested by Fig. 11.2. There
is some polynomial p(n) such that when given input w of length n, the TM
never visits more than p(n) cells of its tape. By Theorem 8.12, we may assume
that the tape is semi-infinite, and the TM never moves left from the beginning
of its input.

Define the class of languages PS (polynomial space) to include all and only
the languages that are L(M) for some polynomial-space-bounded, deterministic
Turing machine M. Also, define the class VPS (nondeterministic polynomial
space) to consist of those languages that. are L(A4) for some nondeterministic,
polynomial-space-bounded TM M. Ewidently PS C NPS, since every deter-
ministic TM is technically nondeterministic also. However, we shall prove the
surprising result that PS =NVPS.!

lYou may see this class written as PSPACE in other works on the subject. However,
we prefer ta use the script PS to denote the class of problema solved in deterministic (or
nondeterministic}) polynomial time, as we shalk drop the use of NPS once the equivalence
PS = NPS has been proved.


--- Page 490 ---
474 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

Finite
control

“input vw —
n cells

—— cells ever used — MWY
RY cells

Figure 11.2: A TM that uses polynomial space

11.2.2 Relationship of PS and NPS to Previously Defined
Classes

To start, the relationships P C PS and VP C NPS should be obvious. The
reason is that if a TM makes only a polynomial number of moves, then it uses
no more than a polynomial number of cells; in particular, it cannot visit more
cells than one plus the number of moves it makes. Once we prove PS = VPS,
we shall see that in fact the three classes form a chain of containment: P C
NP CPS.

An essential property of polynomial-space-bounded TM’s is that they can
make only an exponential number of moves before they must repeat an ID. We
need this fact to prove other interesting facts about PS, and also to show that
PS contains only recursive languages; i.e., languages with algorithms. Note
that there is nothing in the definition of PS or WPS that requires the TM to
halt. It is possible that the TM cycles forever, without leaving a polynomial-
sized region of its tape.

Theorem 11.3: If A¢ is a polynomial-space-bounded TM (deterministic or
nondeterministic), and p(n) is its polynomial space bound, then there is a con-
stant ¢ such that if M accepts its input w of length n, it does so within c'+?(™
moves.

PROOF: The essential idea is that Mf must repeat an ID before making more
than ¢!+"(™) moves. If M repeats an ID and then accepts, there must be a
shorter sequence of ID’s leading to acceptance. That is, if a F BF BE y,


--- Page 491 ---
11.2. PROBLEMS SOLVABLE IN POLYNOMIAL SPACE 475

where a is the initial ID, 8 is the repeated ID, and ¥ is the accepting ID, then
ak B F y is a shorter sequence of ID's leading to acceptance.

The argument that c must exist exploits the fact that there are a limited
number of ID’s if the space used by the TM is limited. In particular, let ¢ be
the number of tape symbols of M, and let be the number of states of M.
Then the number of different ID’s of Mf when only p(n) tape cells are used is
at most sp(n)t™, That is, we can choose one of the ¢ states, place the head
at any of p(n) tape positions, and fill the p(n) cells with any of 2?) sequences
of tape symbols.

Pick ¢ = $+¢. Then consider the binomial expansion of (t+ 3)!+?(™, which
is

pitp(n) +4 (1 + p(n)) st? oes
Notice that the second term is at least as large as sp(n)t?(™ , which proves that
c!+P(*) ig at least equal to the number of possible ID’s of Mf. We conclude the
proof by observing that if Mf accepts w of length n, then it does so by a sequence
of moves that does not repeat an ID. Therefore, M accepts by a scquence of
moves that is no longer than the number of distinct ID’s, which is ete) oO

We can use Theorem 11.3 to convert any polynomial-space-bounded TM
into an equivalent one that always halts after making at most an exponential
number of moves. The essential point is that, since we know the TM accepts
within an exponential number of moves, we can count how many moves have
been made, and we can cause the TM to halt if it has made enough moves
without accepting.

Theorem 11.4: If £ is a language in PS (respectively NPS), then L is ac-
cepted by a polynomial-space-bounded deterministic (respectively nondeter-
ministic) TM that halts after making at most ct) moves, for some polynomial
q(n) and constant c > 1.

PROOF: We'll prove the statement for deterministic TM’s; the same argument
applies to NT'M’s. We know L is accepted by a TM M, that has a polynomial
space bound p(n). Then by Theorem 11.3, if M, accepts w it does so in at most
elt pllel) steps,

Design a new TM M, that has two tapes. On the first tape, Ad, simulates
My, and on the second tape, M> counts in base ¢ up to cl+P(“l, If Ada reaches
this count, it halts without accepting. Mo thus uses 1 + p{|w|) cells on the
second tape. We also assumed that Mf; uses no more than p(|\w|) cells on its
tape, so M> uses no more than p(|w|) cells on its first tape as well.

If we conyert Mz to a one-tape TM M3, we can be sure that Mq3 uses no
more than 1+-p(n) cells of tape, on any input of length n. Although Ad; may use
the square of the running time of Mo, that time is not more than O(c?)

2In fact, the general rule from Theorem 8.10 is not the strongest claim we can make.
Because only 1 -+ p(n) cells are used by any tape, the simulated tape heads in the many-
tapes-to-one construction can get only 1+ p(z) apart. Thus, c!+?(") moves of the multitape
TM Mo can be simulated in O(pinjer(™) steps, which is less than the claimed Ofe?P™),


--- Page 492 ---
476 CHAPTER If. ADDITIONAL CLASSES OF PROBLEMS

As Ad; makes no more than de??(") moves for some constant d, we may pick
q{n) = 2p(n) + log.d. Then Ag makes at most cM") steps. Since Mp always
halts, A¢3 always halts. Since M, accepts L, so do Mz and M3. Thus, M3
satisfies the statement of the theorem. O

11.2.3. Deterministic and Nondeterministic Polynomial
Space

Since the comparison between P and VP seems so difficult, it is surprising that.
the same comparison between PS and N’PS is easy: they are the same classes
of languages. The proof involves simulating a nondeterministic TM that has
a polynomial space bound p(n) by a deterministic TM with polynomial space
bound O(p?(n)).

The heart of the proof is a deterministic, recursive test for whether a NTM
N can move from ID J to ID J in at most m moves. A DTM D systematically
tries all middle ID’s K to check whether J can become K in m/2 moves, and
then K can become J in m/2 moves. That is, imagine there is a recursive
function reach(I, J,m) that decides if FF J by at most m moves.

Think of the tape of D as a stack, where the arguments of the recursive calls
to reach are placed. That is, in one stack frame D holds [I, J,m]. A sketch of
the algorithm executed by reach is shown in Fig. 11.3.

BOOLEAN FUNCTION reach{I,J,m)
ID: I,J; INT: m;

BEGIN
IF (m == 1) THEN /* basis */ BEGIN
test if I == J or I can become J after one move;
RETURN TRUE if so, FALSE if not;
END;

ELSE /* inductive part */ BEGIN
FOR each possible ID K BO
IF (reach(I,K,m/2) AND reach(K,J,m/2)) THEN
RETURN TRUE;
RETURN FALSE;
END;
END;

Figure 11.3: The recursive function reach tests whether one ID can become
another within a stated number of moves

It is important to observe that, although reach calls itself twice, it makes
those calls in sequence, and therefore, only one of the calls is active at a time.
That is, if we start with a stack frame [J,,J,,m], then at any time there is
only one call (Jo, J2,™m/2], one call [23, 3,m/4], another [Z4, Ja, 7/8], and so


--- Page 493 ---
11.2. PROBLEMS SOLVABLE IN POLYNOMIAL SPACE 477

on, until at some point the third argument becomes 1. At that point, reach
can apply the basis step, and needs no more recursive calls. It tests if J = J
or I+ J, returning TRUE if either holds and FALSE if neither does. Figure 11.4
suggests what the stack of the DIM D looks like when there are as many active
calls to reach as possible, given an initial move count of m.

I, Jy m I, J, mf f3 Ig myq | 144, mg | ---

Figure 11.4: Tape of a DTM simulating a NTM by recursive calls to reach

While it may appear that many calls to reach are possible, and the tape
of Fig. 11.4 can become very long, we shall show that it cannot become “too
long.” That is, if started with a move count of m, there can only be loggm
stack frames on the tape at any one time. Since Theorem 11.4 assures us that
the NTM N cannot make more than c’™ moves, m does not have to start
with a number greater than that. Thus, the number of stack frames is at most
log, c?\™, which is O(p(n}). We now have the essentials behind the proof of
the following theorem.

Theorem 11.5: (Savitch’s Theorem) PS = NPS.

PROOF: It is obvious that PS C VPS, since every DTM is technically a NTM
as well. Thus, we need only to show that WPS C PS; that is, if L is accepted
by some NTM NV with space bound p(n), for some polynomial p(n), then FL is
also accepted by some DTM D with polynomial space bound g{n), for some
other polynomial g(n). In fact, we shall show that g(n) can be chosen to be on
the order of the square of p(n).

First, we may assume by Theorem 11.3 that if N accepts, it does so within
c!+v(") steps for some constant c. Given input w of length n, D discovers what
N does with input w by repeatedly placing the triple [Jp, J,m] on its tape and
calling reach with these arguments, where:

j. fp is the initial ID of V with input w.

2. J is any accepting ID that uses at most p(m) tape cells; the different J’s
are enumerated systematically by D, using a scratch tape.

3. m= tH),

We argued above that there will never be more than logy m recursive calls
that are active at the same time, i.e., one with third argument m, one with
m/2, one with m/4, and so on, down to 1. Thus, there are no more than log, m
stack frames on the stack, and log, m is O{p(n)).

Further, the stack frames themseives take O{p(m)) space. The reason is that
the two ID’s each require only 1+ p(n) cells to write down, and if we write m


--- Page 494 ---
478 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

in binary, it requires = logy ct? cells, which is O(p(n)}. Thus, the entire
stack frame, consisting of two ID’s and an integer, takes O(p(n)) space.

Since D can have O(p(n)) stack frames at most, the total amount of space
used is O(p?(n)). This amount of space is a polynomial if p(n) is polynomial,
so we conclude that Z has a DTM that is polynomial-space bounded. O

In summary, we can extend what we know about complexity classes to in-
clude the polynomial-space classes. The complete diagram is shown in Fig. 11.5.

Recursive

Figure 11.5: Known relationships among classes of languages

11.3. A Problem That Is Complete for PS

In this section, we shall introduce a problem called “quantified boolean formu-
las” and show that it is complete for PS.

11.3.1 PS-Completeness
We define a problem P to be complete for PS (PS-complete) if:

1. Pisin PS.

2, All languages L in PS are polynomial-time reducible to P.


--- Page 495 ---
11.3. A PROBLEM THAT IS COMPLETE FOR PS 479

Notice that, although we are thinking about polynomial space, not time, the
requirement for PS-completeness is similar to the requirement for NP-com-
pleteness: the reduction must be performed in polynomial time. The reason
is that we want to know that, should some PS-complete problem turn out to
be in P, then P = PS, and also if some PS-complete problem is in VP, then
NP = PS. If the reduction were only in polynomial space, then the size of the
output might be exponential in the size of the input, and therefore we could
not draw the conclusions of the following theorem. However, since we focus on
polynomial-time reductions, we get the desired relationships.

Theorem 11.6: Suppose P is a PS-complete problem. Then:
a) If Pisin P, then P = PS.
b) If P isin VP, then NP = PS.

PROOF: Let us prove (a). For any L in PS, we know there is a polynomial-time
reduction of Z to P. Let this reduction take time g(n). Also, suppose P is in
P, and therefore has a polynomial-time algorithm; say this algorithm runs in
time p(n).

Given a string w, whose membership in L we wish to test, we can use the
reduction to convert it to a string x that is in P if and only if w is in L. Since
the reduction takes time q(|w|), the string x cannot be longer than q(|w|). We
may test membership of x in P in time p(|z|), which is p(q(\w|)), a polynomial
in |w|. We conclude that there is a polynomial-time algorithm for L.

Therefore, every language L in PS is in P. Since containment of P in PS is
obvious, we conclude that if P is in ?, then P = PS. The proof for (b), where
P is in VP, is quite similar, and we shall leave it to the reader. O

11.3.2 Quantified Boolean Formulas

We are going to exhibit a problem P that is complete for PS. But first, we need
to learn the terms in which this problem, called “quantified boolean formulas”
or QBF, is defined.

Roughly, a quantified boolean formula is a boolean expression with the
addition of the operators ¥ (“for all”) and 3 (“there exists”). The expression
(vz)(E) means that £ is true when all occurrences of x in E are replaced by 1
(true), and also true when all occurrences of x are replaced by 0 (false). The
expression (Ar)(E) means that £ is true either when all occurrences of x are
replaced by 1 or when all occurrences of x are replaced by 0, or both.

To simplify our description, we shall assume that no QBF contains two or
more quantifications (¥ or 3) of the same variable x. This restriction is not
essential, and corresponds roughly to disallowing two different functions in a
program from using the same local variable.* Formally, the quantified boolean

3We can always rename one of two distinct uses of the same variable name, cither in

programs or in quantified boolean formulas. For programs, there is no reason to avoid reuse
of the same loca! narne, but in QBF’s we find it convenient to assume there is no reuse.


--- Page 496 ---
480 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

formulas are defined as follows:
1. 0 (false) , 1 (true), and any variable are QBF’s.

2. If & and F are QBF’s then so are (£), 4(E}, (E) A (F), and (E) v (F),
representing a parenthesized FE, the negation of FE, the AND of F and
F, and the OR of # and F, respectively. Parentheses may be removed if
they are redundant, using the usual precedence rules: NOT, then AND,
then OR (lowest). We shall also tend to use the “arithmetic” style of
representing AND and OR, where AND is represented by juxtaposition
(no operator) and OR is represented by +. That is, we often use (E}(F)
in place of (£) A (F) and use (£) + (F) in place of (£) v (F).

3. If F is a QBF that does not include a quantification of the variable 2,
then (Vz)(E) and (Sz)(E) are QBF’s. We say that the scope of x is the
expression &. Intuitively, x is only defined within EF, much as the scope
of a variable in a program has a scope that is the function in which it
is declared. Parentheses around £ (but not around the quantification)
can be removed if there is no ambiguity. However, to avoid an excess of
nested parentheses, we shall write a chain of quantifiers such as

(vz) ((@y)((v2)(B)) )

with only the one pair of parentheses around E, rather than one pair for
each quantifier on the chain, i.e., as (Vx)(Sy)(Vz)(E).

Example 11.7: Here is an example of a QBF:

(Vx) ((Sy)(ay) + (Vz)(72 + 2)) (11.1)

Starting with the variables z and y, we connect them with AND and then
apply the quantifier (Jy) to make the subexpression (3y)(2y). Similarly, we
construct the boolean expression ~z + 2 and apply the quantifier (Vz) to make
the subexpression (Vz)(2 + 2). Then, we combine these two expressions with
an OR; no parentheses are necessary, because + (OR) has lowest precedence.
Finally, we apply the (Vz) quantifier to this expression to produce the QBF
stated. D

11.3.3 Evaluating Quantified Boolean Formulas

We have yet to define formally what the meaning of a QBF is. However, if we
read V as “for all” and 4 as “exists,” we can get the intuitive idea. The QBF
asserts that for all x (i.e., 2 = 0 or x = 1), either there exists y such that both
x and y are true, or for all z, -7 + 2 is true. This statement happens to be
true. To see why, note that if x = 1, then we can pick y = 1 and make xy true.
if x = 0, then 2 + z is true for both values of z.

If a variable is in the scope of some quantifier of z, then that use of z is
said to be bound. Otherwise, an occurrence of @ is free.


--- Page 497 ---
11.3. A PROBLEM THAT IS COMPLETE FOR P& 481

Example 11.8: Each use of a variable in the QBF of Equation (11.1) is bound,
because it is in the scope of the quantifier for that variable. For instance, the
scope of the variable y, quantified in (3y)(xy), is the expression xy. ‘Thus, the
occurrence of y there is bound. The use of x in wy is bound to the quantifier
(Wx) whose scope is the entire expression. O

The value of a QBF that has no free variables is either 0 or 1 (i.e., false or
true, respectively). We can compute the value of such a QBF by induction on
the length n of the expression.

BASIS: If the expression is of length 1, it can only be a constant 0 or 1, because
any variable would be free. The value of that expression is itself.

INDUCTION: Suppose we are given an expression with no free variables and
length n > 1, and we can evaluate any expression of shorter length, as long as
that expression has no free variables. There are six possible forms such a QBF
ean have:

1. The expression is of the form (Z). Then E is of length n — 2 and can be
evaluated to be either 0 or 1. The value of (£) is the same.

2. The expression is of the form ~H#. Then E is of length n — 1 and can be
evaluated. If # = 1, then 7B = 0, and vice versa.

3. The expression is of the form EF. Then both # and F are shorter than
n, and so can be evaluated. The value of EF is 1 if both # and F have
the value 1, and EF = 0 if either is 0.

4, The expression is of the form E+ F. Then both # and F are shorter
than n, and so can be evaluated. The value of E + F is 1 if either B or
F has the value 1, and E + F =0 if both are 0.

5. If the expression is of the form (¥z)(£), first replace all occurrences of «
in F by 0 to get the expression Eo, and also replace each occurrence of &
in E by 1, to get the expression £,. Observe that Eq and E, both:

(a) Have no free variables, because any occurrence of a free variable in
Eo or E, could not be z, and therefore would be some variable that
is also free in &.

(b) Have length n — 6, and thus are shorter than n.

Evaluate Ey and E,. If both have value 1, then (Vx)(#) has value 1;
otherwise it has the value 0. Note how this rule reflects the “for all 2”
interpretation of (¥z).

6. If the given expression is (Jz)(E), then proceed as in (5), constructing
Eq and Ey, and evaluating them. If either Hy or 2, has value 1, then
(Jz)(E) has value 1; otherwise it has value 0. Note that this rule reflects
the “exists «” interpretation of (Jz).


--- Page 498 ---
482 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

Example 11.9: Let us evaluate the QBF of Equation (11.1). It is of the form
(¥z}{E), so we must first evaluate Ey, which is:

(Sy) (Oy) + (¥z}(-0 + z) (11.2)

The value of this expression depends on the values of the two expressions con-
nected by the OR: (dy)(Oy) and (Vz)(-0 + z); Eo has value 1 if either of those
expressions does. To evaluate (Jy)(Oy), we must substitute y = 0 and yo=lin
subexpression Oy, and check that at least one of them has the value 1. However,
both 0A 0 and 0 A 1 have the value 0, so (Jy)(Oy) has value 0.4

Fortunately, (¥z)(+0 + z) has value 1, as we can see by substituting both
z= 0 and z= 1. Since -0 = 1, the two expressions we must evaluate are 1 V 0
and 1 Vv 1. Since both have value 1, we know that (Vz)(40 +z) has value 1. We
how conclude that Eo, which is Equation (11.2), has value 1.

We must also check that E, which we get by substituting 2 = 1 in Equa-
tion (11.1):

(Ay)(1y) + (¥z)(a1 + 2) (11.3)

also has value 1. Expression (Jy)(1y) has value 1, as we can see by substituting
y = 1. Thus, £,, Equation (11.3), has value 1. We conclude that the entire
expression, Equation (11.1), has value 1. oO

11.3.4 PS-Completeness of the QBF Problem

We can now define the quantified boolean formula problem: Given a QBF with
no free variables, does it have the value 1? We shall refer to this problem
as QBF, while continuing also to use QBF as an abbreviation for “quantified
boolean formula.” The context should allow us to avoid confusion.

We shall show that the QBF problem is complete for PS. The proof com-
bines ideas from Theorems 10.9 and 11.5. From Theorem 10.9, we use the idea
of representing a computation of a TM by logical variables each of which tells
whether a certain cell has a certain value at a certain time. However, when we
were dealing with polynomial time, as we were in Theorem 10.9, there were only
polynomially many variables to concern us. We were thus able to generate, in
polynomial time, an expression saying that the TM accepted its input. When
we deal with a polynomial space bound, the number of ID’s in the computation
can be exponential in the input size, so we cannot, in polynomial time, write
a boolean expression to say that the computation is correct. Fortunately, we
are given a more powerful language to express what we need to say, and the
availability of quantifiers lets us write a polynomial-length QBF that says the
polynomial-space-bounded TM accepts its input.

INotice our use of alternative notations for AND and OR, since we cannot use juxtaposition
and + for expressions involving 0’s and 1's without making the expressions look either like
multidigit numbers or arithmetic addition. We hope the reader can accept both notations as
standing for the same logical operators.


--- Page 499 ---
11.3. A PROBLEM THAT IS COMPLETE FOR PS 483

From Theorem 11.5 we use the idea of “recursive doubling” to express the
idea that one ID can become another in some large number of moves. That is,
to say that ID I can become ID J in m moves, we say that there exists some
ID K such that J becomes K in m/2 moves and K becomes J in another m/2
moves. The language of quantified boolean formulas lets us say these things in
a polynomial-length expression, even if m is exponential in the length of the
input.

Before proceeding to the proof that every language in PS is polynomial-
time reducible to QBF, we need to show that QBF is in PS. Even this part of
the PS-completeness proof requires some thought, so we isolate it as a separate
theorem.

Theorem 11.10: QBF is in PS.

PROOF: We discussed in Section 11.3.3 the recursive process for evaluating a
QBF F. We can implement this algorithm using a stack, which we may store on
the tape of a Turing machine, as we did in the proof of Theorem 11.5. Suppose
F is of length n. Then we create a record of length O(n) for F that includes F’
itself and space for a notation about which subexpression of / we are working
on. Two examples among the six possible forms of F will make the evaluation
process clear.

1. Suppose F = F, + Fy. Then we do the following:

(a) Place ¥) in its own record to the right of the record for F.
(b) Recursively evaluate F,.
(c) If the value of Fj is 1, return the value 1 for F.

(d) But if the value of F, is 0, replace its record by a record for F2 and
recursively evaluate fF.

(ce) Return as the value of # whatever value Fo returns.
2. Suppose F = (3z)(Z£). Then do the following:
(a) Create the expression Ey by substituting 0 for each occurrence of z,
and place Ep in a record of its own, to the right of the record for F’.
(b) Recursively evaluate Eo.
(c) If the value of Fp is 1, then return 1 as the value of F.
{d) But if the value of Eo is 0, create Z, by substituting 1 for z in £.
}

(e) Replace the record for Ey by a record for £), and recursively evaluate
E\.

(f) Return as the value of # whatever value £, returns.

We shall leave to you the similar steps that will evaluate F for the cases that
F is of the other four possible forms: F\F), ~£, (£), or (Vx)(EZ). The basis


--- Page 500 ---
484 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

case, were fis a constant, requires us to return that constant, and no further
records are created on the tape.

In any case, we note that to the right of the record for an expression of
length m will be a record for an expression of length less than m. Note that
even though we often have to evaluate two different subexpressions, we do so
one-at-a-time. Thus, in case (1} above, there are never records for both F, or
any of its subexpressions and F2 or its subexpressions on the tape at the same
time. The same is true of Ho and HE in case (2) above.

Therefore, if we start with an expression of length n, there can never be more
than n records on the stack. Also, each record is O(n) in length. Thus, the
entire tape never grows longer than O(n”). We now have a construction for a
polynomial-space-bounded TM that accepts QBF; its space bound is quadratic.
Note that this algorithm will typically take time that is exponential in n, so it
is noé polynomial-time bounded. O

Now, we turn to the reduction from an arbitrary language L in PS to the
problem QBF. We would like to use propositional variables Yiza as we did in
Theorem 10.9 to assert that the jth position in the ith ID is A. However, since
there are exponentially many ID’s, we could not take an input w of length 7
and even write down these variables in time that is polynomial in n. Instead, we
exploit the availability of quantifiers to make the same set of variables represent
many different ID’s. The idea appears in the proof below.

Theorem 11.11: The problem QBF is PS-complete.

PROOF: Let L be in PS, accepted by a deterministic TM M that uses p(n)
space at most, on input of length n. By Theorem 11.3, we know there is a
constant ¢ such that M accepts within c!+?() moves if it accepts an input of
length m. We shall describe how, in polynomial time, we take an input w of
length m and construct from wa QBF F that has no free variables, and has the
value 1 if and only if w isin £(M).

In writing EZ, we shall have need to introduce polynomially many veriable
ID’s, which are sets of variables y;4 that assert the jth position of the repre-
sented ID has symbol A. We allow j to range from 0 to p(n). Symbol A is either
a tape symbol or state of Af. Thus, the number of propositional variables in a
variable ID is polynomial in n. We assume that all the propositional variables
in different variable ID’s are distinct; that is, no propositional variable belongs
to two different variable ID's. As long as there is only a polynomial number of
variable ID’s, the total number of propositional variables is polynomial.

It is convenient to introduce a notation (4Z), where J is a variable ID.
This quantifier stands for (Ax) (Ara) --- (Sz), where 21, 22,..., 0m are all the
propositional variables in the variable ID J. Likewise, (VI) stands for the V
quantifier applied to all the propositional variables in J.

The QBF we construct for w has the form:

(Ab)(ALS AN AF)


--- Page 501 ---
1L3. A PROBLEM THAT IS COMPLETE FOR PS 485

where:

1. Ig and fy are variable ID’s representing the initial and accepting ID's,
respectively.

2. § is an expression that says “starts right”; i.e., fo is truly the initial ID
of M with input w.

3. N is an expression that says “moves right”; ie., M takes Ig to [y.

4, F is an expression that says “finishes right”; i.e., J» is an accepting ID.

Note that, while the entire expression has no free variables, the variables of Jo
will appear as free variables in S, the variables of Jy appear free in F, and both
groups of variables appear free in NV.

Starts Right

S is the logical AND of literals; each literal is one of the variables of Ip. 3
has literal yj if the jth position of the initial ID with input w is A, and has
literal #ja if not. That is, if w = a@)a2---@p, then Yogo, Pilar, ¥2e2)-- +1 Ynan» and
all yjg, for j =n+1,n+2,...,p(n) appear without negation, and all other
variables of Ig are negated. Here, go is assumed to be the initial state of M,
and 8 is its blank.

Finishes Right

In order for Zy to be an accepting ID, it must have an accepting state. There-
fore, we write F as the logical OR of those variables y;4, chosen from the
propositional variables of Jy, for which A is an accepting state. Position 7 is
arbitrary.

Next Move Is Right

The expression NV is constructed recursively in a way that lets us double the
number of moves considered by adding only O(p{(n)) symbols to the expres-
sion being constructed, and (more importantly) by spending only O(p(n)) time
writing the expression. It is useful to have the shorthand f = J, where J and
J are variable ID’s, to stand for the logical AND of expressions that equate
each of the corresponding variables of J and J. That is, if f consists of vari-
ables y;4 and J consists of variables z;4, then J = J is the AND of expressions
(yja2j4+ (GGA) Za), where j ranges from 0 to p(n), and A is any tape symbol
or state of M.

We now construct expressions N,{I,J), for i = 1,2,4,8,:-- to mean that
IF J by 4 or fewer moves. In these expressions, only the propositional variables
of variable ID’s I and J are free; all other propositional variables are bound.


--- Page 502 ---
486 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

This Construction of N>; Doesn’t Work

Our first instinct about constructing No; from N; might be to use a
straightforward divide-and-conquer approach: if J F J in 2% or fewer
moves, then there must be an ID K such that both IF KandKF Jini

moves or fewer. However, if we write down the formula that expresses this
idea, say No;(7,J) = (AK)(NiUZ, ) A Ni(K,J)), we wind up doubling
the length of the expression as we double i. Since i must be exponential
in in order to express all possible computations of M, we would spend
too much time writing down N, and N would be exponential in length.

BASIS: For i = 1, N,(i,/J) asserts that either J = J, or J + J. We just
discussed how to express the condition J = J above. For the condition J + JJ,
we refer you to the discussion in the “next move is right” portion of the proof of
Theorem 10.9, where we deal with exactly the same problem of asserting that
one ID follows from the previous one. The expression Nj is the logical OR of
these two expressions. Note that we can write N, in O(p(n}) time.

INDUCTION: We construct Ne:(7, 7) from N;. In the box “This Construction
of No; Doesn’t Work” we point out that the direct approach, using two copies
of N; to build No;, doesn’t give us the time and space bounds we need. The
correct way to write MN; is to use one copy of N; in the expression, passing both
the arguments (J, K) and (K, J) to the same expression. That is, No;(I, J) will
use one subexpression N;(P,@Q). We write N2;(J, J) to assert that there exists
ID K such that for all ID’s P and Q, either:

1. (P,Q) # Ui, K) and (P,Q) 4 (K, J) or
2. Ni(P,Q) is true.

Put equivalently, N;(f,K) and N;(K,J) are true, and we don’t care about
whether N;(P, @) is true otherwise. The following is a QBF for No,(f, J):

Noi, J) = (AK)(WP)(VQ)(Ni(P,Q) V
(U = PAK =Q) AWK =PAJ=Q)))

Notice that we can write No; in the time it takes us to write Nj, plus O(p(n))
additional work.

To complete the construction of N, we must construct N,, for the smallest
m. that is a power of 2 and also at least c!t?(™, the maximum possible number
of moves TM M can make before accepting input w of length n. The number
of times we must apply the inductive step above is log,(c!+(™), or O(p(n)).
Since each use of the inductive step takes time O({p(n)), we conclude that N
can be constructed in time O{p?{n)).


--- Page 503 ---
*

—

11.4. LANGUAGE CLASSES BASED ON RANDOMIZATION 487

Conclusion of the Proof of Theorem 11.11

We have now shown how to transform input w into a QBF
(Alo) (Al ,)(S AN AF)

in time that is polynomial in |w]. We have also argued why each of the expres-
sions S, N, and F are true if and only if their free variables represent ID’s Jo
and J, that are respectively the initial and accepting ID’s of a computation of
M on input w, and also Jp F Ty. That is, this QBF has value 1 if and only if
M acceptsw. O

11.3.5 Exercises for Section 11.3

Exercise 11.3.1: Complete the proof of Theorem 11.10 by handling the cases:

a) FSF, Py.
b) F = (Ve)(E).
c) F=-(E).

d) F =(E).

Exercise 11.3.2: Show that the following problem is P5-complete. Given
regular expression £, is E equivalent to &*, where ¥ is the set of symbols that
appear in £? Hint: Instead of trying to reduce QBF to this problem, it inight
be easier to show that any language in PS reduces to it. For each polynomial-
space-bounded TM Jf, show how to take an input w for Af and construct in
polynomial time a regular expression that generates all strings that are not
sequences of ID's of Af leading to acceptance of w.

Exercise 11.3.3: The Shannon Switching Game is as follows. We are given
a graph G with two terminal nodes s and #. There are two players, which we
may call SHORT and CUT. Alternately, with SHORT playing first, each player
selects a vertex of G, other than s and ¢, which then belongs to that player for
the rest of the game. SHORT wins by selecting a set of nodes that, with s and é,
form a path in G from s tot. CUT wins if all the nodes have been selected, and
SHORT has not selected a path from s to t. Show that the following problem is
PS-complete: given G, can SHORT win no matter what choices CUT makes?

11.4 Language Classes Based on Randomization

We now turn our attention to two classes of languages that. are defined by Tur-
ing machines with the capability of using random numbers in their calculation.
You are probably familiar with algorithms written in common programming
languages that use a random-number generator for some useful purpose. Tech-
nically, the function rand) or similarly named function that returns to you


--- Page 504 ---
488 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

what appears to be a “random” or unpredictable number in fact executes a
specific algorithm that can be simulated, although it is very hard to see a “pat-
tern” in the sequence of numbers it produces. A simple example of such a
function (not used in practice) would be a process of taking the previous in-
teger in the sequence, squaring it, and taking the middle bits of the product.
Numbers produced by a complex, mechanical process such as this are called
pseudo-random numbers.

In this section, we shail define a type of Turing machine that models the
gencration of random numbers and the use of those numbers in algorithms. We
then define two classes of languages, RP and ZPP, that use this randomness
and a polynomial time bound in different ways. Interestingly, these classes
appear to include little that is not in P, but the differences are important. In
particular, we shall see in Section 11.5 how some of the most essential matters
regarding computer security are really questions about the relationship of these
classes to P and NP.

11.4.1 Quicksort: an Example of a Randomized

Algorithm
You are probably familiar with the sorting algorithm called “Quicksort.” The
essence of the algorithm is as follows. Given a list of elements @1,42,...,an to

sort, we pick one of the elements, say a,, and divide the list into those elernents
that are a, or less and those that are larger than a). The selected element is
called the pivot. If we are careful with how the data is represented, we can
separate the list of length n into two lists totaling n in length in time O(n).
Moreover, we can then recursively sort the list of low (less than or equal to
the pivot) elements and sort the list of high (greater than the pivot) elements
independently, and the result will be a sorted list of all n elements.

If we are lucky, the pivot will turn out to be a number in the middie of the
sorted list, so the two sublists are each about 7/2 in length. If we are lucky at
each recursive stage, then after about log, n levels of recursion, we shall have
lists of length 1, and these lists are already sorted. Thus, the total work will be
O(log 7) levels, each with O(n) work required, or O(nlogn) time overall.

However, we may not be lucky. For example, if the list happens to be sorted
to begin with, then picking the first element of each list will divide the list with
one element in the low sublist and all the rest in the high sublist. If that is the
case, Quicksort behaves much like Selection-Sort, and takes time proportional
to n? to sort n elements.

Thus, good implementations of Quicksort do not take mechanically any
particular position on the list as the pivot. Rather, the pivot is chosen randomly
from among all the elements on the list. That is, each of the n elements has
probability 1/n of being chosen as the pivot. While we shall not show this
claim here,° it turns out that the expected running time of Quicksort with this

5A proof and analysis of Quicksort’s expected running time can be found in D. E. Knuth,
The Art of Computer Programming, Vol. [1i: Sorting and Searching, Addison-Wesley, 1973.


--- Page 505 ---
11.4. LANGUAGE CLASSES BASED ON RANDOMIZATION 489

randomization included is O(n login). However, since by the tiniest of chances
each of the pivot. choices could take the largest or smallest element, the worst-
case running time of Quicksort is still O(n}. Nevertheless, Quicksort is still the
method of choice in many applications (it is used in the UNIX sort command,
for example), since its expected running time is really quite good compared with
other approaches, even with methods that are O(n logn) in the worst case.

11.4.2 A Turing-Machine Model Using Randomization

To represent abstractly the ability of a Turing machine to make random choices,
much like a program that calls a random-number generator one or more times,
we shall use the variant of a multitape TM suggested in Fig. 11.6. The first tape
holds the input, as is conventional for a multitape TM. The second tape also
begins with nonblanks in its cells. In fact, in principle, its entire tape is covered
with 0’s and 1’s, each chosen randomly and independently with probability 1/2
of a 0 and the same probability of a 1. We shall refer to the second tape as
the random tape. The third and subsequent tapes, if used, are initially blank
and are used as “scratch tapes” by the TM if needed. We call this TM model
a randomized Turing machine.

Finite
control

Random bits ... 001010001010010000100@1111 ...

Scratch tape(s)

Figure 11.6: A Turing machine with the capability of using randomly “gener-
ated” numbers

Since it may not be realistic to imagine that we initialize the randomized
TM by covering an infinite tape with random 0’s and 1’s, an equivalent view of
this TM is that. the second tape is initially blank. However, when the second
head is scanning a blank, an internal “coin flip” occurs, and the randomized


--- Page 506 ---
490 CHAPTER It. ADDITIONAL CLASSES OF PROBLEMS

TM immediately writes either a 0 or a 1 on the tape cell scanned and leaves
it there forever without change. In that way, there is no work — certainly not
infinite work — done prior to starting the randomized TM. Yet the second tape
appears to be covered with random 0’s and 1’s, since those random bits appear
wherever the randomized TM’s second tape head actually looks.

Example 11.12: We can implement the randomized version of Quicksort on a
randomized TM. The important step is the recursive process of taking a sublist,
which we assume is stored consecutively on the input tape and delineated by
markers at both ends, picking a pivot at random, and dividing the sublist into
low and high sub-sublists. The randomized TM does as follows:

1. Suppose the sublist to be divided is of length m. Use about O(log m)
new random bits on the second list to pick a random number between 1
and mn; the mth element of the sublist becomes the pivot. Note that we
may not be able to choose every integer between 1 and m with absolutely
equal probability, since m not be a power of 2. However, if we take, say
[2 log, m] bits from tape 2, think of it as a number in the range 0 to about
m°, take its remainder when divided by m, and add 1, then we shall get
all numbers between 1 and m with probability that is close enough to
L/m to make Quicksort work properly.

2. Put the pivot on tape 3.

3. Scan the sublist delineated on tape 1, copying those that are no greater
than the pivot to tape 4.

4. Again scan the sublist on tape 1, copying those elements greater than the
pivot to tape 5.

. Copy tape 4 and then tape 5 to the space on tape | that formerly held
the delineated sublist. Place a marker between the two lists.

qn

6. If either or both of the sub-sublists have more than one element, recur-
sively sort them by the same algorithm.

Notice that this implementation of Quicksort takes O(n log n) time, even though
the computing device is a multitape TM, rather than a conventional computer.
However, the point of this example is not the running time but rather the use
of the random bits on the second tape to cause random behavior of the Turing
machine. O

11.4.3 The Language of a Randomized Turing Machine

We are used to a situation where every Turing machine (or FA or PDA for
that matter) accepts some language, even if that language is the empty set or
the set. of all strings over the input alphabet. When we deal with randomized
‘Turing machines, we need to be more careful about what it means for the TM


--- Page 507 ---
11.4. LANGUAGE CLASSES BASED ON RANDOMIZATION 491

to accept an input, and it becomes possible that a randomized TM accepts no
language at all. The problem is that when we consider what a randomized TM
M does in response to an input w, we need to consider M with all possible
contents for the random tape. It is entirely possible that M accepts with some
random strings and rejects with others; in fact, if the randomized TM is to do
anything more efficiently than a deterministic TM, it is essential that different
contents of the randomized tape lead to different behaviors.°

If we think of a randomized TM as accepting by entering a final state, as
for a conventional TM, then each input w to the randomizcd TM Af has some
probability of acceptance, which is the fraction of the possible contents of the
random tape that lead to acceptance. Since there are an infinite number of
possible tape contents, we have to be somewhat careful computing this proba-
bility. However, any sequence of moves leading to acceptance looks at only a
finite portion of the random tape, so whatever is seen there occurs with a finite
probability equal to 2—~™ if m is the number of cells of the random tape that
have been scanned and influenced at least one move of the TM. An example
will illustrate the calculation in a very simple case.

Example 11.13: Our randomized TM M has the transition function displayed
in Fig. 11.7. M uses only an input tape and the random tape. It behaves in
a very simple manner, never changing a symbol on either tape, and moving
its heads only to the right (direction R) or keeping them stationary (direction
S). Although we have not defined a formal notation for the transitions of a
randomized TM, the entries in Fig. 11.7 should be understandable; cach row
corresponds to a state, and each column corresponds to a pair of symbols XY,
where X is the symbol scanned on the input tape, and Y is the symbol scanned
on the random tape. The entry in the table gi’) V DE means that the TM enters
state g, writes U/ on the input tape, writes V on the random tape, inoves the
input head in direction D, and moves the head of the random tape in direction
E.

00 01 10 11 BO Bl
so | MOORS g301SR ql0RS g,11SR
qm | qO0RS ga B0Ss
d2 ql ORS aBOsSS
q3 q@O00RR g3 11AR ga BOSS BSS
*Q4

Figure 11.7: The transition function of a randomized Turing machine

8You should be aware that the randomized ‘'M described in Example 11.12 is not a
language-recognizing TM. Rather, it performs a transformation on its input, and the running
time of the transformation, although not the outcome, depends on what was on the random
tape.


--- Page 508 ---
492 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

Here is a summary of how M behaves on an input string w of 0’s and 1’s.
In the start state, gg, Af looks at the first random bit, and makes one of two
tests regarding w, depending on whether that random bit is 0 or 1.

If the random bit is 0, then Mf tests whether or not w consists of only one
symbol — 0 or 1. In this case, M looks at no more random bits, but keeps its
second tape head stationary. If the first bit of w is 0, then Af goes to state gy.
In that state, Jf moves right over 0's, but dies if it sees a 1. If M reaches the
first blank on the input tape while in state q,, it goes to state q4, the accepting
state. Similarly, if the first bit of w is 1, and the first random bit is 0, then
M goes to state gz; in that state it checks if all the other bits of w are 1, and
accepts if so.

Now, let us consider what M does if the first random bit is 1. It compares
w with the second and subsequent random bits, accepting only if they are the
same. Thus, in state go, scanning 1 on the second tape, M goes to state gz.
Notice that when doing so, Mf moves the random-tape head right, so it gets to
see a new random bit, while keeping the input-tape head stationary so all of
w will be compared with random bits. In state g3, Jf matches the two tapes,
moving both tape heads right. If it finds a mismatch at some point, it dies and
fails to accept, while if it reaches the blank on the input tape, it accepts.

Now, let us compute the probability of acceptance of certain inputs. First,
consider a homogeneous input, one that consists of only one symbol, such as 0*
for some i > 1, With probability 1/2, the first random bit will be 0, and if so,
then the test for homogeneity will succeed, and 0! is surely accepted. However,
also with probability 1/2 the first random bit is 1. In that case, 0* will be
accepted if and only if random bits 2 through 7 + 1 are all 0. That occurs with
probability 2—'. Thus, the total probability of acceptance of 0! is

1 1oi_ 1 -(41)
gt52=5+2

Now, consider the case of a heterogeneous input w, i.e., an input that consists
of both 0’s and 1’s, such as 00101. This input is never accepted if the first
random bit is 0. If the first random bit is 1, then its probability of acceptance is
2-? where i is the length of the input. Thus, the total probability of acceptance
of a heterogeneous input of length ¢ is 2-*+1). For instance, the probability of
acceptance of 00101 is 1/64. O

Our conclusion is that we can compute a probability of acceptance of any
given string by any given randomized TM. Whether or not the string is in the
language depends on how “membership” in the language of a randomized TM
is defined. We shall give two different definitions of acceptance in the next
sections, each leads to a different class of languages.

11.4.4 The Class RP

The essence of our first class of languages, called RP, for “random polynomial,”
is that to be in RP, a language L must be accepted by a randomized TM M


--- Page 509 ---
11.4. LANGUAGE CLASSES BASED ON RANDOMIZATION 493

Nondeterminism and Randomness

There are some superficial similarities between a randomized TM and a
nondeterministic TM. We could imagine that the nondeterministic choices
of a NTM are governed by a tape with random bits, and every time the
NTM has a choice of moves it consults the random tape and picks from

among the choices with equal probability. However, if we interpret an
NTM that way, then the acceptance rule is rather different from the rule
for RP. Tastead, an input is rejected if its probability of acceptance is
0, and the input is accepted if its probability of acceptance is any value
greater than 0, no matter how small.

in the following sense:
1. If w is not in L, then the probability that Af accepts w is 0.
2. If w is in L, then the probability that Af accepts w is at least 1/2.

3. There is a polynomial T(n) such that if input w is of length n, then all
runs of AM, regardless of the contents of the random tape, halt after at
most T'(n) steps.

Notice that there are two independent issues addressed by the definition
of RP. Points (1) and (2) define a randomized Turing machine of a special
type, which is sometimes called a Monte-Carlo algorithm. That is, regardless
of running time, we may say that a randomized TM is “Monte-Carlo” if it either
accepts with probability 0 or accepts with probability at least 1/2, with nothing
in between. Point (3) simply addresses the running time, which is independent
of whether or not the TM is “Monte-Carlo.”

Example 11.14: Consider the randomized TM of Example 11.13. It surely
satisfies condition (3), since its running time is O(n) regardless of the contents of
the random tape. However, it does not accept any language at all, in the sense
required by the definition of RP. The reason is that, while the homogeneous
inputs like 000 are accepted with probability at least 1/2, and thus satisfy
point (2), there are other inputs, like 001, that are accepted with a probability
that is neither 0 nor at least 1/2; e.g., 001 is accepted with probability 1/16.
Oo

Example 11.15: Let us describe, informally, a randomized TM that is both
polynomial-time and Monte-Carlo, and therefore accepts a language in RP.
The input will be interpreted as a graph, and the question is whether the graph
has a triangle, that is, three nodes all pairs of which are connected by edges.
Inputs with a triangle are in the language; others are not.


--- Page 510 ---
494 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

The Monte-Carlo algorithm will repeatedly pick an edge (x,y) at random
and pick a node z, other than zx and y, at random as well. Each choice is
determined by looking at some new random bits from the random tape. For
each x, y, and z selected, the TM tests whether the input holds edges (z, z)
and (y,z), and if so it declares that the input graph has a triangle.

A total of k choices of an edge and a node are made; the TM accepts if any
one of them proves to be a triangle, and if not, it gives up and does not accept.
If the graph has no triangle, then it is not possible that one of the & choices
will prove to be a triangle, so condition (1) in the definition of RP is met: if
the input is not in the language, the probability of acceptance is 0.

Suppose the graph has n nodes and e edges. If the graph has at least one
‘triangle, then the probability that its three nodes wili be selected on any one
experiment is (2)(—1,). That is, three of the e edges are in the triangle, and
if any of these three are picked, then the probability is 1/(m — 2) that the
third node will also be selected. That probability is small, but we repeat the
experiment k times. The probability that at least one of the & experiments will
yield the triangle is:

3 k
! (1 Jay) (114)

There is a commonly used approximation that says for small x, (1 — x)* is
approximately e—**, where e = 2.718--- is the base of the natural logarithms.
Thus, if we pick & such that ka = 1, for example, e~** will be significantly less
than 1/2 and t — e~** will be significantly greater than 1/2, about 0.63, to be
more precise. Thus, we can pick k = e(n — 2)/3 to be sure that the probability
of acceptance of a graph with a triangle, as given by Equation 11.4, is at least
1/2. Thus, the algorithm described is Monte-Carlo.

Now, we must consider the running time of the TM. Both e and n are no
greater than the input length, and & was chosen to be no more than the square of
the length, since it is proportional to the product of e and n. Each experiment,
since it scans the input at most four times (to pick the random edge and node,
and then to check the presence of two more edges), is linear in the input length.
Thus, the TM halts after an amount of time that is at most cubic in the input
length; i.e., the TM has a polynomial running time and therefore satisfies the
third and final condition for a language to be in RP.

We conclude that the language of graphs with a triangle is in the class RP.
Note that this language is also in P, since one could do a systematic search
of all possibilities for triangles. However, as we mentioned at the beginning of
Section 11.4, it is actually hard to find examples that appear to be in RP —P.
o

11.4.5 Recognizing Languages in RP

Suppose now that we have a polynomial-time, Monte-Carlo Turing machine
to recognize a language L. We are piven a string w, and we want to know if


--- Page 511 ---
11.4. LANGUAGE CLASSES BASED ON RANDOMIZATION 495

wis in £. If we run M on ZL, using coin-flips or some other random-number-
generating device to simulate the creation of random bits, then we know:

1. If w is not in £, then our run will surely not lead to acceptance of w.

2. If wis in Z, there is at least a 50% chance that w will be accepted.

However, if we simply take the outcome of this run to be definitive, we shall
sometimes reject w when we should have accepted (a false negative result),
although we shall never accept when we should not (a false positive result).
Thus, we must distinguish between the randomized TM itself and the algorithm
that we use to decide whether or not w is in £. We can never avoid false
negatives altogether, although by repeating the test many times, we can reduce
the probability of a false negative to be as small as we like.

For instance, if we want a probability of false negative of one in a billion,
we may run the test thirty times. If wis in £, then the chance that all thirty
tests will fail to lead to acceptance is no greater than 27°, which is less than
10°, or one in a billion. In general, if we want a probability of false negatives
less than ¢ > 0, we must run the test log,(1/c) times. Since this number is a
constant if ¢ is, and since one run of the randomized TM MM takes polynomial
time because £ is assumed to be in RP, we know that the repeated test also
takes a polynomial amount of time. The implication of these considerations is
stated as a theorem, below.

Theorem 11.16: If Z is in RP, then for any constant c > 0, no matter how
small, there is a polynomial-time randomized algorithm that renders a decision
whether its given input w is in £, makes no false-positive errors, and makes
false-negative errors with probability no greater thane. O

11.4.6 The Class ZPP

Our second class of languages involving randomization is called zero-error, prob-
abilistic, polynomial, or ZPP. The class is based on a randomized TM that
always halts, and has an expected time to halt that is some polynomial in the
length of the input. This TM accepts its input if it enters and accepting state
(and therefore halts at that time), and it rejects its input if it halts without ac-
cepting. Thus, the definition of class ZPP is almost the same as the definition
of P, except that 2? allows the behavior of the TM to involve randomness,
and the expected running time, rather than the worst-case running time is
measured.

A TM that always gives the correct answer, but whose running time varies
depending on the values of some random bits, is sometimes called a Las- Vegas
Turing machine or Las-Vegas algorithm. We may thus think of ZPP as the
languages accepted by Las-Vegas Turing machines with a polynomial expected
running time.


--- Page 512 ---
496 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

Is Fraction 1/2 Special in the Definition of RP?

While we defined RP to require that the probability of accepting a string
w in LE should be at least 1/2, we could have defined RP with any constant
that lies properly between 0 and 1 in place of 1/2. Theorem 11.16 says
that. we could, by repeating the experiment made by M the appropriate
number of times, make the probability of acceptance as high as we like,
up to but not including 1. Further, the same technique for decreasing the
probability of nonacceptance for a string in 2 that we used in Section 11.4.5
will allow us to take a randomized TM with any probability greater than
0 of accepting w in L and boosting that probability to 1/2 by repeating
the experiment some constant number of times.

We shall continue to require 1/2 as the probability of acceptance in
the definition of RP, but we should be aware that any nonzero probability
is suffictent to use in the definition of the class RP. On the other hand,
changing the constant from 1/2 will change the language defined by a
particular randomized TM. For instance, we observed in Example 11.14
how lowering the required probability to 1/16 would cause string 001 to
be in the language of the randomized TM discussed there.

11.4.7 Relationship Between RP and ZPP

There is a simple relationship between the two randomized classes we have
defined. To state this theorem, we first need to look at the complements of the
classes. It should be clear that if L is in ZPP, then so is £. The reason is
that, if £ is accepted by a polynomial-expected-time Las-Vegas TM M, then
EL is accepted by a modification of M in which we turn acceptance by M into
halting without acceptance, and if M halts without accepting, we instead go to
an accepting state and halt.

However, it is not obvious that RP is closed under complementation, be-
cause the definition of Monte-Carlo Turing machines treats acceptance and
rejection asymmetrically. Thus, let us define the class co-RP to be the set
of languages £ such that L is in RP; ie., co-RP is the complements of the
languages in RP.

Theorem 11.17: ZPP = RP 1 co-RP.

PROOF: We first show RP OM co-RP C ZPP. Suppose £ is in RP Nco-RP.
That is, both L and £ have Monte-Carlo TM’s, each with a polynomial running
time. Assume that p(n) is a large enough polynomial to bound the running
times of both machines. We design a Las-Vegas TM M for Z as follows.

1. Run the Monte-Carlo TM for Z£; if it accepts, then M accepts and halts.


--- Page 513 ---
11.4. LANGUAGE CLASSES BASED ON RANDOMIZATION 497

2. If not, run the Monte-Carlo TM for L. If that TM accepts, then M halts
without accepting. Otherwise, Mf returns to step {1).

Clearly, Af only accepts an input w if w is in £, and only rejects w if w
is not in L. The expected running time of one round (an execution of steps 1
and 2) is 2p(n). Moreover, the probability that any one round will resolve the
issue is at least 1/2. If w is in L, then step (1) has a 50% chance of leading
to acceptance by Af, and if w is not in L, then step (2) has a 50% chance of
leading to rejection by M. Thus, the expected running time of Af is no more
than

2p(n) + 52pm) + 22p(n) + Z2p(n) += = pln)

Now, let us consider the converse: assume FE is in ZPP and show L£ is in
both RP and co-RP. We know L is accepted by a Las-Vegas TM Ad, with an
expected running time that is some polynomial p(n). We construct a Monte-
Carlo TM Mg for £ as follows. Ady simulates Mj, for 2p(n) steps. If Ad, accepts
during this time, so does Mo; otherwise Mg rejects.

Suppose that input w of length n is not in L. Then Ad, will surely not accept
w, and therefore neither will M2. Now, suppose w isin L. Ad, will surely accept
w eventually, but it might or might not accept within 2p(n) steps.

However, we claim that the probability M, accepts w within 2p(n) steps is
at least 1/2. Suppose the probability of acceptance of w by Ad; within time
2p(nm) were constant ¢ < 1/2. Then the expected running time of M, on input w
is at least (1—c)2p(n), since 1—c is the probability that Af, will take more than
2p(n) time. However, if ¢ < 1/2, then 2(1 — ¢) > 1, and the expected running
time of M, on w is greater than p(n). We have contradicted the assumption
that Ad, has expected running time at most p(n) and conclude therefore that the
probability Af, accepts is at least 1/2. Thus, Me is a polynomial-time-bounded
Monte-Carlo TM, proving that Z is in RP.

For the proof that £ is also in co-RP, we use essentially the same construc-
tion, but we complement the outcome of A¢,. That is, to accept L, we have At,
accept when A4; rejects within time 2p(n), while Mf2 rejects otherwise. Now,

M2 is a polynomial-time-bounded Monte-Carlo TM for £. O

11.4.8 Relationships to the Classes P and VP

Theorem 11.17 tells us that ZPP C RP. We can place these classes between
P and A'P by the following simple theorems.

Theorem 11,18: P C ZPP.

PROOF: Any deterministic, polynomial-time bounded TM is also a Las- Vegas,
polynomial-time bounded TM, that happens not to use its ability to make
random choices. O

Theorem 11.19: RP CAP.


--- Page 514 ---
498 CHAPTER 11, ADDITIONAL CLASSES OF PROBLEMS

PROOF: Suppose we are given a polynomial-time-bounded Monte-Carlo TM
M, for a language L. We can construct a nondeterministic TM Md, for LE with
the same time bound. Whenever 4, examines a random bit for the first time,
Mz chooses, nondeterministically, both possible values for that bit, and writes
it on a tape of its own that simulates the random tape of M1. M2 accepts
whenever M4, accepts, and does not accept otherwise.

Suppose w is in £. Then since M, has at least a 50% probability of ac-
cepting w, there must be some sequence of bits on its random tape that leads
to acceptance of w. Mp will choose that sequence of bits, among others, and
therefore also accepts when that choice is made. Thus, w is in (M2). However,
if w is not in Z, then no sequence of random bits will make M, accept, and
therefore no sequence of choices makes M2 accept. Thus, w is not in L(M2).
O

Figure 11.8 shows the relationship between the classes we have introduced
and the other “nearby” classes.

Figure 11.8: Relationship of ZPP and RP to other classes

11.5 The Complexity of Primality Testing

In this section, we shall look at a particular problem: testing whether an integer
is a prime. We begin with a motivating discussion concerning the way primes


--- Page 515 ---
11.5. THE COMPLEXITY OF PRIMALITY TESTING 499

and primality testing are essential ingredients in computer-security systems.
We then show that the primes are in both ’P and co-V/P. Finally, we discuss
a randomized algorithm that shows the primes are in RP as well.

11.5.1 The Importance of Testing Primality

An integer p is prime if the only integers that divide p evenly are 1 and p itself.
If an integer is not a prime, it is said to be composite. Every composite number
can be written as a product of primes in a unique way, except for the order of
the factors.

Example 11.20: The first few primes are 2, 3, 5, 7, 11, 13, and 17. The
integer 504 is composite, and its prime factorization is 8% 3?x7. O

There are a number of techniques that enhance computer security, for which
the most common methods in use today rely on the assumption that it is hard
to factor numbers, that is, given a composite number, to find its prime factors.
In particular, these schemes, based on what are called RSA codes (for R. Rivest,
A. Shamir, and L. Adelman, the inventors of the technique), use integers of,
say, 128 bits that are the product of two primes, each of about 64 bits. Here
are two scenarios in which primes play an important part.

Public-Key Cryptography

Yon want to buy a book from an on-line bookseller. The seller asks for your
credit-card number, but it is too risky to type the number into a form and
have the form transmitted over phone lines or the Internet. The reason is that
someone could be snooping on your line, or otherwise intercept packets as they
travel over the Internet.

To avoid a snooper being able to read your card number, the seller sends
your browser a key k, perhaps the 128-bit product of two primes that the
seller's computer has generated just for this purpose. Your browser uses a
function y = fx(z) that takes both the key & and the data z that you need to
encrypt. The function f, which is part of the RSA scheme, may be generally
known, including to potential snoopers, but it is believed that without knowing
the factorization of k, the inverse function f,' such that # = fy 1(y) cannot be
computed in time that is less than exponential in the length of k.

Thus, even if a snooper sees y and knows how f works, without first figuring
out what & is and then factoring it, the snooper cannot recover z, which is in this
case your credit-card number. On the other hand, the on-line seller, knowing
the factorization of key k because they generated it in the first place, can easily
apply f, ' and recover x from y.

Public-Key Signatures

The original scenario for which RSA codes were developed is the following.
You would like to be able to “sign” email so that people could easily determine


--- Page 516 ---
500 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

that the email was from you, and yet no one could “forge” your name to an
email. For instance, you might wish to sign the message « = “I promise to
pay Sally Lee $10,” but you don’t want Sally to be able to create the signed
message herself, or for a third party to create such a signed message without
your knowledge.

To support these aims, you pick a key k, whose prime factors only you know.
You publish & widely, say on your Web site, so anyone can apply the function
fx to any message. If you want to sign the message x above and send it to
Sally, you compute y = fr 1(x) and send y to Sally instead. Sally can get f,,
your public key, from your Web site, and with it compute x = f,(y). Thus, she
knows that you have indeed promised to pay $10.

If you deny having sent the message y, Sally can argue before a judge that
only you know the function f,', and it would be “impossible” for either her or
any third party to have discovered that function. Thus, only you could have
created y. This system relies on the likely-but-unproven assumption that it is
too hard to factor numbers that are the product of two large primes.

Requirements Regarding Complexity of Primality Testing

Both scenarios above are believed to work and to be secure, in the sense that
it really does take exponential time to factor the product of two large primes.
The complexity theory we have studied here and in Chapter 10 enter into the
study of security and cryptography in two ways:

1, The construction of public keys requires that we be able to find large
primes quickly. It is a basic fact of number theory that the probability of
an n-bit number being a prime is on the order of 1/n. Thus, if we had
a polynomial-time (in n, not in the value of the prime itself) way to test
whether an n-bit number was prime, we could pick numbers at random,
test them, and stop when we found one to be prime. That would give
us a polynomial-time Las-Vegas algorithm for discovering primes, since
the expected number of numbers we have to test before meeting a prime
of n bits is about n. For instance, if we want 64-bit primes, we would
have to test about 64 integers on the average, although by bad luck we
could have to try indefinitely more than that. Unfortunately, there does
not appear to be a guaranteed, polynomial-time test for primes, although
there is a Monte-Carlo Algorithm that is polynomial-time, as we shall see
in Section 11.5.4.

2. The security of RSA-based cryptography depends on there being no poly-
nomial (in the number of bits of the key) way to factor in general, in
particular no way to factor a number known to be the product of ex-
actly two large primes. We would be very happy if we could show that
the set of primes is an NP-complete language, or even that the set of
composite numbers was NP-complete. For then, a polynomial factoring
algorithm would prove P = A’P, since it would yield polynomial-time


--- Page 517 ---
11.5. THE COMPLEXITY OF PRIMALITY TESTING 501

tests for both these languages. Alas, we shail see in Section 11.5.5 that
both the primes and the composite numbers are in NP. Since they are
complements of each other, should either be NP-complete, it would fol-
low that VP = co-A’P, which we doubt is the case. Further, the fact
that the set of primes is in RP means that if we could show the primes
to be NP-complete then we could conclude RP = AP, another unlikely
situation.

11.5.2 Introduction to Modular Arithmetic

Before looking at algorithms for recognizing the set. of primes, we shall introduce
some basic concepts regarding modular arithmetic, that is, the usual arithmetic
operations executed modulo some integer, often a prime. Let p be any integer.
The integers module p are 0,1,...,p—1.

We can define addition and multiplication modulo p to apply only to this
set of p integers by performing the ordinary calculation and then computing the
remainder when the result is divided by p. Addition is quite straightforward,
since the sum is either less than p, in which case we have nothing additional to
do, or it is between p and 2p — 2, in which case we subtract p to get an integer
in the range 0,1,...,p— 1. Modular addition obeys the usual algebraic laws;
it is commutative, associative, and has 0 as the identity. Subtraction is still
the inverse of addition, and we can compute the modular difference z ~— y by
subtracting as usual, and adding p if the result is below 0. The negation of z,
which is —2z, is the same as 0 — 2, just as in ordinary arithmetic. Thus, —0 — 0,
and if « 4 0, then —2 is the same as p — z.

Example 11.21: Suppose p = 13. Then 3+-5 = 8, and 7+10=4. To see the
latter, note that in ordinary arithmetic, 7 + 10 = 17, which is not less than 13.
We therefore subtract 13 to get the proper result, 4. The value of ~5 modulo
13 is 13 — 5, or 8. The difference 11 — 4 modulo 13 is 7, while the difference
4—1] is 6. To see the latter, in ordinary arithmetic, 4 — 11 = —7, so we must.
add 13 to get 6. O

Multiplication modulo p is performed by multiplying as ordinary numbers,
and then taking the remainder of the result divided by p. Multiplication also
satisfies the usual algebraic laws; it is commutative and associative, 1 is the iden-
tity, 0 is the annihilator, and multiplication distributes over addition. However,
division by nonzero values is trickier, and even the existence of inverses for in-
tegers modulo p depends on whether or not p is a prime. In general, if x is one
of the integers modulo p, that is, 0 < xz < p, then x!, or 1/z is that number
y, if it exists, such that zy = 1 modulo p.

Example 11.22: In Fig. 11.9 we see the multiplication table for the nonzero
integers modulo the prime 7. The entry in row 7 and column j is the product #7
modulo 7. Notice that each of the nonzero integers has an inverse; 2 and 4 are
each other’s inverses, so are 3 and 5, while 1 and 6 are their own inverses. That


--- Page 518 ---
502 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

Figure 11.9: Multiplication modulo 7

is, 2x 4,3x 5,1 1, and 6 x 6 are all 1. Thus, we can divide by any nonzero
number x/y by computing y~* and then multiplying x x y-!. For instance,
3/4=3x4'=3x2=6.

Figure 11.10: Multiplication modulo 6

Compare this situation with the multiplication table modulo 6. First, we
observe that only 1 and 5 even have inverses; they are each their own inverse.
Other numbers have no inverse. In addition, there are numbers that are not
0, but whose product is 0, such as 2 and 3. That situation never occurs for
ordinary integer arithmetic, and it never happens when arithmetic is modulo a
prime. QO

There is another distinction between multiplication modulo a prime and
modulo a composite number that turns out to be quite important for primality

tests. The degree of a number @ modulo p= is the smallest positive power of a
that is equal to 1. Some useful facts, which we shall not prove here are:

e If pis a prime, then a?~'! = 1 modulo p. This statement is called Fermat’s
theorem.*

e The degree of a modulo a prime p is always a divisor of p-1.

¢ If pis a prime, there is always some a that has degree p — 1 modulo p.

*Do not confuse Fermat's theorem with “Fermat ‘a laat theorem,” which asserts the nonex-
istence of integer solutions to #* + y™ = z” forn > 3.


--- Page 519 ---
11.5. THE COMPLEXITY OF PRIMALITY TESTING 503

Example 11.23: Consider again the raultiplication table modulo 7 in Fig.
11.9. The degree of 2 is 3, since 2? = 4, and 2* = 1. The degree of 3 is 6, since
3? = 2, 3? = 6, 34 = 4, 3° = 5, and 3° = 1. By similar calculations, we find
that 4 has degree 3, 5 has degree 6, 6 has degree 2, and 1 has degree 1. O

11.5.3 The Complexity of Modular-Arithmetic
Computations

Before proceeding to the applications of modular arithmetic to primality testing,
we must establish some basic facts about the running time of the essential
operations. Suppose we wish to compute modulo some prime p, and the binary
representation of p is n bits long; i.¢., p itself is around 2". As always, the
running time of a computation is stated in terms of n, the input length, rather
than p, the “value” of the input. For instance, counting up to p takes time
O(2"), so any computation that involves p steps, will not be polynomial-time,
as a function of n.

However, we can surely add two numbers modulo p in O(n) time on a typical
computer or multitape TM. Recall that we simply add the binary numbers,
and if the result is p or greater, then subtract p. Likewise, we can multiply
two numbers in O(n?) time, either on a computer or a Turing machine. After
multiplying the numbers in the ordinary way, and getting a result of at most
2n bits, we divide by p and take the remainder.

Raising a number z to an exponent is trickier, since that exponent may itself
be exponential in n. As we shall see, an important step is raising x to the power
p—1. Since p—1 is around 2”, if we were to multiply x by itself p—2 times, we
would need O(2") multiplications, and even though each multiplication involved
only n-bit numbers and could be carried out in O(n?) time, the total time would
be O(n?2), which is not polynomial in n.

Fortunately, there is a “recursive-doubling” trick that lets us compute 2?—*
(or any other power of z up to p) in time that is polynomial in n:

1. Compute the at most n exponents «,27,z4,2°,... , until the exponent
exceeds p— 1. Each value is an n-bit number that is computed in O(n’)
time by squaring the previous value in the sequence, so the total work is

O(n).

2. Find the binary representation of p—1, say p—1 = Gp—1---@idg9. We can
write
p-—l=agt 2a; + dag +--+ 2" apa
where each a; is either 0 or 1. Therefore,

Pt — pt0t2a1t4agt +2" ~tened

which is the product of those values x?’ for which a; = 1. Since we
computed each of those #?’’s in step (1), and each is an n-bit number, we
can compute the product of these n or fewer numbers in O(n") time.


--- Page 520 ---
504 CHAPTER i1. ADDITIONAL CLASSES OF PROBLEMS
Thus, the entire computation of z?—! takes O{n*) time.

11.5.4 Random-Polynomial Primality Testing

We shall now discuss how to use randomized computation to find large prime
numbers. More precisely, we shall show that the language of composite numbers
is in RP. The method actually used to generate n-bit primes is to pick an n-bit
number at random and apply the Monte-Carlo algorithm to recognize composite
numbers some large number of times, say 50. If any test says that the number
is composite, then we know it is not a prime. If all 50 fail to say that it is
composite, there is no more than 2~*° probability that it really is composite.
Thus, we can fairly safely say that the number is prime and base our secure
operation on that fact.

We shall not give the complete algorithm here, but rather discuss an idea
that works except in a very small number of cases. Recall Fermat’s theorem
tells us that if p is a prime, then z?-! modulo p is alwayg 1. It is also a fact
that if p is a composite number, and there is any « at all for which z?-! modulo
p is not 1, then for at least half the values of 2 in the range 1 to p — 1, we shall
find 2?—' £1.

Thus, we shall use as our Monte-Carlo algorithm for the composite numbers:

1. Pick an z at random in the range 1 to p— 1.

2. Compute z?-! modulo p. Note that if p is an n-bit number, then this
calculation takes O(n?) time by the discussion at the end of Section 11.5.3.

3. If 2-1 4 1 modulo p, accept; x is composite. Otherwise, halt without
accepting.

If p is prime, then x?—’ = 1, so we always halt without accepting; that is one
part of the Monte-Carlo requirement, that if the input is not in the language,
then we never accept. For almost all the composite numbers, at least half the
values of ¢ will have z?—! 4 1, so we have at least 50% chance of acceptance on
any one run of this algorithm; that is the other requirement for an algorithm
to be Monte-Carlo.

What we have described so far would be a demonstration that the compos-
ite numbers are in RP, if it were not for the existence of a small number of
composite numbers ¢ that have z°-! = 1 modulo ¢, for the majority of z in
the range 1 to c— 1, in particular for those z that do not share a common
prime factor with ¢. These numbers, called Carmichael numbers, require us to
do another, more complex test (which we do not describe here) to detect that
they are composite. The smallest Carmichael number is 561. That is, one can
show 2°69 = 1 modulo 561 for all x that are not divisible by 3, 11, or 17, even
though 561 = 3 x 11 x 17 is evidently composite. Thus, we shall claim, but
without a complete proof, that:

Theorem 11.24: The set of composite numbers isin RP. O


--- Page 521 ---
11.5. THE COMPLEXITY OF PRIMALITY TESTING 505

Can We Factor in Random Polynomial Time?

Notice that the algorithm of Section 11.5.4 may tell us that a number is
composite, but. does not tell us how to factor the composite number. It is
believed that there is no way to factor numbers, even using randomness,
that takes only polynomial time, or even expected polynomial time. If
that assumption were incorrect, then the applications that we discussed
in Section 11.5.1 would be insecure and could not be used.

11.5.5 Nondeterministic Primality Tests

Let us now take up another interesting and significant result, about testing pri-
mality: that the language of primes is in WP M co-NVP. Therefore the language
of composite numbers, the complement of the primes, is also in NP NN coNP.
The significance of this fact is that it is unlikely to be the case that the primes or
the composite numbers are NP-complete, for if either were true then we would
have the unexpected equality MP = co-NP.

One part is easy: the composite numbers are obviously in AP, so the primes
are in co. P. We prove that fact first.

Theorem 11.25: The set of composite numbers is in VP.

PROOF: The nondeterministic, polynomial-time algorithm for the composite
numbers is:

1. Given an n-bit number p, guess a factor f of at most n bits. Do not choose
f =1or f =p, however. This part is nondeterministic, with all possible
values of f being guessed along some sequence of choices. However, the
time taken by any sequence of choices is O(n).

2. Divide p by f, and check that the remainder is 0. Accept if so. This part
is deterministic and can be carried out in time O(n”) on a multitape TM.

If p is composite, then it must have at least one factor f other than 1 and p.
The NTM, since it guesses all possible numbers of up to n bits, will in some
branch guess f. That branch leads to acceptance. Conversely, acceptance by
the NTM implies that a factor of p other than 1 or p itself has been found.
Thus, the NTM described accepts the language consisting of all and only the
composite numbers. O

Recognizing the primes with a NTM is harder. While we were able to
guess a reason (a factor) that a number is not a prime, and then check that
our guess is correct, how do we “guess” a reason a number is a prime? The
nondeterministic, polynomial-time algorithm is based on the fact (asserted but
not proved) that if p is a prime, then there is a number x between 1 and p— 1


--- Page 522 ---
506 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

that has degree p—1. For instance, we observed in Example 11.23 that for the
prime p = 7, the numbers 3 and 5 both have degree 6.

While we could guess a number =z easily, using the nondeterministic capa-
bility of a NTM, it is not immediately obvious how one then checks that 2 has
degree p—1. The reason is that if we apply the definition of “degree” directly,
we need to check that none of 2”, 2°,...,2?-? are 1. To do so requires that we
perform p—3 multiplications, and that requires time at least 2”, if p is an n-bit
number.

A better strategy is to make use of another fact that we assert but do not
prove: the degree of « modulo a prime p is a divisor of p— 1. Thus, if we knew
the prime factors of p — 1,8 it would be sufficient to check that ¢(?-!)/¢ # 1 for
each prime factor ¢ of p— 1. If none of these powers of z is equal to 1, then
the degree of z must be p— 1. The number of these tests is O(n), so we can
perform them all in a polynomial-time algorithm. Of course we cannot factor
p— 1 into primes easily. However, nondeterministically we can guess the prime
factors of p — 1, and:

a) Check that their product is indeed p — 1.

b) Check that each is a prime, using the nondeterministic, polynomial-time
algorithm that we have been designing, recursively.

The details of the algorithm, and the proof that it is nondeterministic, poly-
nomial-time, are in the proof of the theorem below.

Theorem 11.26: The set of primes is in WP.

PROOF: Given a number p of n bits, we do the following. First, if n is no more
than 2 (i.e., p is 1, 2, or 3), answer the question directly; 2 and 3 are primes,
while 1 is not. Otherwise:

1. Guess a list of factors (q1,¢@2,...,@%), whose binary representations total
at most 27 bits, and none of which has more than n — 1 bits. It is
permitted for the same prime to appear several times, since p ~ 1 may
have a factor that is a prime raised to a power greater than 1; e.g., if
p = 13, then the prime factors of p — 1 = 12 are in the list (2,2,3). This
part is nondeterministic, but each branch takes O(n) time.

2. Multiply the q’s together, and verify that their product is p—1. This part
takes no more than O(n?) time and is deterministic.

3. If their product is p— 1, recursively verify that each is a prime, using the
algorithm being described here.

®Notice that if p is a prime, then p— 1 is never a prime, except in the uninteresting case
p= 4. The reason is that all primes but 2 are odd.


--- Page 523 ---
11.5. THE COMPLEXITY OF PRIMALITY TESTING 50F

4, If the q’s are all prime, guess a value of x and check that 2'?-1)/@ 4 1 for
any of the g,’s. This test assures that « has degree p— 1 modulo p, since if
it did not, then its degree would have to divide at least one (p—1)/q;, and
we just verified that it did not. Note in justification that any 2, raised to
any power of its degree, must be 1. The exponentiations can be done by
the efficient method described in Section 11.5.3. Thus, there are at most
k exponentiations, which is surely no more than n exponentiations, and
each one can be performed in O(n*) time, giving us a total time of O(n4)
for this step.

Lastly, we must verify that this nondeterministic algorithm is polynomial-
time. Fach of the steps except the recursive step (3) takes time at most Ofn*)
along any nondeterministic branch. While this recursion is complicated, we can
visualize the recursive calls as a tree suggested by Fig. 11.11. At the root is
ihe prime p of n bits that we want to verify. The children of the root are the
qj’s, which are the guessed factors of p—1 that we must also verify are primes.
Below each g; are the guessed factors of gj — 1 that we must verify, and so on,
until we get. down to numbers of at most 2 bits, which are leaves of the tree.

Root level aN
Level 1 q, 4, ene jx one
Level 2 / \

/

Figure 11.11: The recursive calls made by the algorithm of Theorem 11.26 form
a tree of height and width at most 2

Since the product of the children of any node is less than the value of the
node itself, we see that the product of the values of nodes at any depth from the
root is at most p. The work required at a node with value i, exclusive of work
done in recursive calls, is at most a(log,7)* for some constant a; the reason is
that we determined this work to be on the order of the fourth power of the
number of bits needed to represent that value in binary.

Thus, to get an upper bound on the work required by any one level, we must
maximize the sum }7, a(logs ( ay, subject to the constraint that the product
ijig--- is at most p. Because the fourth power is convex, the maximum occurs
when all of the value is in one of the i;’s. If i, = p, and there are no other i;’s,
then the sum is a(log, p)*. That is at most an*, since n is the number of bits
in the binary representation of p, and therefore log, p is at most n.


--- Page 524 ---
508 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

Our conclusion is that the work required at each depth is at most O(n‘).
Since there are at most. n levels, O(n?) work suffices in any branch of the
nondeterministic test for whether pis prime. O

Now we know that both the primes and their complement are in WP. If
either were NP-complete, then by Theorem 11.2 we would have a proof that
NP = coNP.

11.5.6 Exercises for Section 11.5
Exercise 11.5.1: Compute the following modulo 13:
a) 11+9.
*b) 9-11.
ch 5x8.
* d) 5/8.
e) 5%,

Exercise 11.5.2: We claimed in Section 11.5.4 that for most values of x be-
tween 1 and 560, 2°°° = 1 modulo 561. Pick some values of x and verify that
equation. Be sure to express 560 in binary first, and then compute z?' modulo
561, for various values of j, to avoid doing 559 multiplications, as we discussed
in Section 11.8.3.

Exercise 11.5.3: An integer z between 1 and p— 1 is said to be a quadratic
residue modulo p if there is some integer y between 1 and p—1 such that y? = =.

* a) What are the quadratic residues modulo 7? You may use the table of
Fig. 11.9 to help answer the question.

b) What are the quadratic residues modulo 13?

'c) Show that if pis a prime, then the number of quadratic residues modulo p
is (p— 1)/2; ie., exactly half the nonzero integers modulo p are quadratic
residues. Hint: Examine your data from parts (a} and (b). Do you see
a pattern explaining why every quadratic residue is the square of two
different numbers? Could one integer be the square of three different
numbers when p is 4 prime?

11.6 Summary of Chapter 11

+ The Class co-NP: A language is said to be in co-A’P if its complement
is in NP. All languages in P are surely in co-NP, but it is likely that
there are some languages in AP that are not in co-A’P, and vice-versa.
In particular, the NP-complete problems do not appear to be in co-N’P.


--- Page 525 ---
11.6.

SUMMARY OF CHAPTER 11 509

The Class PS: A language is said to be in PS (polynomial space) if it
is accepted by a deterministic TM for which there is a polynomial p(n)
such that on input of length » the TM never uses more than p(7) cells of
its tape.

The Class NPS: We can also define acceptance by a nondeterministic
TM whose tape-usage is limited by a polynomial function of its input
length. The class of these languages is referred to as NPS. However,
Savitch’s theorem tells us that PS = NPS. In particular, a NTM with
space bound p(n) can be simulated by a DTM using space p(n).

Randomized Algorithms and Turing Machines: Many algorithms use ran-
domness productively. On a real computer, a random-number generator
is used to simulate “coin-flipping.” A randomized Turing machine can
achieve the same random behavior if it is given an additional tape on
which a sequence of random bits is written.

The Class RP: A language is accepted in random polynomial time if
there is a polynomial-time, randomized Turing machine that has at least
50% chance of accepting its input if that input is in the language. If the
input is not in the language, then this TM never accepts. Such a TM or
algorithm is called “Monte-Carlo.”

The Class ZPP: A language is in the class of zero-error, probabilistic
polynomial time if it is accepted by a randomized Turing machine that
always gives the correct decision regarding membership in the language;
this TM must run in expected polynomial time, although the worst case
may be greater than any polynomial, Such a TM or algorithm is called
“Las Vegas.”

Relationships Among Language Classes: The class co-R?P is the set of
complements of languages in RP. The following containments are known:
PC ZPP C(RPMcoRP). Also, RP C NP and therefore co-RP C
coNP.

The Primes and NP: Both the primes and the complement of the lan-
guage of primes — the composite numbers — are in NP. These facts
make it unlikely that the primes or composite numbers are NP-complete.
Since there are important cryptographic schemes based on primes, such a
proof would have offered strong evidence of their security.

The Primes and RP: The composite numbers are in RP. The random-
polynomial algorithm for testing compositeness is in common use to allow
the generation of large primes, or at least large numbers that have an
arbitrarily small chance of being composite.


--- Page 526 ---
510 CHAPTER 11. ADDITIONAL CLASSES OF PROBLEMS

11.7 References for Chapter 11

Paper [2] initiated the study of classes of languages defined by bounds on the
amount of space used by a Turing machine. The first PS-complete prob-
lems were given by Karp [4] in his paper that explored the importance of
NP-completeness. The PS-completeness of the problem of Exercise 11.3.2 —
whether a regular expression is equivalent to 5* — is from there.

PS-completeness of quantified boolean formulas is unpublished work of L. J.
Stockmeyer. PS-completeness of the Shannon switching game (Exercise 11.3.3)
is from [1].

The fact that the primes are in A/P is by Pratt [9]. The presence of the
composite numbers in RP was first shown by Rabin [10]. Interestingly, there
was published at about the same time a proof that the primes are actually in
P, provided that an unproved, but generally believed, assumption called the
extended Riemann hypothesis is true (6].

Several books are available to extend your knowledge of the topics intro-
duced in this chapter. [7] covers randomized algorithms, including the complete
algorithms for primality testing. [5] is a source for the algorithms of modular
arithmetic. [3] and [8] treat a number of other complexity classes not mentioned
here.

1. S. Even and R. E. Tarjan, “A combinatorial problem which is complete
for polynomial space,” J. ACM 23:4 (1976), pp. 710-719.

2. J. Hartmanis, P. M. Lewis IT, and R. E. Stearns, “Hierarchies of memory
limited computations,” Proc. Sith Annual IEEE Symposium on Switch-
ing Circuit Theory and Logical Design (1965), pp. 179-190.

3. J. E. Hopcroft and J. D. Uliman, Introduction to Automata Theory, Lan-
guages, and Computation, Addison-Wesley, Reading MA, 1979.

4. R. M. Karp, “Reducibility among combinatorial problems,” in Complexity
of Computer Computations (R. E. Miller, ed.}, Plenum Press, New York,
1972, pp. 85-104.

5. D. E. Knuth, The Art of Computer Programming, Vol. I: Seminumerical
Algorithms, Addison-Wesley, Reading MA, 1997 (third edition).

6. G. L. Miller, “Riemann’s hypothesis and tests for primality,” J. Computer
and System Sciences 13 (1976), pp. 300-317.

7. R. Motwani and P. Raghavan, Randomized Algorithms, Cambridge Univ.
Press, 1995.

8. C. H. Papadimitriou, Computational Complezity, Addison-Wesley, Read-
ing MA, 1994.

9. V. R. Pratt, “Every prime has a succinct certificate,” SIAM J. Computing
4:3 (1975), pp. 214-220.


--- Page 527 ---
11.7%. REFERENCES FOR CHAPTER 11 541

10.

11.

12.

M. O. Rabin, “Probabilistic algorithms,” in Algorithms and Complesity:
Recent Results and New Directions (J. F. Traub, ed.), pp. 21-39, Aca-
demic Press, New York, 1976.

R. L. Rivest, A. Shamir, and L. Adleman, “A method for obtaining digital
signatures and public-key cryptosystems,” Communications of the ACM
21 (1978), pp. 120-126.

W. J. Savitch, “Relationships between deterministic and nondeterministic
tape complexities,” J. Computer and System Sciences 4:2 (1970), pp. 177-
192.


--- Page 528 ---


--- Page 529 ---
Index

A

Acceptance by empty stack 230-235,
249-250

Acceptance by final state 229-23,
249-250

Accepting state 46, 57, 222, 319

Accessible state 45

Ackermann’s function 38]

Address, of memory 357-358

Adelman, L. 499, 511

Aho, A. V. 35, 123, 217

Algebra 85-86, 114-120

Algorithm

See Recursive language

Alphabet 28-29, 132

Alphabetic character 109

Alphanumeric character 109

Alt, of languages 147, 292

Ambiguous grammar 205-212, 249—
251, 302, 404-406

Ancestor 182

Annihilator 95, 115

Arithmetic expression 238-26, 208-
210

Associative law 114-115

Automaton 26-28

See also Counter machine, De-

terministic finite automa-
ton, Finite automaton, Non-
deterministic finite automa-
ton, Pushdown automaton,
Stack machine, Turing ma-
chine

513

B

Backus, J. W. 217

Balanced parentheses 192-193

Bar-Hillel, Y. 166, 304, 411

Basis 19, 22-23

Blank 318-319, 346

Block, of a partition 161

Body 171

Boolean expression 426-428, 436
See also Quantified boolean for-

mula
Borosh, I. I. 468
Bottom-of-stack marker 351

Cc

Cantor, D. C. 217, 411
CFG
See Context-free grammar
CFL
See Context-free language
Character class 108
Child 382
Chomsky, N. 1, 191, 217, 266, 411
Chomsky normal form 266-269, 295-
296
Church, A. 318, 366
Church-Turing thesis 318
Clause 436
Clique problem 462, 465
Closure 85, 87, 104, 109, 117, 199,
284, 382, 425-426
See also e-closure
Closure property 131
See also Alt, of languages, Clo-
sure, Complementation, Con-


--- Page 530 ---
514

catenation, Cycle, of a lan-
guage, Derivative, Differ-
ence, of languages, Homeo-
morphism, Init, of a lan-
guage, Intersection, Inverse
homomorphism, Max, of a
language, Min, of a language,
Partial-removal operation,
Permutation, of a language,
Quotient, Reversal, Shuf-
fle, of languages, Substitu-
tion, Union

CNF

See Conjunctive normal form

Cobham, A. 468

Cocke, J. 298, 304

Code, for Turing machine 369-370

Coloring problem 462-463

Commutative law 114-115

Complementation 132-133, 289, 375-
377, 388, 426

Composite number 499

Computer 315, 355-363

Concatenation 30, 84, 86-87, 95, 104,
115-116, 199, 284, 382, 425—
426

Conclusion 6

Conjunctive normal form 436

See also CSAT

Co-N’P 469-472, 508

Containment, of languages 408

Context-free grammar 4, 169-181,
237-245, 294-296

Context-free language 177, 249

Contradiction, proof by 16-17

Contrapositive 14-16

Converse 16

Cook, S$. C. 1, 424, 468

Cook’s theorem 428-434

Co-RP 496

Countable set 310

Counter machine 351-354

Counterexample 17-19

Cryptography 470, 499-500

CSAT 436-445, 462

INDEX

Cummutative law 14
Cycie, of a language 148, 292
CYK algorithm 298-301

D

Dead state 67
Decidability 5
See also Undecidable problem
Decision property
See Emptiness test, Equivalence,
of languages, Membership

test,
Deductive proof 6-17
é
_ See Transition function
é

See Extended transition func-
tion
Derivation 174-175, 184, 190-191
See also Leftmost derivation, Right-
most derivation
Derivative 146
Descendant 182
Deterministic finite automaton 45-
55, 60-64, 67, 70-72, 79,
91-101, 150-151
Deterministic pushdown automaton
246-251
DFA
See Deterministic finite automa-
ton
DHC
See Directed Hamilton-circuit
problem
Diagonalization 368, 370-372
Difference, of languages 137, 289
Digit 109
Directed Hamilton-circuit problem
453-459, 462
Distinguishable states 154, 157
Distributive law 14, 115-116
Document type definition
See DTD
Dominating-set problem 465
Dot 108


--- Page 531 ---
INDEX

See also Concatenation
DPDA
See Deterministic pushdown au-
tomaton
DTD 169, 192, 199-204
Dynainic programming 298

E

Edge-cover problem 465
Electronic money 38
Emptiness test. 151-152, 296-298
Empty language 31, 86, 95, 102, 115,
117, 384-387
Empty stack
See Acceptance by empty stack
Empty string 29, 86, 102, 115, 117
Endmarker 352, 354 -355
€
See Empty string
e-closure 75
e-NFA 72-80, 96, 101-105, 151
€-production 255, 259-262
e-transition 72, 77-78, 219
Equivalence, of boolean expressions
437
Equivalence, of languages 157-159,
302, 407-408
Equivalence, of regular expressions
117-120
Equivalence, of sets 14, 16
Equivalence, of states 154-157
Even, 8. 510
Evey, J. 253
Exact-cover problem 465
Exponential time 415
Exponentiation 503-504
Expression
See Arithmetic expression, Reg-
ular expression
Extended transition function 49-52,
a4, 58-59, 76-77
Extensible markup language
See XML

F

Factor 209
Factorization 499, 505
False positive/negative 495
Fermat's last theorem 309
Final state
See Acceptance by final state,
Accepting state
Finite automaton 2-4, 37-45, 90,
228, 316
See also Deterministic finite au-
tomaton
Finite control
See State
Finite set 8-9, 339
Firehouse problem 465
Fischer, P. C. 253, 366
Floyd, R. W. 217, 411
For all
See Quantifier

G

Garey, M. R. 468
Generating symbol 256, 258
Ginsburg, 5. 166, 304, 411
Gischer, J. L. 123
Givens

Sec Hypothesis
Gédel, K. 317, 366
Grammar

See Ambiguous grammar, Context-

free grammar, LR(k) gram-
mar, Right-linear grammar
Graph, of a function 328
Greibach normal form 271-273
Greibach, S. A. 304
Grep 110, 123
Gross, M. 217

H

Half, of a language
See Partial-removal operation
Halting, of a Turing machine 327—
328, 380


--- Page 532 ---
516

Hamilton-circuit problem 419-420,
453, 460-462
See also Directed Hamilton-circuit
problem
Hamilton-path problem 466
Hartmanis, J. 167, 366, 468, 510
HC
See Hamilton-circuit problem
Head 171
Hilbert, D. 317
Hochbaum, D. 5. 468
Homomorphism 139-140, 285, 382
See also Inverse homomorphism
Hopcroft, J. E. 166, 510
HTML 196-198
Huffman, D. A. 81, 166
Hypothesis 6

I

ID
See Instantaneous description

Idempotent law 116-117

Identity 95, 115

If-and-only-if proof 11-13, 179

If-else structure 193-194

Incompleteness theorem 317-318

Independent-set problem 448-452, 462

Induction principle 20

Inductive proof 19-28

Inductive step 19, 22-23

Infinite set 9

Inherently ambiguous language 212-
214, 302

Init, of a language 147, 291

Initial state

See Start state

Input symbol 46, 57, 222, 227, 318-
319, 327

Instantaneous description 224-227,
320-321

Instruction cycle 359-360

Integer 22

Interior node 181-182

Intersection 14, 121, 134-137, 285—
288, 302, 382, 407-408

INDEX

Intractable problem 1-2, 5, 361, 413-
414
See also NP-complete problem
Inverse homomorphism 140-143, 289-
291, 382, 425-426
Is
See Independent-set problem

J
Johnson, D. §. 468

K

Karp, R. M. 424, 452, 468, 510
Kasami, T. 298, 304
Kernighan, B. 308
Kleene closure

See Closure
Kleene, S. C. 123, 166, 366
Knapsack problem 465
Knuth, D. E. 253, 488, 510
Kruskal, J. B. Jr. 416
Kruskal’s algorithm 416

L

Language 14, 30-31, 33, 52, 59, 149,
177, 229-231, 326-327, 490-
492

See also Context-free language,

Empty language, Inherently
ambiguous language, Recur-
sive language, Recursively
enumerable language, Reg-
war language, Universal lan-
guage

Las-Vegas Turing machine 495

Leaf 181-182

Leftmost derivation 175-177, 184,
187-189, 211-212

Left-sentential form 184, 187-189,
237-238

Length, of a string 29

Lesk, M. 123

Levin, L. A. 468

Lewis, P. M. II 510


--- Page 533 ---
INDEX

Lex 110-111, 123

Lexical analyzer 2, 84, 109-111

Linear integer programming prob-
Tem 465

Literal 436

LR(k) grammar 253

M

Markup language
See HTML, XML
Max, of a language 147, 292
MeCarthy, J. 81
McCulloch, W. 8. 81
McNaughton, R. 123, 167
Mealy, G. H. 81
Membership test 153, 298-301
Miller, G. L. 510
Min, of a language 147, 292
Minimization, of DFA’s 159-164
Minimum-weight spanning tree 415-
416
Minsky, M. L. 366, 411
Modified Post’s correspondence prob-
lem 394402
Modular arithmetic 501-503
Modus ponens 7
Monte-carlo Turing machine 493
Moore, E. F. 81, 167
Moore’s law 1
Motwani, R. 510
Move
See Transition function
Multihead Turing machine 344
Multiple tracks

See Track
Multiplication 362, 501-503
Multistack machine

See Stack machine
Multitape Turing machine 336-340
Mutual induction 26-28

N

Natural language 191
Naur, P. 217

NC

See Node-cover problem

NFA

See Nondeterministic finite au-
tomaton

Node-cover problem 452-453, 462

Nondeterministic finite automaton
55-70, 96, 150-151, 163

See also e-NFA

Nondeterministic polynomial space

See NPS

Nondeterministic polynomial time

See NP

Nondeterministic Turing machine 340-
342, 473, 476-478, 493

See also AP, A'PS

Nonrecursive language
See Undecidable problem

Nonrecursively enumerable language
See Recursively enumerable lan-

guage

Nonterminal
See Variable

Normal form 255-273

NP 419, 423, 426, 470, 479, 497-

498, 505-508

NP-complete problem 422 - 424, 447,
450, 470-472

See also Clique problem, Color-
ing problem, CSAT, Dom-
inating-set. problem, Edge-
cover problem, Exact-cover
problem, Firehouse prob-
lem, Hamilton-circuit prob-
lem, Hamilton-path prob-
lem, Independent-set. prob-
lem, Knapsack problem, Lin-
ear integer programming prob-
lem, Node-cover problem,
Satisfiability problem, Sub-
graph isomorphism prob-
lem, 3SAT, Traveling sales-
man problem, Unit-execu-
tion-time-scheduling prob-
lem



--- Page 534 ---
518

NP-hard problem 423

See also Intractable problem
NPS 473, 477-478
Nullable symbol 259-260, 298

oO

Observation 17
Oettinger, A. G. 253
Ogden, W. 304
Ogden’s lemma 281

P

P 414, 423, 425-426, 479, 497
Palindrome 170, 177-178
Papadimitriou, C. H. 510
Parent 182
Parse tree 181-189, 207, 274-275
Sec also Tree
Parser 169, 192-195
Partial function 329
Partial solution, to PCP 395
Partial-removal operation 147, 292
Partition 161
Paull, M. C. 304
PCP
See Post’s correspondence prob-
lem
PDA
See Pushdown automaton
Perles, M. 166, 304, 411
Permutation, of a language 293
Pigeonhole principle 66
Pitts, W. 81
Polynomial space 469, 473-478
See also PS
Polynomial time 5
See also P, RP, ZPP
Polynomial-time reduction 413-414,
421-423, 478-479
Pop 220
Post, E. 366, 411
Post's correspondence problem 392-
402
Pratt, V. R. 510

INDEX

Precedence, of operators 88-89, 208
Prefix property 248-249
Prime number 470, 498-508
Problem 31-33, 417
Product construction 134
Production 171
See also €-production, Unit pro-
duction
Proof 5-6, 12
See also Contradiction, proof by,
Deductive proof, If-and-only-
if proof, Inductive proof
Property, of languages 387-388
Protocol 2, 39-45
PS 469, 473, 477-479
PS-complete problem 478-479
See also Quantified boolean for-
mula, Shannon switching
game
Publi¢-key signature 500-3501
Pumping lemma 126-130, 274-281
Push 220
Pushdown automaton 219-246, 204—
295
See also Deterministic pushdown
automaton, Stack machine

Q

QBF

See Quantified boolean formula
Quantified boolean formula 479-487
Quantifier 11, 128
Quicksort 488
Quotient 146, 292

R

Rabin, M. O. 81, 511

Raghavan, P. 510

Randomized Turing machine 489-
492

Random-number generator 469, 487-
489

Random-polynomial language

See RP


--- Page 535 ---
INDEX

Reachable symbol 256, 258-259, 298
Recursive definition 22-23
Recursive function 381
Recursive inference 173-174, 184-
186, 190 -191
Recursive language 327-328, 373-
377, 474
Recursively enumerable language 326,
368 379, 384
Reduction 313-316, 383-384
See also Polynomial-time reduc-
tion
Register 358
Regular expression 4-5, 83-123, 151,
487
Regular language 180, 247-248, 286,
289, 409
See also Deterministic finite an-
tomaton, Nondeterministic
finite automaton, Pumping
lemma, Regular expression
Reversal 137-138, 285, 425-426
Rice, H. G. 411
Rice’s theorem 387-390
Riemann’s hypothesis 510
Right-linear grammar 180
Rightmost derivation 175-177, 184
Right-sentential form 178, 184, 189
Ritchic, D. 308
Rivest, R. L. 499, 511
Root 182-183
Rose, G. F. 166, 304, 411
RP 469-470, 488, 492-498, 504
RSA code 499
Rudich, S. 366
Running time
See Time complexity

be)

SAT
See Satisfiability problem
Satisfiability problem 426-434, 462,
471
Savitch, W. J. 511
Savitch’s theorem 477-478

Scheduling problem
See Unit-execution-time-sched-
uling problem
Scheinberg, 5. 305
Schutzenberger, M. P. 253, 411
Scott, D. 81
Seiferas, J. I. 167
Semi-infinite tape 345.-348
Sentential form 178
See also Left-sentential form, Right-
sentential form
Set former 32
Sethi, R. 123, 217
Shamir, A. 499, 511
Shamir, E. 166, 304, 411
Shannon, C. E. 81
Shannon switching game 487
Shifting-over 334
Shuffle, of languages 292-293
y
See Input symbol
Size, of inputs 417
Spanier, E. H. 166
Spanning tree 415
See also Minimum-weight span-
ning trec
Stack 219-220, 476
Stack machine 348-351
Stack symbol 222, 227
Star 86
Sce also Closure
Start state 46, 57, 222, 319
Start. symbol 171, 222
State 2-3, 39, 46, 57, 221, 227, 319,
327, 330-331, 357
See also Dead state
State elimination 96-101
Stearns, R. E. 167, 366, 468, 510
Stockmeyer, L. J. 510
Storage device 355-356
String 29-30, 49, 176, 369
String search
See Text search
Structural induction 23-26
Subgraph isomorphism problem 464


--- Page 536 ---
520)

Subroutine 333-334
Subset construction 61-66
Substitution 282-285
Switching circuit 125
Symbol
See Generating symbol, Input
symbol, Input symbol, Nul-
lable symbol, Reachable sym-
bol, Stack symbol, Start sym-
bol, Tape symbol, Termi-
nal symbol, Useless sym-
bol
Symbol table 279
Syntactic category
See Variable

T

Tail 238

Tape 318

Tape head 318

Tape symbol 319, 327, 357

Tarjan, R. E. 510

Tautology problem 471

Term 209

Terminal symbol 171, 176

Text search 68-72, 84, 111-113

Theorem 17

There exists

See Quantifier

Thompson, K. 123

3SAT 435-436, 445-446, 462

Time complexity 339-340, 361-363,
414, 503

Token 109-111

Track 331-333

Transition diagram 48, 223-224, 323-
326

Transition function 46, 57, 222, 319

See also Extended transition func-

tion

Transition table 48—49

Transitive law 160

Traveling salesman problem 419-421,
461-462

Tree 23-25

INDEX

See also Parse tree
Treybig, L. B. 468
Trivial property 388
Truth assignment 426-427
TSP
See Traveling salesman problem
Turing, A. M. 1, 318, 323, 366, 411
Turing machine 307, 316-329, 414,
473-474
See also Code, for Turing ma-
chine, Halting, of a Turing
machine, Las-Vegas Turing
machine, Monte-carlo Tur-
ing machine, Multihead Tur-
ing machine, Multitape Tur-
ing machine, Nondetermin-
istic Turing machine, Ran-
domized Turing machine,
Recursively enumerable lan-
guage, Two-dimensional Tur-
ing machine, Universal Tur-
ing machine
‘Two-dimensional Turing machine 345
25AT 437, 446

U

Ullman, J. D. 35, 123, 217, 468, 510
Unambiguous grammar
See Ambiguous grammar
Undecidable problem 302, 310, 367—
368, 373-374, 384, 386-387,
390, 403-408
See also Post’s correspondence
problem, Rice’s theorem, Uni-
versal language
Union 14, 84, 86, 95, 103, 109, 114-
117, 132, 199, 284, 382, 425—
426
Unit pair 263
Unit production 256, 262-266
Unit-execution-time-scheduling prob-
lem 465
Universal language 377-381
Universal Turing machine 357, 377—
378


--- Page 537 ---
INDEX 521

UNIX regular expressions 108-110
Useless symbol 255-259

Vv
Variable 171, 176

WwW

Word
See String
World-wide-web consortium 217

x

XML 169, 198
See also DTD

Y

YACC 194-195, 208, 253
Yamada, H. 123

Yield 183

Younger, D. H. 298, 305

Zz

Zero-error probabilistic polynomial
language
See ZPP
ZPP 469-470, 488, 495-497

